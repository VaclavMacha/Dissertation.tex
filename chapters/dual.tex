\chapter{Dual Formulation: Linear Model}\label{chap: dual}

In Chapter~\ref{chap: framework}, we introduced a general framework for binary classification at the top. Moreover, we showed that several problem classes, considered separate problems so far, fit into this framework. The summary of all formulations is provided in Table~\ref{tab: summary formulations}. In Chapter~\ref{chap: linear} we discussed a special case when the linear model is used. Then formulation~\eqref{eq: aatp surrogate} reads
\begin{mini}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + \lambda_1 \cdot \fps(\bm{s}, t) + \lambda_2 \cdot \fns(\bm{s}, t)
  }{\label{eq: aatp surrogate liner primal}}{}
  \addConstraint{s_i}{= \bm{w}^{\top} \bm{x}_i, \quad i \in \I}
  \addConstraint{t}{= G\Brac{\bm{s}, \bm{y}}.}
\end{mini}
Many formulations have nice theoretical properties such as convexity or differentiability in this specific case. However, many real-world problems are not linearly separable, and in such cases, the approach from the previous section is not sufficient. In this chapter, we use the similarity of~\eqref{eq: aatp surrogate liner primal} to primal formulation of SVM~\cite{cortes1995support} and derive dual forms for all formulations from Table~\ref{tab: summary formulations}. Then we use the kernel method~\cite{scholkopf2001learning} to introduce nonlinearity into the dual formulations. Moreover, as dual problems are generally computationally expensive, we propose an efficient method to solve them.

\section{Derivation of Dual Problems}\label{sec:Derivation of dual problems}

As discussed in the introduction, this section is dedicated to deriving dual forms for all formulations from Table~\ref{tab: summary formulations}. We do not discuss \Grill and \GrillNP formulations in the following text since both formulations are not convex, and therefore their primal and dual formulations are not equivalent. Since many of the remaining formulations are very similar, we divide them into two families:
\begin{itemize}
  \item \textbf{\TopPushK family:} \TopPush, \TopPushK, \TopMeanK and \tauFPL.
  \item \textbf{\PatMat family:} \PatMat and \PatMatNP.
\end{itemize}
Both families use surrogate false-negative rate as an objective function. Moreover, all formulations from \TopPushK family use the mean of~$K$ highest scores of all or negative samples as a threshold and differ only in the definition of~$K.$ Finally, both formulations from \PatMat family use a surrogate approximation of the top $\tau$-quantile of scores of all or negative samples. In other words, we have two families of formulations that share the same objective function and the same form of the decision threshold. Therefore, we derive all results for the general form of these two families. Before we start, we need to introduce the concept of conjugate functions.

\pagebreak

\begin{definition}[Conjugate function~\cite{boyd2004convex}]\label{def: conjugate}
  Let~$l \colon \R^n \to \R.$ The function~$l^{\star} \colon \R^n \to \R,$ defined as
  \begin{equation*}
    l^{\star} (\bm{y})
      =  \sup_{\bm{x} \in \domain l} \{\bm{y}^{\top}\bm{x} - l(\bm{x})\}.
  \end{equation*}
  is called the conjugate function of~$l.$ The domain of the conjugate function
  consists of $\bm{y} \in \R^n$ for which the supremum is finite. 
\end{definition}

These functions will play a crucial role in the resulting form of dual problems. Recall the hinge loss and quadratic hinge loss function defined in Notation~\ref{not: surrogates}
\begin{align*}
    l_{\text{hinge}}(s) & = \max\Brac[c]{0, 1 + s}, &
    l_{\text{quadratic}}(s) & = \Brac{\max\Brac[c]{0, 1 + s}}^2.
\end{align*}
The conjugate function for the hinge loss can be found in~\cite{shnlev2014accelerated} and has the following form
\begin{equation}\label{eq: conjugate hinge}
  l_{\text{hinge}}^{\star}(y) =
  \begin{cases}
    -y & \text{if } y \in [0, 1], \\
    \infty & \text{otherwise.}
  \end{cases}  
\end{equation}
Similarly, the conjugate function for the quadratic hinge was computed in~\cite{kanamori2013conjugate} as
\begin{equation}\label{eq: conjugate quadratic hinge}
  l_{\text{quadratic}}^{\star}(y) =
  \begin{cases}
    \frac{y^2}{4} - y & \text{if } y \geq 0, \\
    \infty & \text{otherwise.}
  \end{cases}
\end{equation}

\begin{notation}[Kernel Matrix]\label{not: kernel matrix}
  To simplify the future notation, we introduce matrix~$\X$ of all samples. Each row of~$\X$ represents one sample and is defined for all~$i \in \I$ as
  \begin{equation*}
    \X_{i, \bullet} = \bm{x}_i^{\top}.
  \end{equation*}
  In the same way, we defined matrices~$\X^+,$~$\X^-$ of all negative and positive samples with rows defined as
  \begin{align*}
    \X^{-}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^-, \\
    \X^{+}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^+.
  \end{align*}
  Moreover, for all formulations that use only negative samples to compute the threshold~$t$, we define kernel matrix~$\Kneg$ as
  \begin{equation*}
    \Kneg = \Matrix{\X^+ \\ - \X^-} \Matrix{\X^+ \\ - \X^-}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} }.
  \end{equation*}
  and for all formulations that use only all samples to compute the threshold~$t$, we define kernel matrix~$\Kall$ as
  \begin{equation*}
    \Kall = \Matrix{\X^+ \\ - \X} \Matrix{\X^+ \\ - \X}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{\top} \\ -\X \X^{+\top} & \X \X^{\top} }.
  \end{equation*}
  In the rest of the text, matrix~$\K$ always refers to one of the kernel matrices defined above. 
\end{notation}

\subsection{Family of \TopPushK Formulations}

In this section, we focus on the family of \TopPushK formulations. The general optimization problem that covers all formulations from this family can be written in the following way
\begin{mini!}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \Ipos} l(t - \bm{w}^{\top} \bm{x}_i)
  }{\label{eq: toppushk family}}{\label{eq: toppushk family L}}
  \addConstraint{s_j}{= \bm{w}^{\top} \bm{x}_j, \quad j \in \Itil \label{eq: toppushk family c1}}
  \addConstraint{t}{= \frac{1}{K} \sum_{j = 1}^{K} s_{[j]}, \label{eq: toppushk family c2}}
\end{mini!}
where~$C \in \R.$ The set of indices~$\Itil$ equals~$\I$ for \TopMeanK and~$\Ineg$ for other formulations. The parameter~$K$ equals~$1$ for \TopPush, $K$ for \TopPushK, $\nall \tau$ for \TopMeanK, and $\nneg \tau$ for \tauFPL. Note that we use an alternative formulation with constant~$C,$ since it is more similar to the standard SVM, and we wanted to stress this similarity. For~$C = \nicefrac{1}{\npos}$ the new formulation is identical to the original one.

The following theorem shows the dual form of formulation~\eqref{eq: toppushk family}. The dual formulation for \TopPush was originally derived in~\cite{li2014top}. We only show, that our general dual formulation also covers this special case. To keep the readability as simple as possible, we postpone all proofs to Appendix~\ref{app: dual}.

\begin{restatable}[Dual formulation for \TopPushK family]{theorem}{topdual}\label{thm: toppushk family dual}
  Consider Notation~\ref{not: kernel matrix}, surrogate function~$l,$ and formulation~\eqref{eq: toppushk family}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
    - \frac{1}{2} \vecab^\top \K \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    }{\label{eq: toppushk family dual}}{\label{eq: toppushk family dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: toppushk family dual c1}}
    \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \ntil, \label{eq: toppushk family dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l$ and
  \begin{center}
    \renewcommand*{\arraystretch}{1}
    \begin{NiceTabular}{lcccc}
        & $K$
        & $\K$
        & $\ntil$
        & $\tilde{\bm{x}}_j$ \\
      \midrule
      \TopPush
        & $1$
        & $\Kneg$
        & $\nneg$
        & $\bm{x}^-_j$ \\
      \TopPushK
        & $K$
        & $\Kneg$
        & $\nneg$
        & $\bm{x}^-_j$ \\
      \TopMeanK
        & $\nall \tau$
        & $\Kall$
        & $\nall$
        & $\bm{x}_j$ \\
      \tauFPL
        & $\nneg \tau$
        & $\Kneg$
        & $\nneg$
        & $\bm{x}^-_j$ \\
    \end{NiceTabular}
  \end{center}
  If~$K = 1,$ the upper bound in the second constraint~\eqref{eq: toppushk family dual c2} vanishes due to the first constraint. Finally, the primal variables~$\bm{w}$ can be computed from dual variables as follows
  \begin{equation}\label{eq: toppushk family dual to primal}
    \bm{w} = \sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\ntil} \beta_j \tilde{\bm{x}}_j.
  \end{equation}
\end{restatable}

\subsection{Family of \PatMat Formulations}

In the same way, as for \TopPushK family, we introduce a general optimization problem that covers all formulations from \PatMat family and reads
\begin{mini}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \Ipos} l(t - \bm{w}^{\top} \bm{x}_i)
  }{\label{eq: patmat family}}{}
  \addConstraint{t}{\;\; \text{solves} \;\; \frac{1}{\ntil}\sum_{i \in \Itil} l\Brac{\vartheta(\bm{w}^{\top} \bm{x}_j - t)} = \tau,}
\end{mini}
where~$C \in \R.$ For \PatMat we have~$\Itil = \I$ and~$\ntil = \nall.$ For \PatMatNP we have~$\Itil = \Ineg$ and~$\ntil = \nneg.$ Again, we use the alternative formulation with constant~$C.$ The following theorem shows the dual form of the formulation~\eqref{eq: patmat family}.

\begin{restatable}[Dual formulation for \PatMat family]{theorem}{patdual}\label{thm: patmat family dual}
  Consider Notation~\ref{not: kernel matrix}, surrogate function~$l,$ and formulation~\eqref{eq: patmat family}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
    - \frac{1}{2} \vecab^\top \K \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    - \delta \sum_{j = 1}^{\ntil} l^{\star} \Brac{\frac{\beta_j}{\delta\vartheta }}
    - \delta \ntil \tau
    }{\label{eq: patmat family dual}}{\label{eq: patmat family dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: patmat family dual c1}}
    \addConstraint{\delta }{\geq 0, \label{eq: patmat family dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l,$~$\vartheta > 0$ is a scaling parameter and
  \begin{center}
    \renewcommand*{\arraystretch}{1}
    \begin{NiceTabular}{lccc}
        & $\K$
        & $\ntil$
        & $\tilde{\bm{x}}_j$ \\
      \midrule
      \PatMat
        & $\Kall$
        & $\nall$
        & $\bm{x}_j$ \\
      \PatMatNP
        & $\Kneg$
        & $\nneg$
        & $\bm{x}^-_j$ \\
    \end{NiceTabular}
  \end{center}
  Finally, the primal variables~$\bm{w}$ can be computed from dual variables as follows
  \begin{equation}\label{eq: patmat family dual to primal}
    \bm{w} = \sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\ntil} \beta_j \tilde{\bm{x}}_j.
  \end{equation}
\end{restatable}

\begin{note}
  For simplicity, the rest of the chapter covers only the \TopPushK formulation with hinge loss. We use this formulation since it is the prototypical example for the \TopPushK family of formulations. The results for the rest of the formulations from this family can be derived almost identically. Moreover, results for the \PatMat family of formulations can be derived similarly. Therefore, derivations for the \TopPushK family with quadratic hinge loss and the \PatMat family with hinge and quadratic hinge loss are postponed to Appendix~\ref{app: dual}.
\end{note}

As we mentioned at the beginning of the chapter, our goal is to extend our framework to be usable for linearly inseparable problems. We derived dual formulations for \TopPushK and \PatMat families in two previous sections. In this section, we show how to employ the kernels method~\cite{scholkopf2001learning} to introduce nonlinearity into these dual formulations. For simplicity, we focus only on formulations from \TopPushK family that compute the decision threshold only from negative samples. As mentioned in Notation~\ref{not: kernel matrix}, all such formulations use kernel matrix~$\Kneg.$ The derivation is the same for all other formulations.

To add kernels, we first realize that the classification score~$s_j$ for any sample~$\bm{x}_j \in \R^d$ reads 
\begin{equation}\label{eq:pred_linear}
  s_j
    = \bm{w}^{\top} \bm{x}_j
    = \sum_{i = 1}^{\npos} \alpha_i \bm{x}_j^{\top} \bm{x}_i^+ - \sum_{i = 1}^{\nneg} \beta_i \bm{x}_j^{\top} \bm{x}_i^-,
\end{equation}
where~$\bm{\alpha} \in \R^{\npos},$~$\bm{\beta} \in \R^{\nneg}$ are dual variables. This relation follows from the proof of Theorem~\ref{thm: toppushk family dual}. Consider now any kernel function~$k: \R^d \times \R^d \to \R.$ Then the first part of the objective function~\eqref{eq: toppushk family dual L} amounts to
\begin{equation*}
  \vecab^\top \Kneg \vecab
    = \vecab^\top \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} } \vecab.
\end{equation*}
Using the standard trick, we can replace the kernel matrix~$\Kneg$ with matrix in the following form
\begin{equation}\label{eq:kernel_nonlinear}
  \Kneg = \Matrix{k\Brac{\X^+, \X^{+}} & -k\Brac{\X^+, \X^{-}} \\ -k\Brac{\X^-, \X^{+}} & k\Brac{\X^-, \X^{-}}},
\end{equation}
where~$k(\cdot,\; \cdot)$ is applied to all rows of both arguments. Then for any sample~$\bm{x}_j$, the classification score~\eqref{eq:pred_linear} is replaced by
\begin{equation}\label{eq: scores dual kernel}
  s_j = \sum_{i = 1}^{\npos} \alpha_i k\Brac{\bm{x}_j, \bm{x}^+_i} - \sum_{i = 1}^{\nneg} \beta_i k\Brac{\bm{x}_j, \bm{x}^-_i}.
\end{equation}

\section{Coordinate Descent Algorithm}\label{sec: coordinate descent}

In the previous sections, we derived dual formulations for \TopPushK and \PatMat families of formulations. Moreover, we showed how to incorporate non-linear kernels into these formulations. As a result, we can use all presented formulations even for linearly non-separable problems. However, the dimension of the dual problems is at least equal to the number of all samples~$n,$ and therefore, it is computationally expensive to use standard techniques such as gradient descent. To handle this issue, the standard coordinate descent algorithm~\cite{chang2008coordinate, hsieh2008dual} has been proposed in the context of SVMs. In this section, we derive a coordinate descent algorithm suitable for our dual problems~(\ref{eq: toppushk family dual},~\ref{eq: patmat family dual}). We also show that we can reduce the whole optimization problem to a one-dimensional quadratic optimization problem with a closed-form solution in every iteration. Therefore, every iteration of our algorithm is cheap. For a review of other approaches see~\cite{batmaz2019review,werner2019review}.

Before we start, recall that the classification scores can be computed directly from dual variables as shown in~\eqref{sec: coordinate descent}. Using the definition of kernel matrix~$\K$, we can define a vector of scores~$\bm{s}$ by
\begin{equation}\label{eq: dual scores}
  \bm{s} = \K \vecab.
\end{equation}
Note that dual scores are not identical to the primal ones~\eqref{eq:pred_linear} (even though we use the same notation). The main difference is that dual scores use kernel function~$k.$ Therefore, they are equivalent only if the kernel function is defined as a dot product, i.e., if~$k(\bm{x}, \bm{z}) = \bm{x}^{\top} \bm{z}.$ Moreover, the vector of dual scores~\eqref{eq: dual scores} is not identical to the vector of primal scores from Notation~\ref{not: scores}, even with this choice of the kernel function. The reason is the construction of the kernel matrix~$\K$ from Notation~\ref{not: kernel matrix}. For example, for~$\Kneg$, the resulting vector of dual scores is a permutation of the vector of primal scores. Similarly, for~$\Kall$, the vector of primal scores corresponds to the elements of~\eqref{eq: dual scores} with indices~$\npos + 1, \ldots, \npos + \nall.$ Therefore for both definitions of kernel matrices from Notation~\ref{not: scores}, we can quickly get the vector of primal scores by permuting vector~\eqref{eq: dual scores} or selecting only some elements from vector~\eqref{eq: dual scores}. To simplify the indexing of the vector of scores~\eqref{eq: dual scores} and kernel matrix~$\K$, we introduce a new notation in Notation~\ref{not: dual update rules}.

\pagebreak

\begin{notation}\label{not: dual update rules}
  Consider any index~$l$ that satisfies~$1 \leq l \leq \npos + \ntil.$ Note that the length of dual variable~$\bm{\alpha}$ is~$\npos$ for both formulations~\eqref{eq: toppushk family dual} and~\eqref{eq: patmat family dual}. Therefore, we can define auxiliary index~$\hat{l}$ as 
  \begin{equation*}
    \hat{l} = \begin{cases}
      l & \text{if } l \leq \npos, \\
      l - \npos & \text{otherwise}.
    \end{cases}
  \end{equation*}
  Then the index~$l$ can be safely used for kernel matrix~$\K$ or vector of scores~$\bm{s},$ while its corresponding version~$\hat{l}$ can be used for dual variables~$\bm{\alpha}$ or~$\bm{\beta}.$
\end{notation}

\subsection{Family of \TopPushK Formulations}\label{sec: Top coordinate descent}

Consider dual formulation~\eqref{eq: toppushk family dual} from Theorem~\ref{thm: toppushk family dual} and fixed feasible dual variables~$\bm{\alpha},$~$\bm{\beta}.$ Our goal in this section is to derive an efficient iterative procedure for solving this problem. We follow the ideas presented in~\cite{chang2008coordinate, hsieh2008dual} for solving SVMs using a coordinate descent algorithm. However, we must modify the approach since we have an additional constraint~\eqref{eq: toppushk family dual c1}.  Due to this constraint, we always have to update (at least) two components of dual variables~$\bm{\alpha},$~$\bm{\beta}.$ There are only three update rules which modify two components of~$\bm{\alpha},$~$\bm{\beta},$ and satisfy constraints~\eqref{eq: toppushk family dual c1} and~\eqref{eq: dual scores}. The first one updates two components of~$\bm{\alpha}$
\begin{subequations}\label{eq: update rules}
\begin{align}\label{eq: update rule a,a}
  \alphak & \to \alphak + \Delta, & \quad
  \alphal & \to \alphal - \Delta, & \quad
  \bm{s} & \to \bm{s} + \Brac{\K_{\bullet, k} - \K_{\bullet, l}}\Delta,
\end{align}
where~$\K_{\bullet, i}$ denotes $i$-th column of~$\K$ and indices~$\hat{k},$~$\hat{l}$ are defined in Notation~\ref{not: dual update rules}. Note that the update rule for~$\bm{s}$ does not use matrix multiplication but only vector addition. The second rule updates one component of~$\bm{\alpha}$ and one component of~$\bm{\beta}$ 
\begin{align}\label{eq: update rule a,b}
  \alphak & \to \alphak + \Delta, & \quad
  \betal  & \to \betal  + \Delta, & \quad
  \bm{s} & \to \bm{s} + \Brac{\K_{\bullet, k} + \K_{\bullet, l}}\Delta,
\end{align}
and the last one updates two components of~$\bm{\beta}$
\begin{align}\label{eq: update rule b,b}
  \betak & \to \betak + \Delta, & \quad
  \betal & \to \betal - \Delta, & \quad
  \bm{s}  & \to \bm{s} + \Brac{\K_{\bullet, k} - \K_{\bullet, l}}\Delta.
\end{align}
\end{subequations}
Using any of the update rules above, the problem~\eqref{eq: toppushk family dual} can be written as a one-dimensional quadratic problem in the following form
\begin{maxi*}{\Delta}{
  -\frac{1}{2} a(\bm{\alpha}, \bm{\beta}) \Delta^2
  - b(\bm{\alpha}, \bm{\beta}) \Delta
  - c(\bm{\alpha}, \bm{\beta})
  }{}{}
  \addConstraint{\Delta_{lb}(\bm{\alpha}, \bm{\beta})}{\leq \Delta \leq \Delta_{ub}(\bm{\alpha}, \bm{\beta})}
\end{maxi*}
where~$a,$~$b,$~$c,$~$\Delta_{lb},$~$\Delta_{ub}$ are constants with respect to~$\Delta.$ The optimal solution to this problem is
\begin{equation}\label{eq: Delta optimal}
  \Delta^{\star} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma},
\end{equation}
where~$\gamma = -\frac{b}{a}$ and~$\clip{a}{b}{x}$ amounts to clipping (projecting)~$x$ to interval~$[a, b].$ Since we assume one of the update rules~\eqref{eq: update rules}, the constraint~\eqref{eq: toppushk family dual c1} is always satisfied after the update. Even though all three update rules hold for any surrogate, the calculation of the optimal~$\Delta^{\star}$ depends on the concrete form of surrogate function. In the following text, we show the closed-form formula for~$\Delta^{\star},$ when the hinge loss or quadratic hinge loss is used as a surrogate function.

\subsubsection{Hinge loss}

We start with the hinge loss function from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate hinge} of the hinge loss into the dual formulation~\eqref{eq: toppushk family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  }{\label{eq: Top dual hinge}}{\label{eq: Top dual hinge L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Top dual hinge c1}}
  \addConstraint{0 \leq \alpha_i}{\leq C,}{i = 1, 2, \ldots, \npos
  \label{eq: Top dual hinge c2}}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad}{j = 1, 2, \ldots, \ntil.
  \label{eq: Top dual hinge c3}}
\end{maxi!}
The form of~$\K$ and~$\ntil$ depends on the used formulation as discussed in Theorem~\ref{thm: toppushk family dual}. Moreover, the upper bound in~\eqref{eq: Top dual hinge c3} can be omitted for~$K = 1.$ Since we know the form of the optimal solution~\eqref{eq: Delta optimal}, we only need to show how to compute~$\Delta_{lb},$~$\Delta_{ub}$ and~$\gamma$ for all update rules~\eqref{eq: update rules}. The following three propositions provide closed-form formulas for all three update rules. To keep the presentation as simple as possible, we postpone all proofs to Appendix~\ref{sec: toppushk family coordinate proofs}.

\begin{restatable}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Top dual hinge}]{proposition}{topruleaa}\label{prop: toppushk family hinge update a,a}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule a,a}, indices~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \max\{- \alphak,\; \alphal - C\}, \\
    \Delta_{ub} & = \min\{C - \alphak,\; \alphal \}, \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{restatable}

\begin{restatable}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Top dual hinge}]{proposition}{topruleab}\label{prop: toppushk family hinge update a,b}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule a,b}, indices~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Let us define
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        \max \Brac[c]{- \alphak, \;  -\betal} & K = 1, \\
        \max \Brac[c]{- \alphak, \;  -\betal, \; K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
          C - \alphak & K = 1, \\
          \min \Brac[c]{C - \alphak, \; \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}}  & \textrm{otherwise}.
      \end{cases*} \\
    \gamma & = - \frac{s_k + s_l - 1}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}.
  \end{align*}
\end{restatable}

\pagebreak

\begin{restatable}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Top dual hinge}]{proposition}{toprulebb}\label{prop: toppushk family hinge update b,b}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule b,b}, indices~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        - \betak & K = 1, \\
        \max \Brac[c]{- \betak,\; \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        \betal & K = 1, \\
        \min \Brac[c]{\frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak,\; \betal} & \textrm{otherwise}.
      \end{cases*} \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{restatable}

\subsubsection{Initialization}

For all update rules~\eqref{eq: update rules} we assumed that the current solution~$\bm{\alpha},$~$\bm{\beta}$ is feasible. So to create an iterative algorithm that solves problem~\eqref{eq: Top dual hinge} or~\eqref{eq: Top dual quadratic}, we need to have a way how to obtain an initial feasible solution. Such a task can be formally written as a projection of random variables~$\bm{\alpha}^0,$~$\bm{\beta}^0$ to the feasible set of solutions
\begin{mini}{\bm{\alpha}, \bm{\beta}}{
  \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
  + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
  }{\label{eq: toppushk family initialization}}{}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j}
  \addConstraint{0 \leq \alpha_i}{\leq C_1, \quad i = 1, 2, \ldots, \npos,}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \ntil,}
\end{mini}
where the upper bound in the second constraint depends on the used surrogate function. For hinge loss function, the upper bound is~$C_1 = C,$ while for the quadratic hinge loss it is~$C_1 = + \infty.$ To solve problem~\eqref{eq: toppushk family initialization}, we follow the same approach as in~\cite{adam2020projections}. In the following theorem, we show that problem~\eqref{eq: toppushk family initialization} can be written as a system of two equations of two variables~$\lambda$ and~$\mu.$ Moreover, the theorem shows the concrete form of feasible solution~$\bm{\alpha},$~$\bm{\beta}$ that depends only on~$\lambda$ and~$\mu.$

\begin{restatable}{theorem}{topinit}\label{thm: toppushk family initialization}
  Consider problem~\eqref{eq: toppushk family initialization}, some initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$ and denote the sorted version (in non-decreasing order) of~$\bm{\beta}^0$ as~$\bm{\beta}_{[\cdot]}^0.$ Then if the following condition holds
  \begin{equation}\label{eq:problem3_cond}
    \sum_{j = 1}^{K} \Brac{\beta_{[\ntil - K + j]}^0 + \max_{i = 1, \ldots, \npos} \alpha_i^0} \le 0,
  \end{equation}
  the optimal solution of~\eqref{eq: toppushk family initialization} amounts to~$\bm{\alpha} = \bm{\beta} = \bm{0}.$ In the opposite case, the following system of two equations
  \begin{subequations}\label{eq: toppushk family init alg}
    \begin{align}
      \sum_{i=1}^{\npos} \clip{0}{C_1}{ \alpha_i^0 - \lambda + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \lambda - \mu}} - K \mu
      & = 0, \label{eq: toppushk family init alg 1} \\
      \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K\mu
      & = 0, \label{eq: toppushk family init alg 2}
    \end{align}
  \end{subequations}
  has a solution~$(\lambda, \mu)$ with $\mu > 0,$ and the optimal solution of~\eqref{eq: toppushk family initialization} is equal to
  \begin{align*}
    \alpha_i
      & = \clip{0}{C_1}{\alpha_i^0 - \lambda + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \lambda - \mu}}, \\
    \beta_j & = \clip{0}{\mu}{\beta_j^0 + \lambda}.
  \end{align*}
\end{restatable}

Theorem~\ref{thm: toppushk family initialization} shows the optimal solution of~\eqref{eq: toppushk family initialization} that depends only on~$(\lambda, \mu)$ but does not provide any way to find such a solution. In the following text, we show that the number of variables in the system of equations~\eqref{eq: toppushk family init alg} can be reduced to one. For any fixed $\mu$, we denote the function on the left-hand side of~\eqref{eq: toppushk family init alg 2} by 
\begin{equation*}
  g(\lambda; \mu) := \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K\mu.
\end{equation*}
Then~$g$ is non-decreasing in~$\lambda$ but not necessarily strictly increasing. We denote by~$\lambda(\mu)$ any such~$\lambda$ solving~\eqref{eq: toppushk family init alg 2} for a fixed~$\mu$. Denote~$\bm{z}$ the sorted version of~$-\bm{\beta}^0$. Then we have
\begin{equation*}
  g(\lambda; \mu)
    = \sum_{\Set{j}{\lambda - z_j \in [0, \mu)}}(\lambda - z_j)
    + \sum_{\Set{j}{\lambda - z_j \ge \mu}}\mu - K\mu.
\end{equation*}
Now we can easily compute~$\lambda(\mu)$ by solving~$g(\lambda(\mu); \mu) = 0$ for fixed~$\mu.$ To get the solution efficiently, we derive Algorithm~\ref{alg: toppushk family lambda}, which can  be described as follows: Index~$i$ will run over~$\bm{z}$ while index~$j$ will run over~$\bm{z} + \mu$. At every iteration, we know the values of~$g(z_{i-1}; \mu)$ and~$g(z_{j-1}+\mu; \mu)$ and we want to evaluate~$g$ at the next point. We denote the number of indices~$j$ such that $\lambda - z_j \in[0, \mu)$ by~$d$. If~$z_i \le z_j + \mu$, then we consider~$\lambda = z_i$ and since one index enters the set~$\Set{j}{\lambda - z_j \in [0, \mu)}$, we increase~$d$ by one. On the other hand, if~$z_i > z_j + \mu$, then we consider $\lambda = z_j + \mu$ and since one index leaves the set~$\Set{j}{\lambda - z_j \in [0, \mu)}$, we decrease~$d$ by one. In both cases,~$g$ is increased by~$d$ times the difference between the new~$\lambda$ and old~$\lambda$. Once~$g$ exceeds~$0$, we stop the algorithm and linearly interpolate between the last two values. To prevent an overflow, we set~$z_{m+1} = + \infty$. Concerning the initial values, since~$z_1 \le z_1 + \mu$, we set $i=2$, $j=1$ and $d=1$. 

\begin{algorithm}
  \centering
  \begin{algorithmic}[1]
    \Require vector $-\bm{\beta}^0$ sorted into $\bm{z}$
    \State $i \gets 2$, $j \gets 1$, $d \gets 1$
    \State $\lambda \gets z_1$, $g \gets - K\mu$
    \While{$g < 0$}
      \If {$z_i \le z_j + \mu$}
        \State $g \gets g + d(z_i - \lambda)$
        \State $\lambda\gets z_i$, $d \gets d+1$, $i \gets i+1$
      \Else
        \State $g \gets g + d(z_j + \mu - \lambda)$
        \State $\lambda \gets z_j + \mu$, $d \gets d - 1$, $j \gets j + 1$
      \EndIf
    \EndWhile
    \State \textbf{return} linear interpolation of the last two values of $\lambda$
  \end{algorithmic}
  \caption{An efficient algorithm for computing~$\lambda(\mu)$ from~\eqref{eq: toppushk family initialization} for fixed~$\mu.$.}
  \label{alg: toppushk family lambda}
\end{algorithm}

Since~$\lambda(\mu)$ can be computed for fixed~$\mu$ using Algorithm~\ref{alg: toppushk family lambda}, we can define auxiliary function~$h$ in the following form
\begin{equation}\label{eq: toppushk family h}
  h(\mu)
    = \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu) + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0+\lambda(\mu) - \mu}} - K \mu.
\end{equation}
Then the system of equations~\eqref{eq: toppushk family init alg} is equivalent to~$h(\mu) = 0.$ The following lemma describes properties of~$h.$ Since~$h$ is decreasing in~$\mu$ on~$(0, \infty)$, any root-finding algorithm such as bisection can be used to find the optimal solution.

\begin{restatable}{lemma}{topinith}\label{lemma: toppushk family h}
  Even though~$\lambda(\mu)$ is not unique, function~$h$ from~\eqref{eq: toppushk family h} is well-defined in the sense that it gives the same value for every choice of~$\lambda(\mu)$. Moreover,~$h$ is decreasing in~$\mu$ on~$(0, + \infty)$.
\end{restatable}

\section{Summary}

In this chapter, we derived dual formulation for \TopPushK and \PatMat family of formulations. Moreover, we derived simple update rules that can be used to improve the current feasible solution. We also showed that these update rules have closed-form formulas, and therefore they are simple to compute. Finally, we showed how to find an initial feasible solution. This section combines all these intermediate results into Algorithm~\ref{alg:Coordinate descent} and discusses its computational complexity.

\begin{algorithm*}
  \begin{minipage}{0.48\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State Set~$\bm{\alpha},$~$\bm{\beta}$ using Theorem~\ref{thm: toppushk family initialization}
      \State Set~$\bm{s}$ based on~\eqref{eq: dual scores} \label{alg: line 1}
      \Repeat \label{alg: line 2}
        \State Pick random~$k$ from~$\{1, \ldots, \npos + \ntil\}$ \label{alg: line 3}
        \For{$l \in \{1, \ldots, \npos + \ntil  \}$} \label{alg: line 4}
            \State Compute~$\Delta_{l}$  \label{alg: line 5}
        \EndFor
        \State Select the best~$\Delta_{l}$ \label{alg: line 7}
        \State Update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq: update rules} \label{alg: line 8}
        \State \label{alg: line 9}
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}%
  \hfill
  \begin{minipage}{0.48\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State Set~$\bm{\alpha},$~$\bm{\beta},$~$\delta$ using Theorem~\ref{thm: patmat family initialization}
      \State Set~$\bm{s}$ based on~\eqref{eq: dual scores}
      \Repeat
        \State Pick random~$k$ from~$\{1, \ldots, \npos + \ntil \}$ 
        \For{$l \in \{1, \ldots, \npos + \ntil \}$}
            \State Compute~$\Delta_{l}$ and~$\delta_{l}$
        \EndFor
        \State Select the best~$\Delta_{l}$ and~$\delta_{l}$
        \State Update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq: update rules}
        \State set~$\delta \leftarrow \delta_{l}$
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \caption{Coordinate descent algorithm for \TopPushK family of formulations (\textbf{left}) and \PatMat  family of formulations (\textbf{right}).}
  \label{alg:Coordinate descent}
\end{algorithm*}

The left column in Algorithm~\ref{alg:Coordinate descent} describe the algorithm for \TopPushK family while the right column for \PatMat family. In step~\ref{alg: line 1} we initialize~$\bm{\alpha}$,~$\bm{\beta}$ and~$\delta$ to some feasible value using Theorem~\ref{thm: toppushk family initialization} or  Theorem~\ref{thm: patmat family initialization}. Then, based on~\eqref{eq: dual scores} we compute scores~$\bm{s}$. Each \repeatloop loop in step~\ref{alg: line 2} updates two coordinates as shown in~\eqref{eq: update rules}. In step~\ref{alg: line 3} we select a random index~$k$ and in the \forloop loop in step~\ref{alg: line 4} we compute the optimal~$(\Delta_l,\delta_l)$ for all possible combinations~$(k,l)$ as in~\eqref{eq: update rules}. In step~\ref{alg: line 7} we select the best pair~$(\Delta_l,\delta_l)$ which maximizes the coresponding objective function. Finally, based on the selected update rule we update~$\bm{\alpha}$,~$\bm{\beta}$,~$\bm{s}$ and~$\delta$ in steps~\ref{alg: line 8} and~\ref{alg: line 9}.

Now we derive the computational complexity of each \repeatloop loop from step~\ref{alg: line 2}. The computation of~$(\Delta_l,\delta_l)$ amounts to solving a quadratic optimization problem in one variable. As we showed in Sections~\ref{sec: Top coordinate descent} and~\ref{sec: Pat coordinate descent}, there is a closed-form solution and step~\ref{alg: line 5} can be performed in~$O(1)$. Since this is embedded in a \forloop loop in step~\ref{alg: line 4}, the whole complexity of this loop is~$O(\npos + \ntil)$. Step~\ref{alg: line 8} requires~$O(1)$ for the update of~$\bm{\alpha}$ and~$\bm{\beta}$ while~$O(\npos + \ntil)$ for the update of~$\bm{s}$. Since the other steps are~$O(1)$, the total complexity of the \repeatloop loop is~$O(\npos + \ntil)$. This holds only if the kernel matrix~$\K$ is precomputed. In the opposite case, all complexities must be multiplied by the cost of computation of components of~$\K$, which is~$O(d)$. This complexity analysis is summarized in Table~\ref{tab:Computational complexity}.

\begin{table}[h]
  \centering
  \begin{NiceTabular}{lcc}
    \CodeBefore
      \rowcolor{\headercol}{1}
      \rowcolors{3}{\rowcol}{}[restart]
    \Body
    \toprule
    \Block[c]{1-1}{Operation}
      & $\K$ precomputed
      & $\K$ not precomputed \\
    \midrule
    Evaluation of~$\Delta_l$
      & $O(1)$
      & $O(d)$ \\
    Update of~$\bm{\alpha}$ and~$\bm{\beta}$
      & $O(1)$
      & $O(1)$ \\
    Update of~$\bm{s}$
      & $O\Brac{\npos + \ntil}$
      & $O\Brac{(\npos + \ntil)d}$ \\
    \midrule
    Total per iteration
    & $O\Brac{\npos + \ntil}$
    & $O\Brac{(\npos + \ntil)d}$ \\
    \bottomrule
  \end{NiceTabular}
  \caption{Computational complexity of one \repeatloop loop (which updates two coordinates of~$\bm{\alpha}$ or~$\bm{\beta}$) from Algorithm~\ref{alg:Coordinate descent}.}
  \label{tab:Computational complexity}
\end{table}
