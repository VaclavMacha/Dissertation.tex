\chapter{Dual Formulation with Linear Model}\label{chap: dual}

In the Chapter~\ref{chap: framework}, we introduced general framework for binary classification at the top. Moreover, we showed that several problem classes, which were considered as separate problems so far, fit into the framework. As an example we can mention ranking problems of hypoothesis testing. Summary of all formulations is in Table~\ref{tab: summary formulations}. In the Chapter~\ref{chap: linear} disscused special case, when the linear classifier is used. In such a case, many of the formulations from Table~\ref{tab: summary formulations} have nice theoretical properties such as convexity or differentiability. However, as many problems are not linearly separable, nonlinear classifiers are needed. In this chapter, we show how to extend our framework into nonlinear classification problems. To do so, we use the fact that our framework is similar to the primal formulation of support vector machines~\cite{cortes1995support}. The classical way to incorporate nonlinearity into SVM is to derive the dual formulation~\cite{boyd2004convex} and to employ the kernels method~\cite{scholkopf2001learning}. In this chapter, we follow this approach, derive dual formulations for the considered problems and add nonlinear kernels to them. Moreover, as dual problems are generally expensive to solve, we derive a quick method to solve them. This is a modification of the coordinate-wise dual ascent from~\cite{hsieh2008dual}. For a review of other approaches see~\cite{batmaz2019review,werner2019review}.

\section{Derivation of dual problems}\label{sec:Derivation of dual problems}

In this section, we derive dual forms of formulations from Table~\ref{tab: summary formulations}. Since many of these formulations are very similar, we divide them into two families. The first one is a family of \TopPushK formulations that consists \TopPush, \TopPushK, \TopMeanK and \tauFPL formulations. All these formulations use false-negative rate as an objective function and the decision threshold is a mean of~$K$ largest scores of all or negative samples. The second family is a family of \PatMat formulations that consists \PatMat and \PatMat formulations. Also these two formulations use false-negative rate as an objective function, but the decision threshold is a surrogate approximation of top $\tau$-quantile of scores of all or negative samples. In other words, we have two families that share the same objective function and the form of the decision threshold, even thogh the decision threshold may be computed from different samples.  



\begin{notation}[Kernel Matrix]\label{not: kernel matrix}
  To simplify the future notation, we use matrix~$\X$ of all samples with rows defined for all~$i \in \I$ as 
  \begin{equation*}
    \X_{i, \bullet} = \bm{x}_i^{\top}.
  \end{equation*}
  In other words, each row of~$\X$ represents one sample. Similarly, we defined matrices~$\X^+,$~$\X^-$ of all negative and positive samples with rows defined as
  \begin{align*}
    \X^{-}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^-, \\
    \X^{+}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^+.
  \end{align*}
  Moreover, for \TopPush, \TopPushK, \tauFPL and \PatMatNP formulations we define the positive semidefinite kernel matrix~$\Kneg$ as
  \begin{equation*}
    \Kneg = \Matrix{\X^+ \\ - \X^-} \Matrix{\X^+ \\ - \X^-}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} }.
  \end{equation*}
  Simlarly, for \TopMeanK and \PatMat formulations we define the positive semidefinite kernel matrix~$\Kall$ as
  \begin{equation*}
    \Kall = \Matrix{\X^+ \\ - \X} \Matrix{\X^+ \\ - \X}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{\top} \\ -\X \X^{+\top} & \X \X^{\top} }.
  \end{equation*}
\end{notation}

\subsection{Family of \TopPushK formulations}

As we mentioned before, the first family is a family of \TopPushK formulations that consists \TopPush, \TopPushK, \TopMeanK and \tauFPL formulations. All these formulations can be written as follows
\begin{mini}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \Ipos} l(t - \bm{w}^{\top} \bm{x}_i)
  }{\label{eq: toppushK family}}{}
  \addConstraint{\tilde{s}_j}{= \bm{w}^{\top} \bm{x}_j, \quad j \in \Itil}
  \addConstraint{t}{= \sum_{j = 1}^{K} \tilde{s}_{[j]},}
\end{mini}
where~$C \in \R$ is a constant and~$\Itil$ and~$K$ is defined as follows
\begin{align*}
  \Itil & = \begin{cases}
    \I & \quad \text{for \TopMeanK}, \\
    \Ineg & \quad \text{otherwise}. \\
  \end{cases} &
  K & = \begin{cases}
    1 & \quad \text{for \TopPush}, \\
    \nall \tau & \quad \text{for \TopMeanK}, \\
    \nneg \tau & \quad \text{for \tauFPL}. \\
  \end{cases}
\end{align*}
It means, that for \TopMeanK, the threshold is computed from all samples and otherwise only from negative ones. Also note, that we use linear classifier and we also use this alternative formulation with constant~$C,$ since it is more similar to the standard SVM. The following theorem show the dual formulation of~\eqref{eq: toppushK family}. To keep the readability as simple as possible, we postpone all proofs to the Appendix~\ref{app: dual}.

\begin{restatable}[Dual formulation for \TopPushK family]{theorem}{topdual}\label{thm: toppushk family dual}
  Consider formulations \TopPush, \TopPushK, \TopMeanK and \tauFPL from Table~\ref{tab: summary formulations} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
    - \frac{1}{2} \vecab^\top \K \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    }{\label{eq: toppushk family dual}}{\label{eq: toppushk family dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: toppushk family dual c1}}
    \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \ntil, \label{eq: toppushk family dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l$ and
  \begin{align*}
    \K & = \begin{cases}
      \Kall & \quad \text{for \TopMeanK}, \\
      \Kneg & \quad \text{otherwise}, \\
    \end{cases} &
    \ntil & = \begin{cases}
      \nall & \quad \text{for \TopMeanK}, \\
      \nneg & \quad \text{otherwise}. \\
    \end{cases}
  \end{align*} 
  Moreover, the variable~$K$ is defined as follows
  \begin{equation*}
    K = \begin{cases}
      1 & \quad \text{for \TopPush}, \\
      \nall \tau & \quad \text{for \TopMeanK}, \\
      \nneg \tau & \quad \text{for \tauFPL}. \\
    \end{cases}
  \end{equation*}
  Finally, if~$K = 1,$ the upper bound in the second constrainet vanishes due to the first constraint.
\end{restatable}

\subsection{Family of \PatMat formulations}

Similarly to the previous section, we introduce the family of \PatMat formulations that consists of \PatMat and \PatMatNP formulations and can be written as follows
\begin{mini}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \Ipos} l(t - \bm{w}^{\top} \bm{x}_i)
  }{\label{eq: patmat family}}{}
  \addConstraint{t}{\quad \text{solves} \quad \frac{1}{\ntil}\sum_{i \in \Itil} l\Brac{\vartheta(\bm{w}^{\top} \bm{x}_j - t)} = \tau.}
\end{mini}
where~$C \in \R$ is a constant and~$\Itil$ and~$\ntil$ is defined as follows
\begin{align*}
  \Itil & = \begin{cases}
    \I & \quad \text{for \PatMat}, \\
    \Ineg & \quad \text{otherwise}. \\
  \end{cases} &
  \ntil & = \begin{cases}
    \nall & \quad \text{for \PatMat}, \\
    \nneg & \quad \text{otherwise}. \\
  \end{cases} &
\end{align*}
Again, we use linear classifier and the alternative formulation with constant~$C.$ The following theorem show the dual formulation of~\eqref{eq: patmat family}. 

\begin{restatable}[Dual formulation for \PatMat family]{theorem}{patdual}\label{thm: patmat family dual}
  Consider formulations \PatMat and \PatMatNP from Table~\ref{tab: summary formulations} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
    - \frac{1}{2} \vecab^\top \K \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    - \delta \sum_{j = 1}^{\ntil} l^{\star} \Brac{\frac{\beta_j}{\delta\vartheta }}
    - \delta \ntil \tau
    }{\label{eq: patmat family dual}}{\label{eq: patmat family dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: patmat family dual c1}}
    \addConstraint{\delta }{\geq 0, \label{eq: patmat family dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l,$~$\vartheta > 0$ is a scaling parameter and
  \begin{align*}
    \K & = \begin{cases}
      \Kall & \quad \text{for \PatMat}, \\
      \Kneg & \quad \text{otherwise}, \\
    \end{cases} &
    \ntil & = \begin{cases}
      \nall & \quad \text{for \PatMat}, \\
      \nneg & \quad \text{otherwise}. \\
    \end{cases}
  \end{align*}
\end{restatable}

\subsection{Adding kernels}

As we mentioned in the beggining of the chapter, our goal is to to extend our framework into nonlinear classification problems. In the previous sections we derived dual formulations for the \TopPushK and \PatMat family of formulations. In this sections we show, how to employ the kernels method~\cite{scholkopf2001learning} to introduce nonlinearity into the formulations. Firstly, consider any formulation that computes the decision threshold only from negative samples and therefore uses~$\Kneg$ as a kernel matrix. To add kernels, we first realize that the classification score~$s_j$ for any sample~$\bm{x}_j \in \R^d$ is given by 
\begin{equation}\label{eq:pred_linear}
  s_j
    = \bm{w}^{\top} \bm{x}_j
    = \sum_{i = 1}^{\npos} \alpha_i \bm{x}_j^{\top} \bm{x}_i^+ - \sum_{j = 1}^{\nneg} \beta_j \bm{x}_j^{\top} \bm{x}_j^-,
\end{equation}
where~$\bm{\alpha} \in \R^{\npos},$~$\bm{\beta} \in \R^{\nneg}$ are dual variables. This relation yields from the proof of Theorems~\ref{thm: toppushk family dual}. Consider now any kernel function~$k: \R^d \times \R^d \to \R.$ Then the first part of the objective function~\eqref{eq: toppushk family dual L} amounts to
\begin{align*}
  \vecab^\top \Kneg \vecab
  & = \vecab^\top \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} } \vecab \\
  & = \Matrix{\bm{\alpha} \\ -\bm{\beta}}^\top \Matrix{\X^+ \X^{+\top} & \X^+ \X^{-\top} \\ \X^- \X^{+\top} & \X^- \X^{-\top} }\Matrix{\bm{\alpha} \\ -\bm{\beta}}.
\end{align*}
Using the standard trick, we can replace the kernel matrix~$\Kneg$ with
\begin{equation}\label{eq:kernel_nonlinear}
  \Kneg = \Matrix{k(\X^+, \X^{+}) & -k(\X^+, \X^{-}) \\ -k(\X^-, \X^{+}) & k(\X^-, \X^{-}) },
\end{equation}
where~$k(\cdot,\; \cdot)$ is applied to all rows of both arguments. Then for any sample~$\bm{x}_j$, the classification score~\eqref{eq:pred_linear} is replaced by
\begin{equation*}
  s_j = \sum_{i = 1}^{\npos} \alpha_i k\Brac{\bm{x}_j, \bm{x}^+_i} - \sum_{j = 1}^{\nneg} \beta_j k\Brac{\bm{x}_j, \bm{x}^-_j}.
\end{equation*}
Similarly, we can use non-linear kernel matrix for any formulation that uses~$\Kall$ and also for all formulations from Theorem~\ref{thm: patmat family dual}.

\section{New Coordinate Descent Algorithm}\label{sec: coordinate descent}

In the previous sections, we showed that that dual formulations of \TopPush, \TopPushK, \TopMeanK and \tauFPL are very similary and can be written in general form summarized in Theorem~\ref{thm: toppushk family dual}. Similarly, dual formulations of \PatMat and \PatMatNP are very similary and can be written in general form summarized in Theorem~\ref{thm: patmat family dual}. We also showed, that these dual formulations  allow us to incorporate nonlinearity using kernels~\cite{scholkopf2001learning} in the same way as in SVM. However, their dimension is at least equal to the number of all samples~$n$ and therefore it is computationally expensive to use standard techniques such as the gradient descent. To handle this issue, the coordinate descent algorithm~\cite{chang2008coordinate,hsieh2008dual} has been proposed in the context of SVMs. Our goal in thsi section is to derived coordinate descent algorithm suitable for dual problems~(\ref{eq: toppushk family dual},~\ref{eq: patmat family dual}). Since these problems differ from original SVMs by additional constraints (\ref{eq: toppushk family dual c1},~\ref{eq: patmat family dual c1}), the key idea of our algorithm is to update two coordinates (instead of one) of dual bariables~$\bm{\alpha},$~$\bm{\beta}$ at every iteration. It will allow us to derive iterative procedure where in every iteration we need to find a solution of a one-dimensional quadratic optimization problem. As we will show later, these one-dimensional problems have a closed form solution, which means that every iteration is cheap.

\subsection{Family of \TopPushK Formulations}\label{sec: Top coordinate descent}

Consider dual formulation~\eqref{eq: toppushk family dual} from Theorem~\ref{thm: toppushk family dual} and fixed feasible dual variables~$\bm{\alpha},$~$\bm{\beta}.$ Let us define vector of scores~$\bm{s}$ by
\begin{equation}\label{eq: dual scores}
  \bm{s} = \K \vecab.
\end{equation}
As we said before, dual formulation~\eqref{eq: toppushk family dual} differs from original SVMs by additional constraintst~\eqref{eq: toppushk family dual c1}. Due to this constraint, we always have to update (at least) two coordinates of dual variables~$\bm{\alpha},$~$\bm{\beta}$ to not violate the constraintst~\eqref{eq: toppushk family dual c1}. Moreover, there are only three update rules which modify two coordinates of~$\bm{\alpha},$~$\bm{\beta}$ and which satisfy constraints~\eqref{eq: toppushk family dual c1} and also keep~\eqref{eq: dual scores} satisfied. The first one updates two components of~$\bm{\alpha}$
\begin{subequations}\label{eq: update rules}
\begin{align}\label{eq: update rule a,a}
  \alphak & \to \alphak + \Delta, & \quad
  \alphal & \to \alphal - \Delta, & \quad
  \bm{s} & \to \bm{s} + \Brac{\K_{\bullet, k} - \K_{\bullet, l}}\Delta,
\end{align}
where~$\K_{\bullet, i}$ denotes~$i$-th column of~$\K$ and indices~$\hat{k},$~$\hat{l}$ are defined in Notation~\ref{not: dual update rules}. Note that the update rule for~$\bm{s}$ does not use matrix multiplication but only vector addition. The second rule updates one component of~$\bm{\alpha}$ and one component of~$\bm{\beta}$ 
\begin{align}\label{eq: update rule a,b}
  \alphak & \to \alphak + \Delta, & \quad
  \betal  & \to \betal  + \Delta, & \quad
  \bm{s} & \to \bm{s} + \Brac{\K_{\bullet, k} + \K_{\bullet, l}}\Delta,
\end{align}
and the last one updates two components of~$\bm{\beta}$
\begin{align}\label{eq: update rule b,b}
  \betak & \to \betak + \Delta, & \quad
  \betal & \to \betal - \Delta, & \quad
  \bm{s}  & \to \bm{s} + \Brac{\K_{\bullet, k} - \K_{\bullet, l}}\Delta.
\end{align}
\end{subequations}
Using any of the update rules defined above, the problem~\eqref{eq: toppushk family dual} can be written as a one-dimensional quadratic problem
\begin{maxi*}{\Delta}{
  -\frac{1}{2} a(\bm{\alpha}, \bm{\beta}) \Delta^2
  - b(\bm{\alpha}, \bm{\beta}) \Delta
  - c(\bm{\alpha}, \bm{\beta})
  }{}{}
  \addConstraint{\Delta_{lb}(\bm{\alpha}, \bm{\beta})}{\leq \Delta \leq \Delta_{ub}(\bm{\alpha}, \bm{\beta})}
\end{maxi*}
where~$a,$~$b,$~$c,$~$\Delta_{lb},$~$\Delta_{ub}$ are constants with respect to~$\Delta.$ The optimal solution to this problem is
\begin{equation}\label{eq: Delta optimal}
  \Delta^{\star} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma},
\end{equation}
where~$-\nicefrac{b}{a}$ and~$\clip{a}{b}{x}$ amounts to clipping (projecting)~$x$ to interval~$[a, b].$ Since we assume one of the update rules~\eqref{eq: update rules}, the constrain~\eqref{eq: toppushk family dual c1} is always satisfied after the update. Evethough all three update rules hold true for any surrogate, the calculation of the optimal~$\Delta^{\star}$ depends on the concrete form of surrogate function. In the following text, we show the closed-form formula for~$\Delta^{\star},$ when the hinge loss or quadratic hinge loss is used as surrogate.

\begin{notation}\label{not: dual update rules}
  Consider any index~$l$ that satisfies~$1 \leq l \leq \npos + \ntil.$ Since the length of~$\bm{\alpha}$ is always~$\npos,$ we define auxiliary index~$\hat{l}$ as 
  \begin{equation*}
    \hat{l} = \begin{cases}
      l & \text{if } l \leq \npos, \\
      l - \npos & \text{otherwise}.
    \end{cases}
  \end{equation*}
  Then the index~$l$ without hat can be safely used for kernel matrix~$\K$ or vector of scores~$\bm{s}$ while its corresponding version with hat~$\hat{l}$ for~$\bm{\alpha}$ or~$\bm{\beta}.$
\end{notation}

\subsubsection{Hinge loss}

We start with the hinge loss function from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate hinge} of the hinge loss into the dual formulation from Theorem~\ref{thm: toppushk family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  }{\label{eq: Top dual hinge}}{\label{eq: Top dual hinge L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Top dual hinge c1}}
  \addConstraint{0 \leq \alpha_i}{\leq C,}{i = 1, 2, \ldots, \npos
  \label{eq: Top dual hinge c2}}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad}{j = 1, 2, \ldots, \ntil.
  \label{eq: Top dual hinge c3}}
\end{maxi!}
The form of~$\K$ and~$\ntil$ depends on the used formulations as discussed in Theorem~\ref{thm: toppushk family dual}. Moreover, the upper limit in~\eqref{eq: Top dual hinge c3} can be ommited for~$K = 1.$ Since we know the form of the optimal solution~\eqref{eq: Delta optimal}, we only need to show the concrete form of~$\Delta_{lb},$~$\Delta_{ub}$ and~$\gamma$ for all update rules~\eqref{eq: update rules} when the hinge loss is used. The following three lemmas provide closed-form formulas all considered update rules. To keep the presentation as simple as possible, we postpone all proofs to Appendix~\ref{sec: toppush family coordinate proofs}.

\begin{restatable}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Top dual hinge}]{lemma}{topruleaa}\label{thm: toppushk family hinge update a,a}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule a,a}, indices~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \max\{- \alphak,\; \alphal - C\}, \\
    \Delta_{ub} & = \min\{C - \alphak,\; \alphal \}, \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{restatable}

\begin{restatable}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Top dual hinge}]{lemma}{topruleab}\label{thm: toppushk family hinge update a,b}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule a,b}, indices~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Let us define
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        \max \Brac[c]{- \alphak, \;  -\betal} & K = 1, \\
        \max \Brac[c]{- \alphak, \;  -\betal, \; K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
          C - \alphak & K = 1, \\
          \min \Brac[c]{C - \alphak, \; \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}}  & \textrm{otherwise}.
      \end{cases*} \\
    \gamma & = - \frac{s_k + s_l - 1}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}.
  \end{align*}
\end{restatable}

\begin{restatable}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Top dual hinge}]{lemma}{toprulebb}\label{thm: toppushk family hinge update b,b}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule b,b}, indices~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        - \betak & K = 1, \\
        \max \Brac[c]{- \betak,\; \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        \betal & K = 1, \\
        \min \Brac[c]{\frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak,\; \betal} & \textrm{otherwise}.
      \end{cases*} \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{restatable}

\subsubsection{Quadratic hinge loss}

The second considered suroagte function is the quadratic hinge loss from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate hinge} of the quadratic hinge loss into the dual formulation from Theorem~\ref{thm: toppushk family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  - \frac{1}{4C} \sum_{i = 1}^{\npos} \alpha_i^2
  }{\label{eq: Top dual quadratic}}{\label{eq: Top dual quadratic L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Top dual quadratic c1}}
  \addConstraint{0 \leq \alpha_i}{,}{i = 1, 2, \ldots, \npos
  \label{eq: Top dual quadratic c2}}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad}{j = 1, 2, \ldots, \ntil,
  \label{eq: Top dual quadratic c3}}
\end{maxi!}
Similarly to the previous case, the form of~$\K$ and~$\ntil$ depends on the used formulations and the upper limit in~\eqref{eq: Top dual quadratic c3} can be ommited for~$K = 1.$ For simplicity, we postpone formulas for update rules~\eqref{eq: update rules} and their proofs to Appendix~\ref{sec: toppush family coordinate proofs}. More specificaly, all update rules can be found in Lemma~\ref{thm: toppushk family quadratic update a,a}-\ref{thm: toppushk family quadratic update b,b}.

\subsubsection{Initialization}

For all update rules~\eqref{eq: update rules} we assumed that the current solution~$\bm{\alpha},$~$\bm{\beta}$ is feasible. So to create an iterative algorithm that solves problem~\eqref{eq: Top dual hinge} or~\eqref{eq: Top dual quadratic}, we need to have a way how to initialize the algorithm. Such a task can be formally written as a projection of initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$ to the feasible set of solutions 
\begin{mini}{\bm{\alpha}, \bm{\beta}}{
  \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
  + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
  }{\label{eq: toppushk family initialization}}{}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j}
  \addConstraint{0 \leq \alpha_i}{\leq C_1, \quad i = 1, 2, \ldots, \npos,}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \ntil,}
\end{mini}
where the upper bound in the second constraint depends on the used surrogate function and is defined as follows
\begin{equation*}
  C_1 = \begin{cases}
    C & \text{for hinge loss}, \\
    +\infty & \text{for quadratic hinge loss}.
  \end{cases}
\end{equation*}
To solve problem~\eqref{eq: toppushk family initialization}, we will follow the same approach as in~\cite{adam2020projections}. In the following theorem, we show that problem~\eqref{eq: toppushk family initialization} can be written as a system of two equations of two variables~$(\lambda, \mu).$ Moreover, the theorem shows the concrete form of feasible solution~$\bm{\alpha},$~$\bm{\beta}$ that depends only on~$(\lambda, \mu).$

\begin{restatable}{theorem}{topinit}\label{thm:problem3}
  Consider problem~\eqref{eq: toppushk family initialization} and some initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$ and denote the sorted version (in non-decreasing order) of~$\bm{\beta}^0$ as~$\bm{\beta}_{[\cdot]}^0.$ Then if the following condition holds
  \begin{equation}\label{eq:problem3_cond}
    \sum_{j = 1}^{K} \Brac{\beta_{[\ntil - K + j]}^0 + \max_{i = 1, \ldots, \npos} \alpha_i^0} \le 0,
  \end{equation}
  the optimal solution of~\eqref{eq: toppushk family initialization} amounts to~$\bm{\alpha} = \bm{\beta} = \bm{0}.$ In the opposite case, the following system of two equations
  \begin{subequations}\label{eq:problem3_system}
    \begin{align}
      \sum_{i=1}^{\npos} \clip{0}{C_1}{ \alpha_i^0 - \lambda + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 + \lambda - \mu}} - K \mu
      & = 0, \label{eq:problem3_system1} \\
      \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K\mu
      & = 0, \label{eq:problem3_system2}
    \end{align}
  \end{subequations}
  has a solution $(\lambda, \mu)$ with $\mu > 0,$ and the optimal solution of~\eqref{eq: toppushk family initialization} equals to
  \begin{align*}
    \alpha_i
      & = \clip{0}{C_1}{\alpha_i^0 - \lambda + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 + \lambda - \mu}}, \\
    \beta_j & = \clip{0}{\mu}{\beta_j^0 + \lambda}.
  \end{align*}
\end{restatable}

In the following text, we show that the number of variables in the system of equations~\eqref{eq:problem3_system} can be reduced to one. For any fixed $\mu$, we denote the function on the left-hand side of~\eqref{eq:problem3_system2} by 
\begin{equation*}
  g(\lambda; \mu) := \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K\mu.
\end{equation*}
Then~$g$ is non-decreasing in~$\lambda$ but not necessarily strictly increasing. We denote by~$\lambda(\mu)$ any such~$\lambda$ solving~\eqref{eq:problem3_system2} for a fixed~$\mu$. Denote~$\bm{z}$ the sorted version of~$-\bm{\beta}^0s$. Then we have
\begin{equation*}
  g(\lambda; \mu)
    = \sum_{\Set{j}{\lambda - z_j \in [0, \mu)}}(\lambda - z_j)
    + \sum_{\Set{j}{\lambda - z_j \ge \mu}}\mu - K\mu.
\end{equation*}
Now we can easily compute~$\lambda(\mu)$ by solving~$g(\lambda(\mu); \mu) = 0$ for fixed~$\mu$ using Algorithm~\ref{alg:g3}. The algorithm can  be described as follows: Index~$i$ will run over~$\bm{z}$ while index~$j$ will run over~$\bm{z} + \mu$. At every iteration, we know the values of~$g(z_{i-1}; \mu)$ and~$g(z_{j-1}+\mu; \mu)$ and we want to evaluate~$g$ at the next point. We denote number of indices~$j$ such that $\lambda - z_j \in[0, \mu)$ by~$d$. If~$z_i \le z_j + \mu$, then we consider~$\lambda = z_i$ and since one index enters the set~$\Set{j}{\lambda - z_j \in [0, \mu)}$, we increase~$d$ by one. On the other hand, if~$z_i > z_j + \mu$, then we consider $\lambda = z_j + \mu$ and since one index leaves the set~$\Set{j}{\lambda - z_j \in [0, \mu)}$, we decrease~$d$ by one. In both cases,~$g$ is increased by~$d$ times the difference between the new~$\lambda$ and old~$\lambda$. Once~$g$ exceeds~$0$, we stop the algorithm and linearly interpolate between the last two values. To prevent an overflow, we set~$z_{m+1}=\infty$. Concerning the initial values, since~$z_1 \le z_1 + s\mu$, we set $i=2$, $j=1$ and $d=1$. 

\begin{algorithm}
  \centering
  \begin{algorithmic}[1]
    \Require vector $-\bm{\beta}^0$ sorted into $\bm{z}$
    \State $i \gets 2$, $j \gets 1$, $d \gets 1$
    \State $\lambda \gets z_1$, $g \gets - K\mu$
    \While{$g < 0$}
      \If {$z_i \le z_j + \mu$}
        \State $g \gets g + d(z_i - \lambda)$
        \State $\lambda\gets z_i$, $d \gets d+1$, $i \gets i+1$
      \Else
        \State $g \gets g + d(z_j + \mu - \lambda)$
        \State $\lambda \gets z_j + \mu$, $d \gets d - 1$, $j \gets j + 1$
      \EndIf
    \EndWhile
    \State \textbf{return} linear interpolation of the last two values of $\lambda$
  \end{algorithmic}
  \caption{For computing~$\lambda(\mu)$ from~\eqref{eq:problem3_system}}
  \label{alg:g3}
\end{algorithm}

Since~$\lambda(\mu)$ can be computed for fixed~$\mu$ using Algorithm~\ref{alg:g3}, we can reduce system~\eqref{eq:problem3_system} into one equation
\begin{equation}\label{eq:defin_f3}
  h(\mu)
    := \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu) + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0+\lambda(\mu) - \mu}} - K \mu
    = 0
\end{equation}
which needs to be solved for $\mu$. The following lemma describes properties of~$h.$ Since~$h$ is decreasing in~$\mu$ on~$(0, \infty)$, we can use root finding algorithms such as Bisection or Newton method to find the solution.

\begin{restatable}{lemma}{topinith}\label{lemma:problem3}
  Even though~$\lambda(\mu)$ is not unique, function~$h$ is well-defined in the sense that it gives the same value for every choice of~$\lambda(\mu)$. Moreover,~$h$ is decreasing in~$\mu$ on~$(0, \infty)$.
\end{restatable}

\todo[inline]{Add figures of h and g}

\subsection{Family of \PatMat Formulations}\label{sec: Pat coordinate descent}

In the beginning of this chapter we derived general dual formulation~\eqref{eq: patmat family dual} for the family of \PatMat formulations. Similarly to the dual formulation~\eqref{eq: toppushk family dual}, we can use update rules~\eqref{eq: update rules} to solve this dual formulation. In this case, however, we have to also consider the third primal variable~$\delta.$ Then the dual formulation~\eqref{eq: patmat family dual}  can be rewritten as a quadratic one-dimensional problem
\begin{maxi*}{\Delta}{
  -\frac{1}{2} a(\bm{\alpha}, \bm{\beta}, \delta) \Delta^2
  - b(\bm{\alpha}, \bm{\beta}, \delta) \Delta
  - c(\bm{\alpha}, \bm{\beta}, \delta)
  }{}{}
  \addConstraint{\Delta_{lb}(\bm{\alpha}, \bm{\beta}, \delta)}{\leq \Delta \leq \Delta_{ub}(\bm{\alpha}, \bm{\beta}, \delta)}
\end{maxi*}
where~$a,$~$b,$~$c,$~$\Delta_{lb},$~$\Delta_{ub}$ are constants with respect to~$\Delta.$ The form of the optimal solution is the same as for problem~\eqref{eq: toppushk family dual} and reads
\begin{equation*}
  \Delta^{\star} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma},
\end{equation*}
where~$-\nicefrac{b}{a}.$ Since we assume one of the update rule~\eqref{eq: update rules}, the constrain~\eqref{eq: patmat family dual c1} is always satisfied after the update. The exact form of the update rules depend on the surrogate function. More, the form of optimal~$\delta$ also depends on the surrogate function. In the following text, we derive update rules for hinge loss and quadratic hinge loss function.

\subsubsection{Hinge Loss}

We again start with the hinge loss function from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate hinge} of the hinge loss into the dual formulation from Theorem~\ref{thm: patmat family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  + \frac{1}{\vartheta} \sum_{j = 1}^{\ntil} \beta_j 
  - \delta \ntil \tau
  }{\label{eq: Pat dual hinge}}{\label{eq: Pat dual hinge L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: Pat dual hinge c1}}
  \addConstraint{0 \leq \alpha_i}{\leq C,}{i = 1, 2, \ldots, \npos \label{eq: Pat dual hinge c2}}
  \addConstraint{0 \leq \beta_j}{\leq \delta \vartheta, \quad}{j = 1, 2, \ldots, \ntil \label{eq: Pat dual hinge c3}}
  \addConstraint{\delta }{\geq 0. \label{eq: Pat dual hinge c4}}
\end{maxi!}
This is a convex quadratic problem and can be solved optimized using update rules~\eqref{eq: update rules}. In such a case, we do not perform a joint maximization in~$(\alphak, \; \beta_l, \; \delta)$ but perform a maximization with respect to~$(\alphak, \; \beta_l)$, update these two values and then optimize the objective with respect to~$\delta$. Then for fixed feasible solution~$\bm{\alpha}$ and~$\bm{\beta},$ maximizing objective function~\eqref{eq: Pat dual hinge L} with respect to~$\delta$ yields
\begin{maxi*}{\delta}{
  - \ntil \tau \delta
  }{}{}
  \addConstraint{0 \leq \beta_j}{\leq \delta \vartheta, \quad}{j = 1, 2, \ldots, \ntil}
  \addConstraint{\delta \geq 0.}
\end{maxi*}
Since~$\ntil \tau \geq 0,$ to maximize the objective function with respect to the~$\delta$, we have to find the smallest possible~$\delta$ that satisfies the constrains. Such~$\delta$ is in the following form
\begin{equation}\label{eq: Pat dual hinge optimal delta}
  \delta^* = \frac{1}{\vartheta} \max_{j \in \{1, 2, \ldots, \ntil \}} \beta_j.
\end{equation}
Since the formulas for optimal update rules are rather long, we postpone them to Appendix~\ref{sec: patmat family coordinate proofs}. More specificaly, all update rules can be found in Lemma~\ref{thm: patmat family hinge update a,a}-\ref{thm: patmat family hinge update b,b}.

\subsubsection{Quadratic Hinge Loss}

The second choice of the surrogate function is the quadratic hinge loss function from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate quadratic hinge} of the quadratic hinge loss into the dual formulation from Theorem~\ref{thm: patmat family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  - \frac{1}{4C} \sum_{i = 1}^{\npos} \alpha_i^2
  }{\label{eq: Pat dual quadratic}}{\label{eq: Pat dual quadratic L1}}
  \breakObjective{
    + \frac{1}{\vartheta} \sum_{j = 1}^{\ntil} \beta_j 
    - \frac{1}{4 \delta \vartheta^2} \sum_{j = 1}^{\ntil} \beta_j^2
    - \delta \ntil \tau \label{eq: Pat dual quadratic L2}
  }
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Pat dual quadratic c1}}
  \addConstraint{\alpha_i}{\geq 0,}{i = 1, 2, \ldots, \npos
  \label{eq: Pat dual quadratic c2}}
  \addConstraint{\beta_j}{\geq 0,}{j = 1, 2, \ldots, \ntil
  \label{eq: Pat dual quadratic c3}}
  \addConstraint{\delta }{\geq 0,
  \label{eq: Pat dual quadratic c4}}
\end{maxi!}
Again we get a convex quadratic problem that can be solved optimized using update rules~\eqref{eq: update rules}. In such a case, we again perform maximization only with respect to~$(\alphak, \; \beta_l)$ and we need to maximazie the objective with respect to~$\delta$ separately. For fixed feasible solution~$\bm{\alpha}$ and~$\bm{\beta},$ maximizing objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) with respect to~$\delta$ leads to the following problem
\begin{maxi*}{\delta}{
  - (\ntil \tau) \delta - \Brac{\frac{1}{4\vartheta^2} \sum_{j = 1}^{\ntil} \beta_j^2} \frac{1}{\delta}
  }{}{}
  \addConstraint{\delta \geq 0,}
\end{maxi*}
with the optimal solution that equals to
\begin{equation}\label{eq: Pat dual quadratic optimal delta}
  \delta^* = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \sum_{j = 1}^{\ntil} \beta_j^2}.
\end{equation}
As in the previous section, we postpone the formulas for optimal update rules to Appendix~\ref{sec: patmat family coordinate proofs}. More specificaly, all update rules can be found in Lemma~\ref{thm: patmat family quadratic update a,a}-\ref{thm: patmat family quadratic update b,b}.

\subsubsection{Initialization}

As in the case of problem~\eqref{eq: toppushk family dual}, for all update rules~\eqref{eq: update rules} we assumed that the current solution~$\bm{\alpha},$~$\bm{\beta}$~$\delta$ is feasible. So to create an iterative algorithm that solves problem~\eqref{eq: Pat dual hinge} or~\eqref{eq: Pat dual quadratic}, we need to have a way how to initialize the algorithm. Such a task can be formally written as a projection of initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$~$\delta^0$ to the feasible set of solutions 
\begin{mini}{\bm{\alpha}, \bm{\beta}, \delta}{
  \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
  + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
  + \frac{1}{2} (\delta - \delta^0)^2
  }{\label{eq: patmat family initialization}}{}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j}
  \addConstraint{0 \leq \alpha_i}{\leq C_1, \quad i = 1, 2, \ldots, \npos}
  \addConstraint{0 \leq \beta_j}{\leq C_2 \delta, \quad j = 1, 2, \ldots, \ntil,}
  \addConstraint{\delta }{\geq 0,}
\end{mini}
where the upper bounds in the second and third constraints depend on the used surrogate function and are defined as follows
\begin{align*}
  C_1 & = \begin{cases}
    C & \text{for hinge loss}, \\
    +\infty & \text{for quadratic hinge loss},
  \end{cases} &
  C_2 & = \begin{cases}
    \delta \vartheta & \text{for hinge loss}, \\
    +\infty & \text{for quadratic hinge loss}.
  \end{cases}
\end{align*}
Again, we will follow the same approach as in~\cite{adam2020projections} to solve problem~\eqref{eq: patmat family initialization}. In the following theorem, we show that problem~\eqref{eq: patmat family initialization} can be written as a system of two equations of two variables~$(\lambda, \mu).$ Moreover, the theorem shows the concrete form of feasible solution~$\bm{\alpha},$~$\bm{\beta}$~$\delta$ sthat depends only on~$(\lambda, \mu).$

\begin{restatable}{theorem}{patinit}\label{thm:problem2}
  Consider problem~\eqref{eq: patmat family initialization} and some initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$ and~$\delta^0.$ Then if the following condition holds
  \begin{equation}\label{eq:problem2_cond}
    \delta^0 \le - C_2 \sum_{j = 1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0}.
  \end{equation}
  the optimal solution of~\eqref{eq: toppushk family initialization} amounts to~$\bm{\alpha} = \bm{\beta} = \bm{0}$ and~$\delta^0 = 0.$ In the opposite case, the following system of two equations
  \begin{subequations}\label{eq:problem2_system}
    \begin{align}
    0
      & = \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda} - \sum_{j=1}^{\ntil} \clip{0}{\lambda + \mu}{\beta_j^0 + \lambda},
    \label{eq:problem2_system1} \\
    \lambda
      & = C_2 \delta^0 + C_2^2 \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 - \mu} - \mu.
    \label{eq:problem2_system2}
    \end{align}
  \end{subequations}
  has a solution $(\lambda,\mu)$ with $\lambda+\mu>0$ and the optimal solution of~\eqref{eq: patmat family initialization} equals to
  \begin{align*}
    \alpha_i & = \clip{0}{C_1}{\alpha_i^0 - \lambda}, \\
    \beta_j & = \clip{0}{\lambda + \mu}{\beta_j^0 + \lambda}, \\
    C_2 \delta &= \lambda + \mu.
  \end{align*}
\end{restatable}

System~\eqref{eq:problem2_system} is relatively simple to solve, since equation~\eqref{eq:problem2_system2} provides an explicit formula for~$\lambda$. Let us denote it as $\lambda(\mu)$, then we denote the right-hand side of~\eqref{eq:problem2_system1} as
\begin{equation}\label{eq:defin_f2}
  h(\mu) :=
    \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu)} - \sum_{j=1}^{\ntil} \clip{0}{\lambda(\mu) + \mu}{\beta_j^0 + \mu}.
\end{equation}
finally, system~\eqref{eq:problem2_system} is equivalent to solving~$h(\mu) = 0$.

\begin{restatable}{lemma}{patinith}\label{lemma:problem2}
  Function $h$ is non-decreasing in~$\mu$ on~$(0,\infty)$.
\end{restatable}

The previous lemma states that~$h$ is a non-decreasing functionin~$\mu$ on~$(0,\infty)$ and thus the equation~$h(\mu) = 0$ is simple to solve numerically using any root finding method such as Bisection or Newton method.  Note that if~$\delta^0 < 0$, then it may happen that~$\lambda + \mu < 0$ if the initial~$\mu$ is chosen large. In such a case, it suffices to decrease~$\mu$ until~$\lambda + \mu$ is positive.

\todo[inline]{Add figures of h and g}

\subsection{Complexity analysis}

In the previous sections, we derived dual formulations for two families of problems. Moreover, we showed, that these dual formulations can be solved iteratively using simple update rules~\eqref{eq: update rules}. Since these update rules assume initial feasible solution, we also showed how to find such initial solution. Then the final algorithm can be summarized as in Algorithm~\ref{alg:Coordinate descent}.

The left column in Algorithm~\ref{alg:Coordinate descent} describe the algorithm for \TopPushK family of formulations and the right for \PatMat family of formulations. In step~\ref{alg: line 1} we initialize~$\bm{\alpha}$,~$\bm{\beta}$ and~$\delta$ to some feasible value using Theorem~\ref{thm:problem3} for \TopPushK family and  Theorem~\ref{thm:problem2} for \PatMat family. Then, based on~\eqref{eq: dual scores} we compute~$\bm{s}$. Each \repeatloop loop in step~\ref{alg: line 2} updates two coordinates as shown in~\eqref{eq: update rules}. In step~\ref{alg: line 3} we select a random index~$k$ and in the \forloop loop in step~\ref{alg: line 4} we compute the optimal~$(\Delta_l,\delta_l)$ for all possible combinations~$(k,l)$ as in~\eqref{eq: update rules}. In step~\ref{alg: line 7} we select the best pair~$(\Delta_l,\delta_l)$ which maximizes the coresponding objective function. Finally, based on the selected update rule we update~$\bm{\alpha}$,~$\bm{\beta}$,~$\bm{s}$ and~$\delta$ in steps~\ref{alg: line 8} and~\ref{alg: line 9}.

\begin{algorithm*}[t]
  \begin{minipage}{0.50\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State set~$(\bm{\alpha}, \bm{\beta})$ using Theorem~\ref{thm:problem3}
      \State set~$\bm{s}$ based on~\eqref{eq: dual scores} \label{alg: line 1}
      \Repeat \label{alg: line 2}
        \State random~$k$ from~$\{1, 2, \ldots, \npos+ \ntil \}$ \label{alg: line 3}
        \For{$l \in \{1, 2, \ldots, \npos + \ntil  \}$} \label{alg: line 4}
            \State compute~$\Delta_{l}$  \label{alg: line 5}
        \EndFor
        \State select best~$\Delta_{l}$ \label{alg: line 7}
        \State update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq: update rules} \label{alg: line 8}
        \State \label{alg: line 9}
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \hfill
  \begin{minipage}{0.50\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State set~$(\bm{\alpha}, \bm{\beta}, \delta)$ using Theorem~\ref{thm:problem2}
      \State set~$\bm{s}$ based on~\eqref{eq: dual scores}
      \Repeat
        \State random~$k$ from~$\{1, 2, \ldots, \npos + \ntil \}$ 
        \For{$l \in \{1, 2, \ldots, \npos + \ntil \}$}
            \State compute~$(\Delta_{l}, \; \delta_{l})$
        \EndFor
        \State select best~$(\Delta_{l}, \; \delta_{l})$
        \State update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq: update rules}
        \State set~$\delta \leftarrow \delta_{l}$
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \caption{Coordinate descent algorithm for \TopPushK family of formulations (left) and \PatMat  family of formulations (right).}
  \label{alg:Coordinate descent}
\end{algorithm*}

Now we derive the computational complexity of each \repeatloop loop from step~\ref{alg: line 2}. The computation of~$(\Delta_l,\delta_l)$ amounts to solving a quadratic optimization problem in one variable. As we showed in Sections~\ref{sec: Top coordinate descent} and~\ref{sec: Pat coordinate descent}, there is a closed-form solution and step~\ref{alg: line 5} can be performed in~$O(1)$. Since this is embedded in a \forloop loop in step~\ref{alg: line 4}, the whole complexity of this loop is~$O(\npos + \ntil)$. Step~\ref{alg: line 8} requires~$O(1)$ for the update of~$\bm{\alpha}$ and~$\bm{\beta}$ while~$O(\npos + \ntil)$ for the update of~$\bm{s}$. Since the other steps are~$O(1)$, the total complexity of the \repeatloop loop is~$O(\npos + \ntil)$. This holds true only if the kernel matrix~$\K$ is precomputed. In the opposite case, all complexities must by multiplied by the cost of computation of components of~$\K$ which is~$O(d)$. This complexity analysis is summarized in Table~\ref{tab:Computational complexity}. 

\begin{table}[h]
  \centering
  \begin{NiceTabular}{lcc}
    \CodeBefore
      \rowcolor{\headercol}{1}
      \rowcolors{3}{\rowcol}{}[restart]
    \Body
    \toprule
    \Block[c]{1-1}{Operation}
      & $\K$ precomputed
      & $\K$ not precomputed \\
    \midrule
    Evaluation of~$\Delta_l$
      & $O(1)$
      & $O(d)$ \\
    Update of~$\bm{\alpha}$ and~$\bm{\beta}$
      & $O(1)$
      & $O(1)$ \\
    Update of~$\bm{s}$
      & $O(\npos + \ntil)$
      & $O((\npos + \ntil)d)$ \\
    \midrule
    Total per iteration
      & $O(\npos + \ntil)$
      & $O((\npos + \ntil)d)$ \\
    \bottomrule
  \end{NiceTabular}
  \caption{Computational complexity of one \repeatloop loop (which updates two coordinates of~$\bm{\alpha}$ or~$\bm{\beta}$) from Algorithm~\ref{alg:Coordinate descent}.}
  \label{tab:Computational complexity}
\end{table}