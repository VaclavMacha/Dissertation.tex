\chapter{Non-linear Classification at the Top}\label{chap: dual}

In the Chapter~\ref{chap: framework}, we introduced general framework for binary classification at the top. Moreover, we showed that several problem classes, which were considered as separate problems so far, fit into the framework. As an example we can mention ranking problems of hypoothesis testing. Summary of all formulations is in Table~\ref{tab: summary formulations}. In the Chapter~\ref{chap: linear} disscused special case, when the linear classifier is used. In such a case, many of the formulations from Table~\ref{tab: summary formulations} have nice theoretical properties such as convexity or differentiability. However, as many problems are not linearly separable, nonlinear classifiers are needed. In this chapter, we show how to extend our framework into nonlinear classification problems. To do so, we use the fact that our framework is similar to the primal formulation of support vector machines~\cite{cortes1995support}. The classical way to incorporate nonlinearity into SVM is to derive the dual formulation~\cite{boyd2004convex} and to employ the kernels method~\cite{scholkopf2001learning}. In this chapter, we follow this approach, derive dual formulations for the considered problems and add nonlinear kernels to them. Moreover, as dual problems are generally expensive to solve, we derive a quick method to solve them. This is a modification of the coordinate-wise dual ascent from~\cite{hsieh2008dual}. For a review of other approaches see~\cite{batmaz2019review,werner2019review}.

\section{Derivation of dual problems}\label{sec:Derivation of dual problems}

In this section, we derive dual forms of formulations from Table~\ref{tab: summary formulations}. Since many of these formulations are very similar, we divide them into two families. The first one is a family of \TopPushK formulations that consists \TopPush, \TopPushK, \TopMeanK and \tauFPL formulations. All these formulations use false-negative rate as an objective function and the decision threshold is a mean of~$K$ largest scores of all or negative samples. The second family is a family of \PatMat formulations that consists \PatMat and \PatMat formulations. Also these two formulations use false-negative rate as an objective function, but the decision threshold is a surrogate approximation of top $\tau$-quantile of scores of all or negative samples. In other words, we have two families that share the same objective function and the form of the decision threshold, even thogh the decision threshold may be computed from different samples.  

\pagebreak

\begin{notation}[Kernel Matrix]\label{not: kernel matrix}
  To simplify the future notation, we use matrix~$\X$ of all samples with rows defined for all~$i \in \I$ as 
  \begin{equation*}
    \X_{i, \bullet} = \bm{x}_i^{\top}.
  \end{equation*}
  In other words, each row of~$\X$ represents one sample. Similarly, we defined matrices~$\X^+,$~$\X^-$ of all negative and positive samples with rows defined as
  \begin{align*}
    \X^{-}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^-, \\
    \X^{+}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^+.
  \end{align*}
  Moreover, for \TopPush, \TopPushK, \tauFPL and \PatMatNP formulations we define the positive semidefinite kernel matrix~$\Kneg$ as
  \begin{equation*}
    \Kneg = \Matrix{\X^+ \\ - \X^-} \Matrix{\X^+ \\ - \X^-}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} }.
  \end{equation*}
  Simlarly, for \TopMeanK and \PatMat formulations we define the positive semidefinite kernel matrix~$\Kall$ as
  \begin{equation*}
    \Kall = \Matrix{\X^+ \\ - \X} \Matrix{\X^+ \\ - \X}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{\top} \\ -\X \X^{+\top} & \X \X^{\top} }.
  \end{equation*}
\end{notation}

\subsection{Family of \TopPushK formulations}

As we mentioned before, the first family is a family of \TopPushK formulations that consists \TopPush, \TopPushK, \TopMeanK and \tauFPL formulations. All these formulations can be written as follows
\begin{mini}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \Ipos} l(t - \bm{w}^{\top} \bm{x}_i)
  }{\label{eq: toppushK family}}{}
  \addConstraint{\tilde{s}_j}{= \bm{w}^{\top} \bm{x}_j, \quad j \in \Itil}
  \addConstraint{t}{= \sum_{j = 1}^{K} \tilde{s}_{[j]},}
\end{mini}
where~$C \in \R$ is a constant and~$\Itil$ and~$K$ is defined as follows
\begin{align*}
  \Itil & = \begin{cases}
    \I & \quad \text{for \TopMeanK}, \\
    \Ineg & \quad \text{otherwise}. \\
  \end{cases} &
  K & = \begin{cases}
    1 & \quad \text{for \TopPush}, \\
    \nall \tau & \quad \text{for \TopMeanK}, \\
    \nneg \tau & \quad \text{for \tauFPL}. \\
  \end{cases}
\end{align*}
It means, that for \TopMeanK, the threshold is computed from all samples and otherwise only from negative ones. Also note, that we use linear classifier and we also use this alternative formulation with constant~$C,$ since it is more similar to the standard SVM. The following theorem show the dual formulation of~\eqref{eq: toppushK family}. To keep the readability as simple as possible, we postpone all proofs to the Appendix~\ref{app: dual}.

\begin{restatable}[Dual formulation for \TopPushK family]{theorem}{topdual}\label{thm: toppushk family dual}
  Consider formulations \TopPush, \TopPushK, \TopMeanK and \tauFPL from Table~\ref{tab: summary formulations} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
    - \frac{1}{2} \vecab^\top \K \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    }{\label{eq: toppushk family dual}}{\label{eq: toppushk family dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: toppushk family dual c1}}
    \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \ntil, \label{eq: toppushk family dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l$ and
  \begin{align*}
    \K & = \begin{cases}
      \Kall & \quad \text{for \TopMeanK}, \\
      \Kneg & \quad \text{otherwise}, \\
    \end{cases} &
    \ntil & = \begin{cases}
      \nall & \quad \text{for \TopMeanK}, \\
      \nneg & \quad \text{otherwise}. \\
    \end{cases}
  \end{align*} 
  Moreover, the variable~$K$ is defined as follows
  \begin{equation*}
    K = \begin{cases}
      1 & \quad \text{for \TopPush}, \\
      \nall \tau & \quad \text{for \TopMeanK}, \\
      \nneg \tau & \quad \text{for \tauFPL}. \\
    \end{cases}
  \end{equation*}
  Finally, if~$K = 1,$ the upper bound in the second constrainet vanishes due to the first constraint.
\end{restatable}

\subsection{Family of \PatMat formulations}

Similarly to the previous section, we introduce the family of \PatMat formulations that consists of \PatMat and \PatMatNP formulations and can be written as follows
\begin{mini}{\bm{w}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \Ipos} l(t - \bm{w}^{\top} \bm{x}_i)
  }{\label{eq: patmat family}}{}
  \addConstraint{t}{\quad \text{solves} \quad \frac{1}{\ntil}\sum_{i \in \Itil} l\Brac{\vartheta(\bm{w}^{\top} \bm{x}_j - t)} = \tau.}
\end{mini}
where~$C \in \R$ is a constant and~$\Itil$ and~$\ntil$ is defined as follows
\begin{align*}
  \Itil & = \begin{cases}
    \I & \quad \text{for \PatMat}, \\
    \Ineg & \quad \text{otherwise}. \\
  \end{cases} &
  \ntil & = \begin{cases}
    \nall & \quad \text{for \PatMat}, \\
    \nneg & \quad \text{otherwise}. \\
  \end{cases} &
\end{align*}
Again, we use linear classifier and the alternative formulation with constant~$C.$ The following theorem show the dual formulation of~\eqref{eq: patmat family}. 

\begin{restatable}[Dual formulation for \PatMat family]{theorem}{patdual}\label{thm: patmat family dual}
  Consider formulations \PatMat and \PatMatNP from Table~\ref{tab: summary formulations} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
    - \frac{1}{2} \vecab^\top \K \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    - \delta \sum_{j = 1}^{\ntil} l^{\star} \Brac{\frac{\beta_j}{\delta\vartheta }}
    - \delta \ntil \tau
    }{\label{eq: patmat family dual}}{\label{eq: patmat family dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: patmat family dual c1}}
    \addConstraint{\delta }{\geq 0, \label{eq: patmat family dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l,$~$\vartheta > 0$ is a scaling parameter and
  \begin{align*}
    \K & = \begin{cases}
      \Kall & \quad \text{for \PatMat}, \\
      \Kneg & \quad \text{otherwise}, \\
    \end{cases} &
    \ntil & = \begin{cases}
      \nall & \quad \text{for \PatMat}, \\
      \nneg & \quad \text{otherwise}. \\
    \end{cases}
  \end{align*}
\end{restatable}

\subsection{Adding kernels}

As we mentioned in the beggining of the chapter, our goal is to to extend our framework into nonlinear classification problems. In the previous sections we derived dual formulations for the \TopPushK and \PatMat family of formulations. In this sections we show, how to employ the kernels method~\cite{scholkopf2001learning} to introduce nonlinearity into the formulations. Firstly, consider any formulation that computes the decision threshold only from negative samples and therefore uses~$\Kneg$ as a kernel matrix. To add kernels, we first realize that the classification score~$s_j$ for any sample~$\bm{x}_j \in \R^d$ is given by 
\begin{equation}\label{eq:pred_linear}
  s_j
    = \bm{w}^{\top} \bm{x}_j
    = \sum_{i = 1}^{\npos} \alpha_i \bm{x}_j^{\top} \bm{x}_i^+ - \sum_{j = 1}^{\nneg} \beta_j \bm{x}_j^{\top} \bm{x}_j^-,
\end{equation}
where~$\bm{\alpha} \in \R^{\npos},$~$\bm{\beta} \in \R^{\nneg}$ are dual variables. This relation yields from the proofs of Theorems~\ref{thm: toppushk family dual} and~\ref{thm: patmat family dual} and holds for any considered formulation. Consider now any kernel function~$k: \R^d \times \R^d \to \R.$ Then the first part of the objective function of~\eqref{eq: toppushk family dual} and~\eqref{eq: patmat family dual} amounts to
\begin{equation*}
  \vecab^\top \K \vecab
  = \vecab^\top \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} } \vecab
  = \Matrix{\bm{\alpha} \\ -\bm{\beta}}^\top \Matrix{\X^+ \X^{+\top} & \X^+ \X^{-\top} \\ \X^- \X^{+\top} & \X^- \X^{-\top} }\Matrix{\bm{\alpha} \\ -\bm{\beta}}.
\end{equation*}
Using the standard trick, we can replace the kernel matrix~$\Kneg$
\begin{equation}\label{eq:kernel_nonlinear}
  \Kneg = \Matrix{k(\X^+, \X^{+}) & -k(\X^+, \X^{-}) \\ -k(\X^-, \X^{+}) & k(\X^-, \X^{-}) },
\end{equation}
where~$k(\cdot,\; \cdot)$ is applied to all rows of both arguments. Then for any sample~$\bm{x}_j$, the classification score~\eqref{eq:pred_linear} is replaced by
\begin{equation*}
  s_j = \sum_{i = 1}^{\npos} \alpha_i k\Brac{\bm{x}_j, \bm{x}^+_i} - \sum_{j = 1}^{\nneg} \beta_j k\Brac{\bm{x}_j, \bm{x}^-_j}.
\end{equation*}
Similarly, we can get the formula for classification scores fo formulations that use the decision threshold computed from all samples. 

\section{New Coordinate Descent Algorithm}\label{sec: coordinate descent}

In the previous sections, we showed that that dual formulations of \TopPush, \TopPushK, \TopMeanK and \tauFPL are very similary and can be written in general form summarized in Theorem~\ref{thm: toppushk family dual}. Similarly, dual formulations of \PatMat and \PatMatNP are very similary and can be written in general form summarized in Theorem~\ref{thm: patmat family dual}. We also showed, that these dual formulations  allow us to incorporate nonlinearity using kernels~\cite{scholkopf2001learning} in the same way as in SVM. However, their dimension is at least equal to the number of all samples~$n$ and therefore it is computationally expensive to use standard techniques such as the gradient descent. To handle this issue, the coordinate descent algorithm~\cite{chang2008coordinate,hsieh2008dual} has been proposed in the context of SVMs. Our goal in thsi section is to derived coordinate descent algorithm suitable for dual problems~(\ref{eq: toppushk family dual},~\ref{eq: patmat family dual}). Since these problems differ from original SVMs by additional constraints (\ref{eq: toppushk family dual c1},~\ref{eq: patmat family dual c1}), the key idea of our algorithm is to update two coordinates (instead of one) of dual bariables~$\bm{\alpha},$~$\bm{\beta}$ at every iteration. It will allow us to derive iterative procedure where in every iteration we need to find a solution of a one-dimensional quadratic optimization problem. As we will show later, these one-dimensional problems have a closed form solution, which means that every iteration is cheap.

\subsection{Family of \TopPushK Formulations}\label{sec: Top coordinate descent}

Consider dual formulation~\eqref{eq: toppushk family dual} from Theorem~\ref{thm: toppushk family dual} and fixed feasible dual variables~$\bm{\alpha},$~$\bm{\beta}.$ Let us define vector of scores~$\bm{s}$ by
\begin{equation}\label{eq: dual scores}
  \bm{s} = \K \vecab.
\end{equation}
As we said before, dual formulation~\eqref{eq: toppushk family dual} differs from original SVMs by additional constraintst~\eqref{eq: toppushk family dual c1}. Due to this constraint, we always have to update (at least) two coordinates of dual variables~$\bm{\alpha},$~$\bm{\beta}$ to not violate the constraintst~\eqref{eq: toppushk family dual c1}. Moreover, there are only three update rules which modify two coordinates of~$\bm{\alpha},$~$\bm{\beta}$ and which satisfy constraints~\eqref{eq: toppushk family dual c1} and keep~\eqref{eq: dual scores} satisfied. The first one updates two components of~$\bm{\alpha}$
\begin{subequations}\label{eq: update rules}
\begin{align}\label{eq: update rule a,a}
  \alphak & \to \alphak + \Delta, & \quad
  \alphal & \to \alphal - \Delta, & \quad
  \bm{s} & \to \bm{s} + \Brac{\K_{\bullet, k} - \K_{\bullet, l}}\Delta,
\end{align}
where~$K_{\bullet, i}$ denotes~$i$-th column of~$\K.$ Note that the update rule for~$\bm{s}$ does not use matrix multiplication but only vector addition. The second rule updates one component of~$\bm{\alpha}$ and one component of~$\bm{\beta}$ 
\begin{align}\label{eq: update rule a,b}
  \alphak & \to \alphak + \Delta, & \quad
  \betal  & \to \betal  + \Delta, & \quad
  \bm{s} & \to \bm{s} + \Brac{\K_{\bullet, k} + \K_{\bullet, l}}\Delta,
\end{align}
and the last one updates two components of~$\bm{\beta}$
\begin{align}\label{eq: update rule b,b}
  \betak & \to \betak + \Delta, & \quad
  \betal & \to \betal - \Delta, & \quad
  \bm{s}  & \to \bm{s} + \Brac{\K_{\bullet, k} - \K_{\bullet, l}}\Delta.
\end{align}
\end{subequations}
Using any of the update rules defined above, the problem~\eqref{eq: toppushk family dual} can be written as a one-dimensional quadratic problem with respect to~$\Delta$
\begin{maxi*}{\Delta}{
  -\frac{1}{2} a(\bm{\alpha}, \bm{\beta}) \Delta^2
  - b(\bm{\alpha}, \bm{\beta}) \Delta
  - c(\bm{\alpha}, \bm{\beta})
  }{}{}
  \addConstraint{\Delta_{lb}(\bm{\alpha}, \bm{\beta})}{\leq \Delta \leq \Delta_{ub}(\bm{\alpha}, \bm{\beta})}
\end{maxi*}
where~$a,$~$b,$~$c,$~$\Delta_{lb},$~$\Delta_{ub}$ are constants with respect to~$\Delta.$ The optimal solution to this problem is
\begin{equation}\label{eq: Delta optimal}
  \Delta^{\star} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma},
\end{equation}
where~$-\nicefrac{b}{a}$ and~$\clip{a}{b}{x}$ amounts to clipping (projecting)~$x$ ti interval~$[a, b].$ Since we assume one of the update rule~\eqref{eq: update rules}, the constrain~\eqref{eq: toppushk family dual c1} is always satisfied after the update. Evethough all three update update rules hold true for any surrogate, the calculation of the optimal~$\Delta^{\star}$ depends on the used surrogate function. In the following text, we show the closed-form formula for~$\Delta^{\star},$ when the hinge loss or quadratic hinge loss is used as surrogate.

\begin{notation}\label{not: dual update rules}
  Consider any index~$l$ that satisfies~$1 \leq l \leq \npos + \ntil.$ Since the length of~$\bm{\alpha}$ is always~$\npos,$ we define auxiliary index~$\hat{l}$ as 
  \begin{equation*}
    \hat{l} = \begin{cases}
      l & \text{if } l \leq \npos, \\
      l - \npos & \text{otherwise}.
    \end{cases}
  \end{equation*}
  Then the index~$l$ without hat can be safely used for kernel matrix~$\K$ or vector of scores~$\bm{s}$ while its corresponding version with hat~$\hat{l}$ for~$\bm{\alpha}$ or~$\bm{\beta}.$
\end{notation}

\subsubsection{Hinge loss}

Plugging the conjugate~\eqref{eq: conjugate hinge} of the hinge loss into the dual formulation from Theorem~\ref{thm: toppushk family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  }{\label{eq: Top dual hinge}}{\label{eq: Top dual hinge L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Top dual hinge c1}}
  \addConstraint{0 \leq \alpha_i}{\leq C,}{i = 1, 2, \ldots, \npos
  \label{eq: Top dual hinge c2}}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad}{j = 1, 2, \ldots, \ntil,
  \label{eq: Top dual hinge c3}}
\end{maxi!}
Moreover, for~$K = 1,$ the upper limit in~\eqref{eq: Top dual hinge c3} is always satisfied due to~\eqref{eq: Top dual hinge c1} and the problem can be simplified. The following three lemmas provide formulas for optimal~$\Delta$ for each of update rules~\eqref{eq: update rules}.

\begin{restatable}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Top dual hinge}]{lemma}{topruleaa}\label{thm: toppushk family hinge update a,a}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule a,a}, indices~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \max\{- \alphak,\; \alphal - C\}, \\
    \Delta_{ub} & = \min\{C - \alphak,\; \alphal \}, \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{restatable}

\begin{restatable}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Top dual hinge}]{lemma}{topruleab}\label{thm: toppushk family hinge update a,b}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule a,b}, indices~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Let us define
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        \max \Brac[c]{- \alphak, \;  -\betal} & K = 1, \\
        \max \Brac[c]{- \alphak, \;  -\betal, \; K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
          C - \alphak & K = 1, \\
          \min \Brac[c]{C - \alphak, \; \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}}  & \textrm{otherwise}.
      \end{cases*} \\
    \gamma & = - \frac{s_k + s_l - 1}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}.
  \end{align*}
\end{restatable}

\begin{restatable}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Top dual hinge}]{lemma}{toprulebb}\label{thm: toppushk family hinge update b,b}
  Consider problem~\eqref{eq: Top dual hinge}, update rule~\eqref{eq: update rule b,b}, indices~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        - \betak & K = 1, \\
        \max \Brac[c]{- \betak,\; \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        \betal & K = 1, \\
        \min \Brac[c]{\frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak,\; \betal} & \textrm{otherwise}.
      \end{cases*} \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{restatable}

\subsubsection{Quadratic hinge loss}

Plugging the conjugate~\eqref{eq: conjugate hinge} of the quadratic hinge loss into the dual formulation from Theorem~\ref{thm: toppushk family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  - \frac{1}{4C} \sum_{i = 1}^{\npos} \alpha_i^2
  }{\label{eq: Top dual quadratic}}{\label{eq: Top dual quadratic L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Top dual quadratic c1}}
  \addConstraint{0 \leq \alpha_i}{,}{i = 1, 2, \ldots, \npos
  \label{eq: Top dual quadratic c2}}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad}{j = 1, 2, \ldots, \ntil,
  \label{eq: Top dual quadratic c3}}
\end{maxi!}
Moreover, for~$K = 1,$ the upper limit in~\eqref{eq: Top dual quadratic c3} is always satisfied due to~\eqref{eq: Top dual quadratic c1} and the problem can be simplified. The following three lemmas provide formulas for optimal~$\Delta$ for each of update rules~\eqref{eq: update rules}.

\subsubsection{Initialization}

\begin{mini}{\bm{\alpha}, \bm{\beta}}{
  \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
  + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
  }{\label{eq: toppushk family initialization}}{}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j}
  \addConstraint{0 \leq \alpha_i}{\leq C, \quad i = 1, 2, \ldots, \npos,}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \ntil,}
\end{mini}

\begin{restatable}{theorem}{topinit}\label{thm:problem3}
  Consider problem~\eqref{eq: toppushk family initialization} and some initial solution~$\bm{\alpha}^0$ and~$\bm{\beta}^0.$ Denote~$\bm{\beta}_{[\cdot]}^0$ the sorted version of~$\bm{\beta}^0$. Then the optimal solution of~\eqref{eq: toppushk family initialization} amounts to~$\bm{\alpha} = \bm{\beta} = \bm{0}$ if
  \begin{equation}\label{eq:problem3_cond}
    \sum_{j = 1}^{K} \Brac{\beta_{[\ntil - K + j]}^0 + \max_{i = 1, \ldots, \npos} p_i^0} \le 0.
  \end{equation}
  In the opposite case, the following system of two equations
  \begin{subequations}\label{eq:problem3_system}
    \begin{align}
      \sum_{i=1}^{\npos} \clip{0}{C_1}{ \alpha_i^0 - \lambda + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 + \lambda - \mu}} - K \mu
      & = 0, \label{eq:problem3_system1} \\
      \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K\mu
      & = 0, \label{eq:problem3_system2}
    \end{align}
  \end{subequations}
  has a solution $(\lambda, \mu)$ with $\mu > 0,$ and the optimal solution of~\eqref{eq: toppushk family initialization} equals to
  \begin{align*}
    \alpha_i
      & = \clip{0}{C_1}{\alpha_i^0 - \lambda + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 + \lambda - \mu}}, \\
    \beta_j & = \clip{0}{\mu}{\beta_j^0 + \lambda}.
  \end{align*}
\end{restatable}

\noindent For any fixed $\mu$, we denote the function on the left-hand side of~\eqref{eq:problem3_system2} by 
\begin{equation*}
  g(\lambda; \mu) := \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K\mu.
\end{equation*}
Then~$g$ is non-decreasing in~$\lambda$ but not necessarily strictly increasing. We denote by~$\lambda(\mu)$ any such~$\lambda$ solving~\eqref{eq:problem3_system2} for a fixed~$\mu$. Then we reduce system~\eqref{eq:problem3_system} into one equation
\begin{equation}\label{eq:defin_f3}
  h(\mu)
    := \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu) + \frac{1}{K} \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0+\lambda(\mu) - \mu}} - K \mu
    = 0
\end{equation}
which needs to be solved for $\mu$. As the next results states, there are fast algorithms which provably find the solution.

\begin{restatable}{lemma}{topinith}\label{lemma:problem3}
  Even though~$\lambda(\mu)$ is not unique, function~$h$ is well-defined in the sense that it gives the same value for every choice of~$\lambda(\mu)$. Moreover,~$h$ is decreasing in~$\mu$ on~$(0, \infty)$.
\end{restatable}

\subsection{Family of \PatMat Formulations}\label{sec: Pat coordinate descent}

In the beginning of this subsection we derived problem~\eqref{eq: Pat dual quadratic}. Similarly to the dual formulation from Theorem~\ref{thm: toppushk family dual}, this dual formulation can be rewritten rewritten as a quadratic one-dimensional problem with respect to~$\Delta$ for any of the update rules~\eqref{eq: update rules}. In this case, however, we have to also consider the third primal variable~$\delta$
\begin{maxi*}{\Delta}{
  -\frac{1}{2} a(\bm{\alpha}, \bm{\beta}, \delta) \Delta^2
  - b(\bm{\alpha}, \bm{\beta}, \delta) \Delta
  - c(\bm{\alpha}, \bm{\beta}, \delta)
  }{}{}
  \addConstraint{\Delta_{lb}(\bm{\alpha}, \bm{\beta}, \delta)}{\leq \Delta \leq \Delta_{ub}(\bm{\alpha}, \bm{\beta}, \delta)}
\end{maxi*}
where~$a,$~$b,$~$c,$~$\Delta_{lb},$~$\Delta_{ub}$ are constants with respect to~$\Delta.$ The optimal solution to this problem is
\begin{equation*}
  \Delta^{\star} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma},
\end{equation*}
where~$-\nicefrac{b}{a}.$ Since we assume one of the update rule~\eqref{eq: update rules}, the constrain~\eqref{eq: toppushk family dual c1} is always satisfied after the update.

\subsubsection{Hinge Loss}

Similarly to the previous sections, Plugging the conjugate~\eqref{eq: conjugate hinge} of the hinge loss into the dual formulation from Theorem~\ref{thm: patmat family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  + \frac{1}{\vartheta} \sum_{j = 1}^{\ntil} \beta_j 
  - \delta \ntil \tau
  }{\label{eq: Pat dual hinge}}{\label{eq: Pat dual hinge L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: Pat dual hinge c1}}
  \addConstraint{0 \leq \alpha_i}{\leq C,}{i = 1, 2, \ldots, \npos \label{eq: Pat dual hinge c2}}
  \addConstraint{0 \leq \beta_j}{\leq \delta \vartheta, \quad}{j = 1, 2, \ldots, \ntil \label{eq: Pat dual hinge c3}}
  \addConstraint{\delta }{\geq 0. \label{eq: Pat dual hinge c4}}
\end{maxi!}
This is again a convex quadratic problem. The following three lemmas provide formulas for optimal~$\Delta$ for each of update rules~\eqref{eq: update rules}.

For fixed~$\bm{\alpha}$ and~$\bm{\beta},$ maximizing objective function~\eqref{eq: Pat dual hinge L} with respect to~$\delta$ leads to the
\begin{maxi*}{\delta}{
  - (\ntil \tau) \delta
  }{}{}
  \addConstraint{0 \leq \beta_j}{\leq \delta \vartheta, \quad}{j = 1, 2, \ldots, \ntil}
  \addConstraint{\delta \geq 0.}
\end{maxi*}
If we want to maximize the objective function with respect to the~$\delta$, we have to find the smallest possible~$\delta$ that satisfies the constrains, which is in the following form
\begin{equation}\label{eq: Pat dual hinge optimal delta}
  \delta^* = \frac{1}{\vartheta} \max_{j \in \{1, 2, \ldots, \ntil \}} \beta_j.
\end{equation}
In the following list, we discuss each of update rules~\eqref{eq: update rules}:

\subsubsection{Quadratic Hinge Loss}

Plugging the conjugate~\eqref{eq: conjugate quadratic hinge} of the quadratic hinge loss into the dual formulation from Theorem~\ref{thm: patmat family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  - \frac{1}{4C} \sum_{i = 1}^{\npos} \alpha_i^2
  }{\label{eq: Pat dual quadratic}}{\label{eq: Pat dual quadratic L1}}
  \breakObjective{
    + \frac{1}{\vartheta} \sum_{j = 1}^{\ntil} \beta_j 
    - \frac{1}{4 \delta \vartheta^2} \sum_{j = 1}^{\ntil} \beta_j^2
    - \delta \ntil \tau \label{eq: Pat dual quadratic L2}
  }
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Pat dual quadratic c1}}
  \addConstraint{\alpha_i}{\geq 0,}{i = 1, 2, \ldots, \npos
  \label{eq: Pat dual quadratic c2}}
  \addConstraint{\beta_j}{\geq 0,}{j = 1, 2, \ldots, \ntil
  \label{eq: Pat dual quadratic c3}}
  \addConstraint{\delta }{\geq 0,
  \label{eq: Pat dual quadratic c4}}
\end{maxi!}
This is again a convex quadratic problem. The following theorem provides a formula for the optimal step~$\Delta^\star$ for the update rule~\eqref{eq: update rules}. Note that we do not perform a joint minimization in~$(\alphak, \; \beta_l, \; \delta)$ but perform a minimization with respect to~$(\alphak, \; \beta_l)$, update these two values and then optimize the objective with respect to~$\delta$. 

For fixed~$\bm{\alpha}$ and~$\bm{\beta},$ maximizing objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) with respect to~$\delta$ leads to the
\begin{maxi*}{\delta}{
  - (\ntil \tau) \delta - \Brac{\frac{1}{4\vartheta^2} \sum_{j = 1}^{\ntil} \beta_j^2} \frac{1}{\delta}
  }{}{}
  \addConstraint{\delta \geq 0.}
\end{maxi*}
The solution of this problem equals to
\begin{equation}\label{eq: Pat dual quadratic optimal delta}
  \delta^* = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \sum_{j = 1}^{\ntil} \beta_j^2}.
\end{equation}
In the following list, we discuss each of update rules~\eqref{eq: update rules}:

\subsubsection{Initialization}

\begin{mini}{\bm{\alpha}, \bm{\beta}, \delta}{
  \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
  + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
  + \frac{1}{2} (\delta - \delta^0)^2
  }{\label{eq: patmat family initialization}}{}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j}
  \addConstraint{0 \leq \alpha_i}{\leq C_1, \quad i = 1, 2, \ldots, \npos}
  \addConstraint{0 \leq \beta_j}{\leq C_2 \delta, \quad j = 1, 2, \ldots, \ntil,}
  \addConstraint{\delta }{\geq 0,}
\end{mini}

\begin{restatable}{theorem}{patinit}\label{thm:problem2}
  Consider problem~\eqref{eq: patmat family initialization} and some initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$ and~$\delta^0.$ Then the optimal solution of~\eqref{eq: toppushk family initialization} amounts to~$\bm{\alpha} = \bm{\beta} = \bm{0}$ and~$\delta^0 = 0$ if
  \begin{equation}\label{eq:problem2_cond}
    \delta^0 \le - C_2 \sum_{j = 1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0}.
  \end{equation}
  In the opposite case, the following system of two equations
  \begin{subequations}\label{eq:problem2_system}
    \begin{align}
    0
      & = \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda} - \sum_{j=1}^{\ntil} \clip{0}{\lambda + \mu}{\beta_j^0 + \lambda},
    \label{eq:problem2_system1} \\
    \lambda
      & = C_2 \delta^0 + C_2^2 \sum_{j=1}^{\ntil} \clip[u]{0}{\infty}{\beta_j^0 - \mu} - \mu.
    \label{eq:problem2_system2}
    \end{align}
  \end{subequations}
  has a solution $(\lambda,\mu)$ with $\lambda+\mu>0$ and the optimal solution of~\eqref{eq: patmat family initialization} equals to
  \begin{align*}
    \alpha_i & = \clip{0}{C_1}{\alpha_i^0 - \lambda}, \\
    \beta_j & = \clip{0}{\lambda + \mu}{\beta_j^0 + \lambda}, \\
    C_2 \delta &= \lambda + \mu.
  \end{align*}
\end{restatable}

System~\eqref{eq:problem2_system} is relatively simple to solve. Equation~\eqref{eq:problem2_system2} provides an explicit formula for~$\lambda$, let us denote it as $\lambda(\mu)$. As in the previous section, denote
\begin{equation}\label{eq:defin_f2}
  h(\mu) :=
    \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu)} - \sum_{j=1}^{\ntil} \clip{0}{\lambda(\mu) + \mu}{\beta_j^0 + \mu}.
\end{equation}
System~\eqref{eq:problem2_system} is equivalent to solving~$h(\mu) = 0$. The next results states that~$h$ is a non-decreasing function and thus the equation~$h(\mu) = 0$ is simple to solve.

\begin{restatable}{lemma}{patinith}\label{lemma:problem2}
  Function $h$ is non-decreasing in~$\mu$ on~$(0,\infty)$.
\end{restatable}
  
\noindent The equation~$h(\mu) = 0$ needs to be solved numerically. Note that if~$\delta^0 < 0$, then it may happen that~$\lambda + \mu < 0$ if the initial~$\mu$ is chosen large. In such a case, it suffices to decrease~$\mu$ until~$\lambda + \mu$ is positive.

\subsection{Complexity analysis}

We summarize the whole procedure in Algorithm~\ref{alg:Coordinate descent}. The left column describe the algorithm for \TopPushK family of formulations and the right for \PatMat family of formulations. In step~\ref{alg: line 1} we initialize~$\bm{\alpha}$,~$\bm{\beta}$ and~$\delta$ to some feasible value and based on~\eqref{eq: dual scores} compute~$\bm{s}$. Each \repeatloop loop in step~\ref{alg: line 2} updates two coordinates as shown in~\eqref{eq: update rules}. In step~\ref{alg: line 3} we select a random index~$k$ and in the \forloop loop in step~\ref{alg: line 4} we compute the optimal~$(\Delta_l,\delta_l)$ for all possible combinations~$(k,l)$ as in~\eqref{eq: update rules}. In step~\ref{alg: line 7} we select the best pair~$(\Delta_l,\delta_l)$ which maximizes the objective. Finally, based on~\eqref{eq: update rules} we update~$\bm{\alpha}$,~$\bm{\beta}$,~$\bm{s}$ and~$\delta$ in steps~\ref{alg: line 8} and~\ref{alg: line 9}.

\begin{algorithm*}
  \begin{minipage}{0.50\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State set~$(\bm{\alpha}, \bm{\beta})$ feasible
      \State set~$\bm{s}$ based on~\eqref{eq: dual scores} \label{alg: line 1}
      \Repeat \label{alg: line 2}
        \State pick random~$k$ from~$\{1, 2, \ldots, \npos+ \ntil \}$ \label{alg: line 3}
        \For{$l \in \{1, 2, \ldots, \npos + \ntil  \}$} \label{alg: line 4}
            \State compute~$\Delta_{l}$  \label{alg: line 5}
        \EndFor
        \State select best~$\Delta_{l}$ \label{alg: line 7}
        \State update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq: update rules} \label{alg: line 8}
        \State \label{alg: line 9}
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \hfill
  \begin{minipage}{0.50\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State set~$(\bm{\alpha}, \bm{\beta}, \delta)$ feasible
      \State set~$\bm{s}$ based on~\eqref{eq: dual scores}
      \Repeat
        \State pick random~$k$ from~$\{1, 2, \ldots, \npos + \ntil \}$ 
        \For{$l \in \{1, 2, \ldots, \npos + \ntil \}$}
            \State compute~$(\Delta_{l}, \; \delta_{l})$
        \EndFor
        \State select best~$(\Delta_{l}, \; \delta_{l})$
        \State update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq: update rules}
        \State set~$\delta \leftarrow \delta_{l}$
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \caption{Coordinate descent algorithm for \TopPushK family of formulations (left) and \PatMat  family of formulations (right).}
  \label{alg:Coordinate descent}
\end{algorithm*}

Now we derive the computational complexity of each \repeatloop loop from step~\ref{alg: line 2}. The computation of~$(\Delta_l,\delta_l)$ amounts to solving a quadratic optimization problem in one variable. As we showed in Sections~\ref{sec: Top coordinate descent} and~\ref{sec: Pat coordinate descent}, there is a closed-form solution and step~\ref{alg: line 5} can be performed in~$O(1)$. Since this is embedded in a \forloop loop in step~\ref{alg: line 4}, the whole complexity of this loop is~$O(\npos + \ntil)$. Step~\ref{alg: line 8} requires~$O(1)$ for the update of~$\bm{\alpha}$ and~$\bm{\beta}$ while~$O(\npos + \ntil)$ for the update of~$\bm{s}$. Since the other steps are~$O(1)$, the total complexity of the \repeatloop loop is~$O(\npos + \ntil)$. This holds true only if the kernel matrix~$\K$ is precomputed. In the opposite case, all complexities must by multiplied by the cost of computation of components of~$\K$ which is~$O(d)$. This complexity analysis is summarized in Table~\ref{tab:Computational complexity}. 

\begin{table}
  \centering
  \begin{tabular}{lcc} 
    \toprule
    Operation
      & $\K$ precomputed
      & $\K$ not precomputed \\
    \midrule
    % Initialization
    %   & $O()$
    %   & $O()$ \\
    Evaluation of~$\Delta_l$
      & $O(1)$
      & $O(d)$ \\
    Update of~$\bm{\alpha}$ and~$\bm{\beta}$
      & $O(1)$
      & $O(1)$ \\
    Update of~$\bm{s}$
      & $O(\npos + \ntil)$
      & $O((\npos + \ntil)d)$ \\
    \midrule
    Total per iteration
      & $O(\npos + \ntil)$
      & $O((\npos + \ntil)d)$ \\
    \bottomrule
  \end{tabular}
  \caption{Computational complexity of one \repeatloop loop (which updates two coordinates of~$\bm{\alpha}$ or~$\bm{\beta}$) from Algorithm~\ref{alg:Coordinate descent}.}
  \label{tab:Computational complexity}
\end{table}