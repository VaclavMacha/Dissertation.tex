\chapter{Non-linear Classification at the Top}\label{chap: dual}

\section{Introduction}

These problems can be generally written as pushing the positive samples above some decision threshold. The methods differ in the definition of the decision threshold. In our previous work~\cite{adam2021general}, we introduced a general framework that unifies these methods. We showed that several problem classes, which were considered as separate problems so far, fit into the framework. The deficiency of methods from this framework is that they usually cover only linear classifiers. However, as many problems are not linearly separable, nonlinear classifiers are needed. In this work, we show how to extend our framework into nonlinear classification problems. To do so, we use the fact that our framework is similar to the primal formulation of support vector machines~\cite{cortes1995support}. The classical way to incorporate nonlinearity into SVM is to derive the dual formulation~\cite{boyd2004convex} and to employ the kernels method~\cite{scholkopf2001learning}. In this work, we follow this approach, derive dual formulations for the considered problems and add nonlinear kernels to them. Moreover, as dual problems are generally expensive to solve, we derive a quick method to solve them. This is a modification of the coordinate-wise dual ascent from~\cite{hsieh2008dual}. For a review of other approaches see~\cite{batmaz2019review,werner2019review}.

The paper is organized as follows: In Section~\ref{sec:Derivation of dual problems} we recall the unified framework derived in~\cite{adam2021general} and two class of problems that falls into it. Moreover, for selected methods, we derive their dual formulations. Namely, we focus on \TopPush, \TopPushK and \PatMat. In Section~\ref{sec:New method for solving dual problems}, we show how to add nonlinear kernels into dual formulations, derive a new method for solving these dual problems and perform its complexity analysis. Since our method depends on the chosen problem and surrogate function, we provide a concrete form of the solution for \TopPushK with the truncated quadratic loss. Solutions for other problems are provided in Appendix~\ref{sec:Computing Delta with truncated quadratic surrogate} and~\ref{sec:Computing Delta with hinge surrogate}. Finally, in Section~\ref{sec:Numerical experiments} we present the description of performance criteria, choice of hyperparameters and description of datasets. The rest of the section is focused on the results of numerical experiments. Here we compare all methods in terms of overall accuracy and accuracy at a given threshold. We also discuss the convergence and time consumption of all methods. All our codes are available online.\footnote{\texttt{https://github.com/VaclavMacha/ClassificationOnTop\_nonlinear.jl}}

\section{Derivation of dual problems}\label{sec:Derivation of dual problems}

Linear binary classification is a problem of finding a linear hyperplane that separates a group of positive samples from a group of negative samples and achieves the lowest possible error. For a sample~$\bm{x} \in \R^d,$ the prediction for a linear classifier amounts to
\begin{equation*}
  \bm{x} \textnormal{ has }
  \begin{dcases*}
    \textnormal{positive label} & if~$\bm{w}^{\top} \bm{x} \geq t$, \\
    \textnormal{negative label} & otherwise.
  \end{dcases*}
\end{equation*}
Here,~$\bm{w} \in \R^{d}$ is the normal vector to the separating hyperplane and~$t \in \R$ is a decision threshold. The well-known example of such a classifier is a support vector machine~\cite{cortes1995support} where the decision threshold~$t$ is a free variable. However, many important binary classification problems maximize the performance only for a certain amount of samples with the highest scores~$s = \bm{w}^{\top}\bm{x}.$ In these cases, the threshold~$t$ is not a free variable but a function of the scores. In our previous work~\cite{adam2021general}, we formulated a general framework for maximizing performance above the threshold~$t$ as
\begin{equation}\label{eq:General problem}
  \begin{aligned}
    \minimize{\bm{w}, t}
    & \frac{1}{2} \norm{\bm{w}}_{2}^{2} + C \sum_{i = 1}^{\npos} [t - \bm{w}^{\top}\bm{x}^+_{i}] \\
    \st
    & \textnormal{threshold} \ t \ \textnormal{is a function of } \{\bm{w}^{\top} \bm{x_i}\}_{i=1}^n,
  \end{aligned}
\end{equation}
where~$C \in \R$ is a constant;~$[\cdot]$ is the~$0-1$~loss defined as~$1$ if the argument is true and~$0$ otherwise. To denote positive and negative samples, we use~$+$ and~$-$ symbols in the superscript, respectively. Note that~$[t - \bm{w}^{\top}\bm{x}^+_{i}]$ counts the number of positive samples~$\bm x_i^+$ whose score~$\bm{w}^\top \bm x_i^+$ is below the threshold~$t$. Since the objective is to be minimized, the positive samples should lie above the threshold~$t$.

Since the objective function in~\eqref{eq:General problem} is discontinuous due to the~$0-1$~loss, the optimization problem~\eqref{eq:General problem} is difficult. A typical approach to remove this unwanted feature is to approximate the~$0-1$~loss by a surrogate function such as the truncated quadratic or the hinge functions
\begin{align}
    l_{\textnormal{quadratic}}(s)
    & = (\max\{0,1 + \vartheta s\})^{2}, \label{eq:Truncated quadratic loss} \\
    l_{\textnormal{hinge}}(s)
    & = \max\{0,1 + \vartheta s\} \label{eq:Hinge loss},
\end{align}
where~$\vartheta > 0$ is a scaling parameter. In the following text, we use the symbol~$l$ to denote any convex non-negative non-decreasing function with~$l(0) = 1.$ Replacing the~$0-1$~loss in~\eqref{eq:General problem} by a surrogate function results in
\begin{equation}\label{eq:General surrogate problem}
  \begin{aligned}
    \minimize{\bm{w}, t}
    & \frac{1}{2} \norm{\bm{w}}_{2}^{2} + C \sum_{i = 1}^{\npos} l(t - \bm{w}^{\top}\bm{x}^+_{i}) \\
    \st
    & \textnormal{threshold} \ t \ \textnormal{is a function of } \{\bm{w}^{\top} \bm{x_i}\}_{i=1}^n.
  \end{aligned}
\end{equation}
Note that the objective function in~\eqref{eq:General surrogate problem} is continuous.

As we derived in~\cite{adam2021general}, there are many problems belonging to the general framework~\eqref{eq:General problem}. However, this framework handles only linear classification problems. As many problems are not linearly separable, this is often not sufficient. To generalize the framework to nonlinear classifiers, we realize that~\eqref{eq:General surrogate problem} is similar to the primal formulation of the SVM~\cite{cortes1995support}. We will follow the standard way to incorporate nonlinearity into SVM by deriving the dual problem~\cite{boyd2004convex} and using the kernels methods~\cite{scholkopf2001learning}.

In the remainder of this section, we recall two problem classes from~\cite{adam2021general} and their convex approximations, and for each of them, we derive its dual formulation. Namely, we will discuss \TopPushK and \AccatTop with its convex approximation \PatMat.

\begin{notation}[Kernel Matrix]\label{not: kernel matrix}
  To simplify the future notation, we use matrix~$\X$ of all samples with rows defined for all~$i \in \I$ as 
  \begin{equation*}
    \X_{i, \bullet} = \bm{x}_i^{\top}.
  \end{equation*}
  In other words, each row of~$\X$ represents one sample. Similarly, we defined matrices~$\X^+,$~$\X^-$ of all negative and positive samples with rows defined as
  \begin{align*}
    \X^{-}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^-, \\
    \X^{+}_{i, \bullet} & = \bm{x}_i^{\top} \quad i = 1, \;, 2, \ldots, \; n^+.
  \end{align*}
  Moreover, for \TopPush, \TopPushK, \tauFPL and \PatMatNP formulations we define kernel matrix~$\K$ as
  \begin{equation*}
    \Kneg = \Matrix{\X^+ \\ - \X^-} \Matrix{\X^+ \\ - \X^-}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} }.
  \end{equation*}
  Simlarly, for \TopMeanK and \PatMat formulations we define kernel matrix~$\K$ as
  \begin{equation*}
    \Kneg = \Matrix{\X^+ \\ - \X} \Matrix{\X^+ \\ - \X}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{\top} \\ -\X \X^{+\top} & \X \X^{\top} }.
  \end{equation*}
\end{notation}

\subsection{\TopPushK}

The first problem \TopPushK is our modification of the \TopPush method introduced in~\cite{li2014top}. It selects the threshold~$t$ as the mean of the scores corresponding to~$K$ highest ranked negatives. By doing so, it enforces the positives to be ranked above negatives. Therefore, both \TopPush ($K=1$) and \TopPushK ($K>1$) fall into the category of ranking problems.

Writing this more formally, define vector~$\bm{s}$ of all scores as~$s_i = \bm{w}^{\top} \bm{x}_i$ for all~$i = 1, 2, \ldots, n$ and its sorted version~$\bm{s}_{[\cdot]}$ with decreasing components, i.e.~$s_{[1]} \geq s_{[2]} \geq \cdots  \geq s_{[n]}$. Then \TopPushK reads
\begin{equation}\label{eq:TopPushK primal}
  \begin{aligned}
    \minimize{\bm{w}, t, \bm s^-}
    & \frac{1}{2} \norm{\bm{w}}_{2}^{2} + C \sum_{i = 1}^{\npos} l(t - \bm{w}^{\top}\bm{x}^+_{i}) \\
    \st
    & t = \frac{1}{K}\sum_{j = 1}^{K} s^{-}_{[j]}, \\
    & s_j^- = \bm{w}^\top \bm x_j^-, \quad \forall j = 1, 2, \ldots, \nneg.
  \end{aligned}
\end{equation}
It can be showed that this problem is convex and that for~$K = 1$ we get the original \TopPush.

In the following theorem, we denote the positive semidefinite kernel matrix~$\K$ by
\begin{equation}\label{eq:kernel_linear}
  \K = \Matrix{\X^+ \\ - \X^-} \Matrix{\X^+ \\ - \X^-}^\top = \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} }.
\end{equation}
and show the form of the \TopPushK dual problem\footnote{To keep the readability of the paper, we postpone all proofs to the Appendix}.

\begin{restatable}{theorem}{topdual}\label{thm: Top dual}
  Consider formulations \TopPush, \TopPushK, \TopMeanK and \tauFPL from Table~\ref{tab: summary formulations} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi*}{\bm{\alpha}, \bm{\beta}}{
    - \frac{1}{2} \vecab^\top \K \vecab - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    }{}{}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\tilde{n}} \beta_j}
    \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \tilde{n},}
  \end{maxi*}
  where~$l^{\star}$ is conjugate function of~$l$ and
  \begin{align*}
    \K & = \begin{cases}
      \Kall & \quad \text{for \TopMeanK}, \\
      \Kneg & \quad \text{otherwise}, \\
    \end{cases} &
    \tilde{n} & = \begin{cases}
      \nall & \quad \text{for \TopMeanK}, \\
      \nneg & \quad \text{otherwise}. \\
    \end{cases}
  \end{align*} 
  Moreover, the variable~$K$ is defined as follows
  \begin{equation*}
    K = \begin{cases}
      1 & \quad \text{for \TopPush}, \\
      \nall \tau & \quad \text{for \TopMeanK}, \\
      \nneg \tau & \quad \text{for \tauFPL}. \\
    \end{cases}
  \end{equation*}
  Finally, if~$K = 1,$ the upper bound in the second constrainet vanishes due to the first constraint.
\end{restatable}

\subsection{\AccatTop}

The second problem \AccatTop was introduced in~\cite{boyd2012accuracy}. On the contrary to \TopPushK, which focuses on the minimization of the number of positive samples below~$K$~highest ranked negatives, the \AccatTop minimizes the number of positive samples below the top~$\tau$-quantile from negative samples defined as
\begin{equation}\label{eq:Quantile}
    t = \max \Set{t}{\sum_{j = 1}^{\nneg} [\bm{w}^\top \bm{x}_j^- - t] \geq n \tau}.
\end{equation}
Then \AccatTop is an optimization problem written as follows
\begin{equation}\label{eq:AccAtTop}
  \begin{aligned}
    \minimize{\bm{w}, t}
    & \frac{1}{2} \norm{\bm{w}}_{2}^{2} + C \sum_{i = 1}^{\npos} l_1(t - \bm{w}^{\top}\bm{x}^+_{i}) \\
    \st
    & t \ \textnormal{is the surrogate top} \ \tau\textnormal{-quantile: it solves~\eqref{eq:Quantile}}.
  \end{aligned}
\end{equation}
Since it is known that the quantile function~\eqref{eq:Quantile} is non-convex, we derived its convex surrogate approximation
\begin{equation}\label{eq:Surrogate quantile}
  t \quad \textnormal{solves} \quad \sum_{j = 1}^{\nneg} l_2(\bm{w}^{\top}\bm{x}_{j}^- - t) \leq n \tau.
\end{equation}
Replacing the true quantile~\eqref{eq:Quantile} by its surrogate approximation~\eqref{eq:Surrogate quantile} yields
\begin{equation}\label{eq: PatMatNP primal}
  \begin{aligned}
    \minimize{\bm{w}, t}
    & \frac{1}{2} \norm{\bm{w}}_{2}^{2} + C \sum_{i = 1}^{\npos} l_1(t - \bm{w}^{\top}\bm{x}^+_{i}) \\
    \st & t \ \textnormal{is the top} \ \tau\textnormal{-quantile: it solves~\eqref{eq:Surrogate quantile}}.
  \end{aligned}
\end{equation}
In~\cite{adam2021general}, we called the convex problem~\eqref{eq: PatMatNP primal} \PatMat. The following theorem shows its dual form.

\begin{restatable}[Dual formulation 2]{theorem}{patdual}\label{thm: Pat dual}
  Consider formulations \PatMat and \PatMatNP from Table~\ref{tab: summary formulations} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi*}{\bm{\alpha}, \bm{\beta}, \delta}{
    - \frac{1}{2} \vecab^\top \K \vecab - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}} - \delta \sum_{j = 1}^{\tilde{n}} l^{\star} \Brac{\frac{\beta_j}{\delta}} - \delta \tilde{n} \tau
    }{}{}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\tilde{n}} \beta_j}
    \addConstraint{\delta }{\ge 0,}
  \end{maxi*}
  where~$l^{\star}$ is conjugate function of~$l$ and
  \begin{align*}
    \K & = \begin{cases}
      \Kall & \quad \text{for \PatMat}, \\
      \Kneg & \quad \text{otherwise}, \\
    \end{cases} &
    \tilde{n} & = \begin{cases}
      \nall & \quad \text{for \PatMat}, \\
      \nneg & \quad \text{otherwise}. \\
    \end{cases}
  \end{align*}
\end{restatable}

\section{New method for solving dual problems}\label{sec:New method for solving dual problems}

In the previous section, we derived the dual formulations for the \TopPushK and \PatMat problems. These dual formulations allow us to incorporate nonlinearity using kernels~\cite{scholkopf2001learning} in the same way as in SVM. Since the dimension of~\eqref{eq: TopPushK dual} and~\eqref{eq: PatMatNP dual} equals to the number of samples~$n$, it is computationally expensive to use standard techniques such as the gradient descent. To handle this issue, the coordinate descent algorithm~\cite{chang2008coordinate,hsieh2008dual} has been proposed in the context of SVMs. Since problems~(\ref{eq: TopPushK dual},~\ref{eq: PatMatNP dual}) differ from original SVMs by additional constraints (\ref{eq: TopPushK dual c1},~\ref{eq: PatMatNP dual c1}), the key idea of our algorithm is to update two coordinates (instead of one) of~$\bm{\alpha},$~$\bm{\beta}$ at every iteration. To summarize, we will solve the original tasks~(\ref{eq: TopPushK dual},~\ref{eq: PatMatNP dual}) by an iterative procedure where in every iteration we need to find a solution of a one-dimensional quadratic optimization problem. As we will show later, these one-dimensional problems have a closed form solution, which means that every iteration is cheap.

\subsection{Adding kernels}

To add kernels, we realize first that from the proofs of Theorems~\ref{thm: TopPushK dual} and~\ref{thm: PatMatNP dual} for any~$\bm{z}\in\R^d$ we have
\begin{equation}\label{eq:pred_linear}
  \bm{w}^\top \bm{z} = \sum_{i = 1}^{\npos} \alpha_i \bm{z}^\top \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{z}^\top \bm{x}^-_j
\end{equation}
Consider now any kernel function~$k:\R^d\times\R^d\to\R$. Using the standard trick, we replace the kernel matrix~\eqref{eq:kernel_linear} by\footnote{
The first part of the objective of~\eqref{eq: TopPushK dual} and~\eqref{eq: PatMatNP dual} amounts to
\begin{equation*}
  \Matrix{\bm{\alpha} \\ \bm{\beta}}^\top \K \Matrix{\bm{\alpha} \\ \bm{\beta}}
  = \Matrix{\bm{\alpha} \\ \bm{\beta}}^\top \Matrix{\X^+ \X^{+\top} & -\X^+ \X^{-\top} \\ -\X^- \X^{+\top} & \X^- \X^{-\top} }\Matrix{\bm{\alpha} \\ \bm{\beta}}
  = \Matrix{\bm{\alpha} \\ -\bm{\beta}}^\top \Matrix{\X^+ \X^{+\top} & \X^+ \X^{-\top} \\ \X^- \X^{+\top} & \X^- \X^{-\top} }\Matrix{\bm{\alpha} \\ -\bm{\beta}},
\end{equation*}
from which~\eqref{eq:kernel_nonlinear} follows.}
\begin{equation}\label{eq:kernel_nonlinear}
  \K = \Matrix{k(\X^+, \X^{+}) & -k(\X^+, \X^{-}) \\ -k(\X^-, \X^{+}) & k(\X^-, \X^{-}) },
\end{equation}
where~$k(\cdot,\cdot)$ is applied to all rows of both arguments. Then for a new sample~$\bm{z}$, the prediction~\eqref{eq:pred_linear} is replaced by
\begin{equation}\label{eq:pred_nonlinear}
  \textrm{pred}(\bm{z}) = \sum_{i = 1}^{\npos} \alpha_i k\left(\bm{z}, \bm{x}^+_i\right) - \sum_{j = 1}^{\nneg} \beta_j k\left(\bm{z}, \bm{x}^-_j\right),
\end{equation}
where~$\bm{\alpha}$ and~$\bm{\beta}$ are the optimal solution of~\eqref{eq: TopPushK dual} or~\eqref{eq: PatMatNP dual}.

\subsection{Update of dual optimization variables}

Let us consider the kernel matrix~$\K$ as in~\eqref{eq:kernel_nonlinear} and define the score vector~$\bm{s}$ by
\begin{equation}\label{eq:defin_s}
    \bm{s} = \K \Matrix{\bm{\alpha} \\ \bm{\beta}}.
\end{equation}
There are three possible update rules which modify two coordinates of~$\bm{\alpha},$~$\bm{\beta}$ and which satisfy constraints (\ref{eq: TopPushK dual c1},~\ref{eq: PatMatNP dual c1}) and keep~\eqref{eq:defin_s} satisfied. The first one updates two components of~$\bm{\alpha}$
\begin{subequations}\label{eq:Update rules}
  \begin{equation}\label{eq:Update rule a,a}
    \begin{alignedat}{2}
      \alpha_k & \rightarrow \alpha_k + \Delta, & \qquad
      \alpha_l & \rightarrow \alpha_l - \Delta, & \qquad
      \bm{s}   & \rightarrow \bm{s} + (\K_{\bullet k} - \K_{\bullet l})\Delta,
    \end{alignedat}
  \end{equation}
  where~$K_{\bullet i}$ denotes~$i$-th column of~$\K.$ Note that the update rule for~$\bm{s}$ does not use matrix multiplication but only vector addition. The second rule updates one component of~$\bm{\alpha}$ and one component of~$\bm{\beta}$ 
  \begin{equation}\label{eq:Update rule a,b}
    \begin{alignedat}{2}
      \alpha_k & \rightarrow \alpha_k + \Delta, & \qquad
      \beta_l  & \rightarrow \beta_l  + \Delta, & \qquad
      \bm{s}   & \rightarrow \bm{s} + (\K_{\bullet k} + \K_{\bullet l})\Delta,
    \end{alignedat}
  \end{equation}
  and the last one updates two components of~$\bm{\beta}$
  \begin{equation}\label{eq:Update rule b,b}
    \begin{alignedat}{2}
      \beta_k & \rightarrow \beta_k + \Delta, & \qquad
      \beta_l & \rightarrow \beta_l - \Delta, & \qquad
      \bm{s}  & \rightarrow \bm{s} + (\K_{\bullet k} - \K_{\bullet l})\Delta.
    \end{alignedat}
  \end{equation}
\end{subequations}
These three update rules hold true for any surrogate function. However, the calculation of the optimal~$\Delta$ depends on the used problem formulation and surrogate function. In Subsection~\ref{sec:Computing Delta for TopPushK with truncated quadratic loss}, we show the closed-form formula for~$\Delta$ for \TopPushK problem~\eqref{eq: TopPushK dual} with truncated quadratic surrogate function~\eqref{eq:Truncated quadratic loss}. Computation of~$\Delta$ for the hinge surrogate or \PatMat is presented in Appendices~\ref{sec:Computing Delta with truncated quadratic surrogate} and~\ref{sec:Computing Delta with hinge surrogate}. 

\subsection{Algorithm summary and Complexity analysis}

We summarize the whole procedure in Algorithm~\ref{alg:Coordinate descent}. We will describe it only for \PatMat (right column) as for \TopPushK (left column) it is almost identical. In step~\ref{alg: line 1} we initialize~$\bm{\alpha}$,~$\bm{\beta}$ and~$\delta$ to some feasible value and based on~\eqref{eq:defin_s} compute~$\bm s$. Each \repeatloop loop in step~\ref{alg: line 2} updates two coordinates as shown in~\eqref{eq:Update rules}. In step~\ref{alg: line 3} we select a random index~$k$ and in the \forloop loop in step~\ref{alg: line 4} we compute the optimal~$(\Delta_l,\delta_l)$ for all possible combinations~$(k,l)$ as in~\eqref{eq:Update rules}. In step~\ref{alg: line 7} we select the pair~$(\Delta_l,\delta_l)$ which maximizes the objective. Finally, based on~\eqref{eq:Update rules} we update~$\bm{\alpha}$,~$\bm{\beta}$,~$\bm s$ and~$\delta$ in steps~\ref{alg: line 8} and~\ref{alg: line 9}.

\begin{algorithm*}[!ht]
  \begin{minipage}{0.48\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State set~$(\bm{\alpha}, \bm{\beta})$ feasible, set~$\bm{s}$ based on \eqref{eq:defin_s} \label{alg: line 1}
      \Repeat \label{alg: line 2}
        \State select random~$k$ from~$\{1, 2, \ldots, n\}$ \label{alg: line 3}
        \For{$l \in \{1, 2, \ldots, n\}$} \label{alg: line 4}
            \State compute~$\Delta_{l}$  \label{alg: line 5}
        \EndFor
        \State select best~$\Delta_{l}$ \label{alg: line 7}
        \State update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq:Update rules} \label{alg: line 8}
        \State \label{alg: line 9}
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \hfill
  \begin{minipage}{0.51\textwidth}
    \centering
    \begin{algorithmic}[1]
      \State set~$(\bm{\alpha}, \bm{\beta}, \delta)$ feasible, set~$\bm{s}$ based on \eqref{eq:defin_s}
      \Repeat
        \State select random~$k$ from~$\{1, 2, \ldots, n\}$ 
        \For{$l \in \{1, 2, \ldots, n\}$}
            \State compute~$(\Delta_{l}, \; \delta_{l})$
        \EndFor
        \State select best~$(\Delta_{l}, \; \delta_{l})$
        \State update~$\bm{\alpha}$,~$\bm{\beta},$~$\bm{s}$ according to~\eqref{eq:Update rules}
        \State set~$\delta \leftarrow \delta_{l}$
      \Until{stopping criterion is satisfied}
    \end{algorithmic}
  \end{minipage}
  \caption{Coordinate descent algorithm for \TopPushK (left) and \PatMat (right).}
  \label{alg:Coordinate descent}
\end{algorithm*}

Now we derive the computational complexity of each \repeatloop loop from step~\ref{alg: line 2}. Since the computation of~$(\Delta_l,\delta_l)$ amounts to solving a quadratic optimization problem in one variable, there is a closed-form solution (see Section~\ref{sec:Computing Delta for TopPushK with truncated quadratic loss}) and step~\ref{alg: line 5} can be performed in~$O(1)$. Since this is embedded in a \forloop loop in step~\ref{alg: line 4}, the whole complexity of this loop is~$O(n)$. Step~\ref{alg: line 8} requires~$O(1)$ for the update of~$\bm{\alpha}$ and~$\bm{\beta}$ while~$O(n)$ for the update of~$\bm s$. Since the other steps are~$O(1)$, the total complexity of the \repeatloop loop is~$O(n)$. This holds true only if the kernel matrix~$\K$ is precomputed. In the opposite case, all complexities must by multiplied by the cost of computation of components of~$\K$ which is~$O(d)$. This complexity analysis is summarized in Table~\ref{tab:Computational complexity}. 


\begin{table}[!ht]
  \centering
  \begin{tabular}{@{}clll@{}l} 
    \toprule
    Precomputed~$\K$ & Evaluation of~$\Delta_l$ & Update of~$\bm{s}$ & Total per iteration \\
    \midrule
    \yesmark & $O(1)$ & $O(n)$  & $O(n)$ \\
    \nomark  & $O(d)$ & $O(nd)$ & $O(nd)$ \\
    \bottomrule
  \end{tabular}
  \caption{Computational complexity of one \repeatloop loop (which updates two coordinates of~$\bm{\alpha}$ or~$\bm{\beta}$) from Algorithm~\ref{alg:Coordinate descent}.}
  \label{tab:Computational complexity}
\end{table}

\subsection{Computing~$\Delta$ for \TopPushK with truncated quadratic loss}\label{sec:Computing Delta for TopPushK with truncated quadratic loss}

In this section, we show how to compute the stepsize~$\Delta$ from~\eqref{eq:Update rules} for \TopPushK~\eqref{eq: TopPushK dual} with the truncated quadratic surrogate function~\eqref{eq:Truncated quadratic loss}. Optimal~$\Delta$ for \PatMat can be found in a similar way as we show in Appendix~\ref{sec:Computing Delta with truncated quadratic surrogate}. In Appendix~\ref{sec:Computing Delta with hinge surrogate} we present the computation of optimal~$\Delta$ for \TopPushK and \PatMat with the hinge loss function~\eqref{eq:Hinge loss}.

Plugging the conjugate~\eqref{eq:Conjugate of truncated quadratic loss} of the truncated quadratic loss~\eqref{eq:Truncated quadratic loss} into \TopPushK dual formulation~\eqref{eq: TopPushK dual} yields
\begin{subequations}\label{eq:TopPushK dual quadratic}
  \begin{alignat}{2}
    \maximize{\bm{\alpha}, \bm{\beta}}
    & - \frac{1}{2} \Matrix{\bm{\alpha} \\ \bm{\beta}}^\top \K \Matrix{\bm{\alpha} \\ \bm{\beta}} + \frac{1}{\vartheta} \sum_{i = 1}^{\npos} \alpha_i - \frac{1}{4 C \vartheta^2} \sum_{i = 1}^{\npos} \alpha_i^2 \span \span \label{eq:TopPushK dual quadratic objective} \\
    \st 
    & \sum_{i = 1}^{\npos} \alpha_i = \sum_{j = 1}^{\nneg} \beta_j, \label{eq:TopPushK dual quadratic constraint 1} \\
    & \alpha_i \geq 0, && \quad \forall i = 1, 2, \ldots, \npos, \label{eq:TopPushK dual quadratic constraint 2} \\
    & 0 \leq \beta_j  \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, && \quad \forall j = 1, 2, \ldots, \nneg. \label{eq:TopPushK dual quadratic constraint 3}
  \end{alignat}
\end{subequations}
This is a quadratic optimization problem. Moreover, for~$K=1$ the upper bound in~\eqref{eq:TopPushK dual quadratic constraint 3} automatically follows from~\eqref{eq:TopPushK dual quadratic constraint 1} and the problem can be simplified. In the following theorem, we show that for each of update rules~\eqref{eq:Update rules}, problem~\eqref{eq:TopPushK dual quadratic} has a simple closed-form solution. For simplicity, we use~$\clip{a}{b}{c}$ to denote clipping (projecting)~$c$ to the interval~$[a,b]$.

\begin{restatable}[Update rule for~$\Delta^*$ for \TopPushK with truncated quadratic loss]{theorem}{toppushkupdatequadratic}\label{thm:Update rule TopPushK with quadratic loss}
  Consider problem~\eqref{eq:TopPushK dual quadratic}. Then the optimal step~$\Delta^\star$ equals to
  \begin{equation}\label{eq:Optimal delta}
    \Delta^{*} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma},
  \end{equation}
  where there are the following three cases (each corresponding to one update rule in~\eqref{eq:Update rules}):
  \begin{itemize}
    \item For any~$1\le k, l \le \npos$ we have
    \begin{align*}
      \Delta_{lb} & = -\alpha_k, \\
      \Delta_{ub} & = \alpha_l, \\
      \gamma      & = -\frac{s_k - s_l + \frac{1}{2C\vartheta^2}(\alpha_k - \alpha_l)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C\vartheta^2}}.
    \end{align*}
    \item For any~$1 \le k \le \npos$ and~$\npos + 1 \le l \le n$ we define~$\hat{l} = l - \npos$  and~$\beta_{\max} = \max_{j \in \{1, 2, \ldots, \nneg \} \setminus \{\hat l\}} \beta_j.$ Then we have
    \begin{align*}
      \Delta_{lb} & = 
        \begin{cases*}
          \max \Brac[c]{- \alpha_k,\;  -\beta_{\hat l}} & K = 1, \\
          \max \Brac[c]{- \alpha_k,\;  -\beta_{\hat l}, \; K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
        \end{cases*} \\
      \Delta_{ub} & = 
        \begin{cases*}
          + \infty & K = 1, \\
          \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \beta_{\hat l}} & \textrm{otherwise},
        \end{cases*} \\
      \gamma & = -\frac{s_k + s_l - \frac{1}{\vartheta} + \frac{1}{2C\vartheta^2} \alpha_k}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C\vartheta^2}}.
    \end{align*}
    \item For any~$\npos + 1\le k,l \le n$ we define~$\hat{k} = k - \npos,$~$\hat{l} = l - \npos$ and then  have
    \begin{align*}
      \Delta_{lb} & = 
        \begin{cases*}
          -\beta_{\hat k} & K = 1, \\
          \max \Brac[c]{- \beta_{\hat k},\; \beta_{\hat l} - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
        \end{cases*} \\
      \Delta_{ub} & = 
        \begin{cases*}
          \beta_{\hat l} & K = 1, \\
          \min \Brac[c]{\beta_{\hat l},\; \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \beta_{\hat k}} & \textrm{otherwise},
        \end{cases*} \\
      \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
    \end{align*}
  \end{itemize}
\end{restatable}
