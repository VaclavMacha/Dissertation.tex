\chapter{Appendix for Chapter~\ref{chap: dual}}\label{app: dual}

\section{Derivation of Dual Problems}

\subsection{Family of \TopPushK Formulations}

\topdual*
\begin{proof}
  We show the proof only for \TopPushK formulation, i.e., the decision threshold is computed only from negative samples. The proof for the remaining formulations is identical. Firstly, we derive an alternative formulation to formulation~\eqref{eq: toppushk family}. Using Lemma~1 from~\cite{ogryczak2003minimizing}, we can rewrite the formula for the decision threshold to the following form
  \begin{equation*}
    \sum_{j = 1}^{K} s^-_{[j]} = \min_{t} \Brac[c]{Kt + \sum_{j = 1}^{\nneg} \max\{0, \; s^-_j - t\}}.
  \end{equation*}
  By substituting this formula into the objective function of~\eqref{eq: toppushk family}, we get
  \begin{align*}
    \sum_{i = 1}^{\npos} l\Brac{\frac{1}{K}\sum_{j = 1}^{K} s^-_{[j]} - s^+_{i}}
      & = \sum_{i = 1}^{\npos} l\Brac{ \frac{1}{K} \min_{t} \Brac[c]{Kt + \sum_{j = 1}^{\nneg} \max\Brac[c]{0, \; s^-_j - t}} - s^+_{i}} \\
      & = \min_{t} \; \sum_{i = 1}^{\npos} l\Brac{t + \frac{1}{K} \sum_{j = 1}^{\nneg} \max\Brac[c]{0, \; s^-_j - t} - s^+_{i}}.
  \end{align*}
  where the last equality follows from the fact that the surrogate function~$l$ is non-decreasing. The max operator can be replaced using an auxiliary variable~$\bm{z} \in \R^{\nneg}$ that fulfills~$z _j \geq s^-_j - t$ and~$z _j \geq 0$ for all~$j = 1, \ldots, \; \nneg.$ Furthermore, we use auxilliary variable~$\bm{y} \in \R^{\npos}$ defined for all~$i = 1, \ldots, \; \npos$ as
  \begin{equation*}
    y_i = t + \frac{1}{K} \sum_{j = 1}^{\nneg} z_j - s^+_i.
  \end{equation*}
  The combination of all the above relations and the use of a linear model yields to
  \begin{mini*}{\bm{w}, t, \bm{y}, \bm{z}}{
    \frac{1}{2} \norm{\bm{w}}_{2}^{2}+ C \sum_{i = 1}^{\npos} l(y_i)
    }{}{}
    \addConstraint{y_i}{= t + \frac{1}{K} \sum_{j = 1}^{\nneg} z_j - \bm{w}^{\top} \bm{x}^+_i, \quad}{i = 1, \; 2, \ldots, \; \npos}
    \addConstraint{z_j}{\geq \bm{w}^{\top} \bm{x}^-_j - t,}{j = 1, \; 2, \ldots, \; \nneg}
    \addConstraint{z_j}{\geq 0,}{j = 1, \; 2, \ldots, \; \nneg,}
  \end{mini*}

  The Lagrangian of this formulation is defined as 
  \begin{align*}
    \mathcal{L}(\bm{w}, t, \bm{y}, \bm{z}; \bm{\alpha}, \bm{\beta}, \bm{\gamma})
     & = \frac{1}{2} \norm{\bm{w}}_{2}^{2}
       + C \sum_{i = 1}^{\npos} l(y_i)
       + \sum_{i = 1}^{\npos} \alpha_i \Brac{t + \frac{1}{K} \sum_{j = 1}^{\nneg} z_j - \bm{w}^{\top} \bm{x}^+_i - y_i} \\
     & + \sum_{j = 1}^{\nneg} \beta_j \Brac{\bm{w}^{\top} \bm{x}^-_j - t - z_j}
       - \sum_{j = 1}^{\nneg} \gamma_j z_j,
  \end{align*}
  with feasibility conditions~$\beta_j \geq 0$ and~$\gamma_j \geq 0$ for all~$j = 1, \ldots, \; \nneg.$ Since the Lagrangian~$\mathcal{L}$ is separable in primal variables, it can be minimized with respect to each variable separately. Then the dual objective function (to be maximized) reads
  \begin{subequations}\label{eq: TopPushK dual function}
    \begin{align}
      g(\bm{\alpha}, \bm{\beta}, \bm{\gamma})
        & = \min_{\bm{w}} \; \frac{1}{2} \norm{\bm{w}}_{2}^{2}
          - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j} \label{eq: TopPushK dual function w}\\
        & + \min_{t} \; t \Brac{\sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nneg} \beta_j} \label{eq: TopPushK dual function t}\\
        & + \min_{\bm{y}} \; C \sum_{i = 1}^{\npos} \Brac{l(y_i) - \frac{\alpha_i}{C}y_i} \label{eq: TopPushK dual function y}\\
        & + \min_{\bm{z}} \; \sum_{j = 1}^{\nneg} \Brac{\frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \beta_j - \gamma_j}z_j \label{eq: TopPushK dual function z}
    \end{align}
  \end{subequations}
  
  From optimality conditions with respect to~$\bm{w},$ we deduce 
  \begin{equation*}
    \bm{w}
      = \sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j
      = \Matrix{\X^+ \\ - \X^-}^\top \vecab
    \; \implies \;
    \frac{1}{2} \norm{\bm{w}}_{2}^{2} - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j}
      = - \frac{1}{2} \vecab^{\top} \Kneg \vecab,
  \end{equation*}
  where we use Notation~\ref{not: kernel matrix}. It mean, that we get the first part of the objective function~\eqref{eq: toppushk family dual L}.
  
  Optimality condition with respect to~$t$ reads 
  \begin{equation*}
    \sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nneg} \beta_j = 0,
  \end{equation*}
  and implies constrain~\eqref{eq: toppushk family dual c1}.
  
  Similarly, optimality condition of~\eqref{eq: TopPushK dual function z} with respect to~$\bm{z}$ reads for all $j = 1, \ldots, \; \nneg$ as 
  \begin{equation*}
    \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \beta_j - \gamma_j = 0.
  \end{equation*}
  Plugging the feasibility condition~$\gamma_j \geq 0$ into this equality and combining it with the feasibility conditions~$\beta_j \geq 0,$ yields constraint~\eqref{eq: toppushk family dual c2}.
  
  Finally, the second part of the objective function~\eqref{eq: toppushk family dual L} follows from Definition~\ref{def: conjugate} of the conjugate function. Using the definition, minimization of~\eqref{eq: TopPushK dual function y} with respect to~$\bm{y}$ yields
  \begin{equation*}
    C \min_{y_i} \Brac{l(y_i) - \frac{\alpha_i}{C} y_i} = - C l^{\star} \Brac{\frac{\alpha_i}{C}},
  \end{equation*}
  for all $i = 1, \ldots, \; \npos,$ which finishes the proof for \TopPushK. For \TopPush, we have~$K = 1.$ From~\eqref{eq: toppushk family dual c1} and non-negativity of~$\beta_j$ we deduce that the upper bound in~\eqref{eq: toppushk family dual c2} is always fulfilled and can be omitted.
\end{proof}

\subsection{Family of \PatMat Formulations}

\patdual*

\pagebreak

\begin{proof}
  For simplicity, we show the proof only for \PatMatNP, i.e. the threshold is computed only from negative samples. Let us first realize that formulation~\eqref{eq: patmat family} is equivalent to the following formulation
  \begin{mini*}{\bm{w}, t, \bm{y}, \bm{z}}{
    \frac{1}{2} \norm{\bm{w}}_{2}^{2}+ C \sum_{i = 1}^{\npos} l(y_i)
    }{}{}
    \addConstraint{}{\sum_{j = 1}^{\nneg} l(\vartheta z_i)\leq \nneg \tau}
    \addConstraint{}{y_i = t - \bm{w}^{\top} \bm{x}^+_i, \quad}{i = 1, \; 2, \ldots, \; \npos}
    \addConstraint{}{z_j = \bm{w}^{\top} \bm{x}^-_j - t, \quad}{j = 1, \; 2, \ldots, \; \nneg.}
  \end{mini*}

  The corresponding Lagrangian then reads
  \begin{align*}
    \mathcal{L}(\bm{w}, t, \bm{y}, \bm{z}; \bm{\alpha}, \bm{\beta}, \delta)
    & = \frac{1}{2} \norm{\bm{w}}_{2}^{2}
      + C \sum_{i = 1}^{\npos} l(y_i)
      + \sum_{i = 1}^{\npos} \alpha_i (t - \bm{w}^{\top}\bm{x}^+_{i} - y_i) \\
    & + \sum_{j = 1}^{\nneg} \beta_j(\bm{w}^{\top}\bm{x}^-_j - t - z_j)
      + \delta \Brac{\sum_{j = 1}^{\nneg} l(\vartheta z_j) - \nneg \tau}.
  \end{align*}
  with feasibility condition~$\delta \geq 0.$ Since the Lagrangian~$\mathcal{L}$ is separable in primal variables, it can be minimized with respect to each variable separately. Then the dual objective function (to be maximized) can be rewritten as follows
  \begin{subequations}\label{eq: PatMat dual function}
    \begin{align}
      g(\bm{\alpha}, \bm{\beta}, \delta)
        & = \min_{\bm{w}} \; \frac{1}{2} \norm{\bm{w}}_{2}^{2}
          - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j} \label{eq: PatMat dual function w}\\
        & + \min_{t} \; t \Brac{\sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nneg} \beta_j} \label{eq: PatMat dual function t} \\
        & + \min_{\bm{y}} \; C \sum_{i = 1}^{\npos} \Brac{l(y_i) - \frac{\alpha_i}{C}y_i} \label{eq: PatMat dual function y} \\
        & + \min_{\bm{z}} \; \delta \sum_{j = 1}^{\nneg} \Brac{l(\vartheta z_j) - \frac{\beta_j}{\delta}z_j} \label{eq: PatMat dual function z} \\
        & - \delta \nneg \tau. \label{eq: PatMat dual function delta}
    \end{align}
  \end{subequations}

  Note that the resulting dual function is very similar to one~\eqref{eq: TopPushK dual function} for \TopPushK. In fact, the first three parts of~\eqref{eq: TopPushK dual function} and~\eqref{eq: PatMat dual function} are identical. Therefore, we only have to show how to minimize~\eqref{eq: PatMat dual function} with respect to~$\bm{z}.$ For that, we can use the conjugate function as in the case of minimization of~\eqref{eq: TopPushK dual function} with respect to~$\bm{y}.$  Then, for all $j = 1, \ldots, \; \nneg,$ we get 
  \begin{equation*}
    \delta \min_{\bm{z}} \; \Brac{l(\vartheta z_j) - \frac{\beta_j}{\delta\vartheta } \vartheta z_j} = - \delta l^{\star} \Brac{\frac{\beta_i}{\delta\vartheta }},
  \end{equation*}
  where the equality follows from Definition~\ref{def: conjugate} of a conjugate function. Plugging this back into~\eqref{eq: PatMat dual function z} yields the third part of the objective function~\eqref{eq: patmat family dual L}, which finishes the proof.
\end{proof}

\section{Coordinate Descent Algorithm}
\subsection{Family of \TopPushK Formulations}\label{sec: toppushk family coordinate proofs}
\subsubsection{Hinge Loss}

\topruleaa*
\begin{proof}[Proof of Proposition~\ref{prop: toppushk family hinge update a,a} on page~\pageref{prop: toppushk family hinge update a,a}]
  Constraint~\eqref{eq: Top dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}, and constraint~\eqref{eq: Top dual hinge c3} is always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Top dual hinge c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta \leq C
    & \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak, \\
    0 \leq \alphal - \Delta \leq C
    & \quad \implies \quad
    \alphal - C \leq \Delta \leq \alphal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$
  
  Using the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Top dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\topruleab*
\begin{proof}[Proof of Proposition~\ref{prop: toppushk family hinge update a,b} on page~\pageref{prop: toppushk family hinge update a,b}]
  Constraint~\eqref{eq: Top dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraint~\eqref{eq: Top dual hinge c2} reads
  \begin{equation*}
    0 \leq \alphak + \Delta \leq C
    \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak.
  \end{equation*}
  Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Top dual hinge c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \beta_{\max} \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K} 
    & \quad \implies \quad
    K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i \leq \Delta, \\
    0 \leq \betal + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K}
    & \quad \implies \quad
    -\betal \leq \Delta \quad \land \quad \Delta \leq \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}.
  \end{align*}
  The combination of these bounds yields the lower bound~$\Delta_{lb}$ and upper bound~$\Delta_{ub}.$ If~$K = 1,$ the upper bound in~\eqref{eq: Top dual hinge c3} is always satisfied due to~\eqref{eq: Top dual hinge c1} and the lower and upper bound of~$\Delta$ can be simplified.
  
  Using the update rule~\eqref{eq: update rule a,b}, objective function~\eqref{eq: Top dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}} \Delta^2
    - \Brac[s]{s_k + s_l - 1} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\toprulebb*
\begin{proof}[Proof of Proposition~\ref{prop: toppushk family hinge update b,b} on page~\pageref{prop: toppushk family hinge update b,b}]
  Constraint~\eqref{eq: Top dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}, and constraint~\eqref{eq: Top dual hinge c2} is satisfied since no~$\alpha_i$ is updated. Constraint~\eqref{eq: Top dual hinge c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \betak + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i 
    & \quad \implies \quad
    -\betak \leq \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak, \\
    0 \leq \betal - \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i
    & \quad \implies \quad
    \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i \leq \Delta \leq \betal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ If~$K = 1,$ the upper bound in~\eqref{eq: Top dual hinge c3} is always satisfied due to~\eqref{eq: Top dual hinge c1} and the lower and upper bound of~$\Delta$ can be simplified.
  
  Using the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Top dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\subsubsection{Quadratic Hinge Loss}

The second considered surrogate function is the quadratic hinge loss from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate hinge} of the quadratic hinge loss into the dual formulation~\eqref{eq: toppushk family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  - \frac{1}{4C} \sum_{i = 1}^{\npos} \alpha_i^2
  }{\label{eq: Top dual quadratic}}{\label{eq: Top dual quadratic L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Top dual quadratic c1}}
  \addConstraint{0 \leq \alpha_i}{,}{i = 1, 2, \ldots, \npos
  \label{eq: Top dual quadratic c2}}
  \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad}{j = 1, 2, \ldots, \ntil,
  \label{eq: Top dual quadratic c3}}
\end{maxi!}
Similarly to the previous case, the form of~$\K$ and~$\ntil$ depends on the used formulation and the upper bound in~\eqref{eq: Top dual quadratic c3} can be omitted for~$K = 1.$

\begin{proposition}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Top dual quadratic}]\label{prop: toppushk family quadratic update a,a}
  Consider problem~\eqref{eq: Top dual quadratic}, update rule~\eqref{eq: update rule a,a}, indeices~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = -\alphak, &
    \Delta_{ub} & = \alphal, &
    \gamma & = -\frac{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}}.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Top dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Top dual quadratic c3} is also always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Top dual quadratic c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta
    & \quad \implies \quad
    - \alphak \leq \Delta, \\
    0 \leq \alphal - \Delta
    & \quad \implies \quad
    \Delta \leq \alphal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$
  
  Using the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Top dual quadratic L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}} \Delta^2
    - \Brac[s]{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\begin{proposition}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Top dual quadratic}]\label{prop: toppushk family quadratic update a,b}
  Consider problem~\eqref{eq: Top dual quadratic}, update rule~\eqref{eq: update rule a,b}, indeices~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil$  and Notation~\ref{not: dual update rules}. Let us define
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        \max \Brac[c]{- \alphak,\;  -\betal} & K = 1, \\
        \max \Brac[c]{- \alphak,\;  -\betal, \; K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        + \infty & K = 1, \\
        \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal} & \textrm{otherwise},
      \end{cases*} \\
    \gamma & = -\frac{s_k + s_l - 1 + \frac{1}{2C} \alphak}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C}}.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Top dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraint~\eqref{eq: Top dual quadratic c2} reads
  \begin{equation*}
    0 \leq \alphak + \Delta
    \quad \implies \quad
    - \alphak \leq \Delta.
  \end{equation*}
  Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Top dual quadratic c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \beta_{\max} \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K} 
    & \quad \implies \quad
    K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i \leq \Delta, \\
    0 \leq \betal + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K}
    & \quad \implies \quad
    -\betal \leq \Delta \quad \land \quad \Delta \leq \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}.
  \end{align*}
  The combination of these bounds yields the lower bound~$\Delta_{lb}$ and upper bound~$\Delta_{ub}.$ If~$K = 1,$ the upper bound in~\eqref{eq: Top dual quadratic c3} is always satisfied due to~\eqref{eq: Top dual quadratic c1} and the lower and upper bound of~$\Delta$ can be simplified.
  
  Using the update rule~\eqref{eq: update rule a,b}, objective function~\eqref{eq: Top dual quadratic L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C}} \Delta^2
    - \Brac[s]{s_k + s_l - 1 + \frac{1}{2C} \alphak} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\begin{proposition}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Top dual quadratic}]\label{prop: toppushk family quadratic update b,b}
  Consider problem~\eqref{eq: Top dual quadratic}, update rule~\eqref{eq: update rule b,b}, indices~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil$  and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        -\betak & K = 1, \\
        \max \Brac[c]{- \betak,\; \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        \betal & K = 1, \\
        \min \Brac[c]{\betal,\; \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak} & \textrm{otherwise},
      \end{cases*} \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Top dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Top dual quadratic c2} is also always satisfied since no~$\alpha_i$ is updated. Constraint~\eqref{eq: Top dual quadratic c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \betak + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i 
    & \quad \implies \quad
    -\betak \leq \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak, \\
    0 \leq \betal - \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i
    & \quad \implies \quad
    \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i \leq \Delta \leq \betal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ If~$K = 1,$ the upper bound in~\eqref{eq: Top dual quadratic c3} is always satisfied due to~\eqref{eq: Top dual quadratic c1} and the lower and upper bound of~$\Delta$ can be simplified.
  
  Using the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Top dual quadratic L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\pagebreak

\subsubsection{Initialization}

\topinit*
\begin{proof}[Proof of Theorem~\ref{thm: toppushk family initialization} on page~\pageref{thm: toppushk family initialization}]
  The Lagrangian of~\eqref{eq: toppushk family initialization} reads
  \begin{align*}
    \mathcal{L}(\bm{\alpha}, \bm{\beta}; \lambda, \bm{p}, \bm{q}, \bm{u}, \bm{v})
     = \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
     + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
     + \lambda \Brac{\sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\ntil} \beta_j} \\
     - \sum_{i = 1}^{\npos} p_i \alpha_i
     + \sum_{i = 1}^{\npos} q_i (\alpha_i - C_1)
     - \sum_{j = 1}^{\ntil} u_j \beta_j
     + \sum_{j = 1}^{\ntil} v_j \Brac{\beta_j - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i}.
  \end{align*}
  The KKT conditions then amount to
  \begin{subequations}\label{eq:problem3_KKT}
  \begin{align}
    \frac{\partial \mathcal{L}}{\partial \alpha_i}
      & = \alpha_i - \alpha_i^0 + \lambda - p_i + q_i - \frac{1}{K} \sum_{j=1}^{\ntil} v_j = 0,
      && i = 1, 2, \ldots, \npos, \label{eq:problem3_KKT_opt1}\\
    \frac{\partial \mathcal{L}(\cdot)}{\partial \beta_j}
      & = \beta_j- \beta_j^0 - \lambda - u_j + v_j = 0,
      && j = 1, 2, \ldots, \ntil, \label{eq:problem3_KKT_opt2}
  \end{align}
  the primal feasibility conditions~\eqref{eq: toppushk family initialization}, the dual feasibility conditions $\lambda \in \R$, $p_i \ge 0$, $q_i \ge 0$, $u_j \ge 0$, $v_j \ge 0$ and finally the complementarity conditions
  \begin{align}
    p_i \alpha_i & = 0,
      && i = 1, 2, \ldots, \npos, \label{eq:problem3_KKT_comp1} \\
    q_i \Brac{\alpha_i - C_1} & = 0,
      && i = 1, 2, \ldots, \npos, \label{eq:problem3_KKT_comp2} \\
    u_j \beta_j & = 0,
      && j = 1, 2, \ldots, \ntil, \label{eq:problem3_KKT_comp3} \\
    v_j \Brac{\beta_j -\frac{1}{K} \sum_{i=1}^{\npos} \alpha_i} & =0,
      && j = 1, 2, \ldots, \ntil. \label{eq:problem3_KKT_comp4}
  \end{align}
  \end{subequations}
  
  \paragraph*{Case 1:} The first case concerns when the optimal solution satisfies~$\sum_i  \alpha_i = 0$. From the primal feasibility conditions, we immediately get~$\alpha_i = 0$ for all~$i$ and~$\beta_j = 0$ for all~$j$. Then~\eqref{eq:problem3_KKT_comp2} implies~$q_i = 0$ for all~$i$ and all complementarity conditions are satisfied. Moreover, optimality condition~\eqref{eq:problem3_KKT_opt1} implies
  \begin{equation*}
    \lambda = \alpha_i^0 + p_i + \frac{1}{K} \sum_{j = 1}^{\ntil} v_j.
  \end{equation*}
  Since the only condition on $p_i$ is the non-negativity, this implies
  \begin{equation*}
    \lambda \ge \max_{i=1, \ldots, \npos} \alpha_i^0 + \frac{1}{K} \sum_{j = 1}^{\ntil} v_j.
  \end{equation*}

  Similarly, from optimality condition~\eqref{eq:problem3_KKT_opt2} we deduce
  \begin{equation*}
    v_j
      = \beta_j^0 + \lambda + u_j
      \ge \beta_j^0 + \lambda
      \ge \beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0 + \frac{1}{K} \sum_{i = 1}^{\ntil} v_i.
  \end{equation*}
  Since we need to fulfill $v_j \ge 0$, this amounts to
  \begin{equation*}
    v_j 
      \ge \clip[u]{0}{+\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0 + \frac{1}{K} \sum_{i = 1}^{\ntil} v_i}.
  \end{equation*}
  Summing this with respect to $j$ and using the substitution $\bar{v} = \frac{1}{K}\sum_i v_i$ results in
  \begin{equation}\label{eq:problem3_proof0}
    K\bar{v} - \sum_{j=1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0 + \bar{v}} \geq 0.
  \end{equation}
  Denote by~$\beta_{[j]}^0$ the sorted version of~$\beta_j^0$. Then the function on the left-hand side of~\eqref{eq:problem3_proof0} as a function of~$\bar{v}$ is increasing on~$\left(-\infty, \; -\beta_{[\npos - K + 1]}^0 - \max_{i} \alpha_i^0 \right]$ and non-decreasing otherwise. Thus,~\eqref{eq:problem3_proof0} can be satisfied if and only if its function value at~$- \beta_{[\npos - K + 1]}^0 - \max_{i} \alpha_i^0$ is non-negative. But this is precisely the violation of~\eqref{eq:problem3_cond}.
  
  \paragraph*{Case 2:} If~\eqref{eq:problem3_cond} holds true, then from the discussion above we obtain that the optimal solution satisfies~$\sum_i \alpha_i > 0$. For simplicity, we define
  \begin{align*}
    \bar{\alpha} & = \frac{1}{K} \sum_{i=1}^{\npos} \alpha_i, &
    \bar{\beta} & = \frac{1}{K} \sum_{j=1}^{\ntil} \beta_j, &
    \bar{v} & = \frac{1}{K} \sum_{j=1}^{\ntil} v_j.
  \end{align*}
  For any fixed~$i$, the standard trick is to combine the optimality condition~\eqref{eq:problem3_KKT_opt1} with the primal feasibility condition~$0 \le \alpha_i \le C_1$, the dual feasibility conditions $p_i \ge 0$, $q_i \ge 0$ and the complementarity conditions~(\ref{eq:problem3_KKT_comp1}, \ref{eq:problem3_KKT_comp2}) to obtain
  \begin{equation}\label{eq:problem3_alpha}
    \alpha_i = \clip{0}{C_1}{\alpha_i^0 - \lambda + \bar{v}}.
  \end{equation}
  
  Similarly for any fixed~$j$, we combine the optimality condition~\eqref{eq:problem3_KKT_opt2} with the primal feasibility condition~$0 \le \beta_j \le \bar{\alpha}$, the dual feasibility conditions $u_j \ge 0$, $v_j \ge 0$ and the complementarity conditions~(\ref{eq:problem3_KKT_comp3}, \ref{eq:problem3_KKT_comp4}) to obtain
  \begin{align}
    \beta_j
      & = \clip{0}{\bar{\alpha}}{\beta_j^0 + \lambda}, \label{eq:problem3_beta} \\
    v_j
      & = \clip[u]{0}{+\infty}{\beta_j^0 + \lambda - \bar{\alpha}}. \label{eq:problem3_rho}
  \end{align}
  Summing equations~\eqref{eq:problem3_alpha},~\eqref{eq:problem3_beta} and~\eqref{eq:problem3_rho} respectively with respect to~$i$ and~$j$ results in
  \begin{subequations}
    \begin{align}
      K \bar{\alpha}
        & = \sum_{i=1}^{\npos}\clip{0}{C_1}{\alpha_i^0 - \lambda + \bar{v}},
        \label{eq:problem3_proof1} \\
      K \bar{\beta}
        &= \sum_{j=1}^{\ntil} \clip{0}{\bar{\alpha}}{\beta_j^0 + \lambda},
        \label{eq:problem3_proof2} \\
      K \bar{v}
        & = \sum_{j=1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \lambda - \bar{\alpha}}.
        \label{eq:problem3_proof3}
    \end{align}
  \end{subequations}
  We denote $\mu = \bar{\alpha}$. Then~\eqref{eq: toppushk family init alg 1} results by plugging~\eqref{eq:problem3_proof3} into~\eqref{eq:problem3_proof1} while~\eqref{eq: toppushk family init alg 2} follows from~\eqref{eq:problem3_proof2} and $\sum_i \alpha_i = \sum_j \beta_j$. 
\end{proof}

\topinith*
\begin{proof}[Proof of Lemma~\ref{lemma: toppushk family h} on page~\pageref{lemma: toppushk family h}]
  Recall that based on~\eqref{eq: toppushk family init alg 2} we defined
  \begin{equation*}
    g(\lambda; \mu) := \sum_{j=1}^{\ntil} \clip{0}{\mu}{\beta_j^0 + \lambda} - K \mu,
  \end{equation*}
  and solutions of~$g(\lambda; \mu) = 0$ for a fixed~$\mu$ are denoted by~$\lambda(\mu)$.
  
  Let us first consider the case, when the solution to~$g(\lambda) = 0$ is not unique. Since function~$g(\cdot; \, \mu)$ is non-decreasing and~$K$ is an integer, it can happen only if the solution~$\lambda(\mu)$ satisfies
  \begin{equation*}
    \beta_{[j]}^0 + \lambda(\mu) \;
    \begin{cases}
      \ge \mu & \text{for } j = \ntil - K + 1, \dots , \ntil, \\
      \le 0 & \text{otherwise.}
    \end{cases}
  \end{equation*}
  Here, we again denote~$\bm{\beta}_{[\cdot]}^0$ to be the sorted version of~$\bm{\beta}_j^0$. Then~$h$ defined in~\eqref{eq: toppushk family h} equals to
  \begin{align*}
    h(\mu)
      & = \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu) + \frac{1}{K} \sum_{j=\ntil - K + 1}^{\ntil} \Brac{\beta_j^0+\lambda(\mu) - \mu}} - K \mu \\
      & = \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \mu + \frac{1}{K} \sum_{j=\ntil - K + 1}^{\ntil} \beta_j^0} - K \mu.
  \end{align*}
  This implies the first statement of the lemma that~$h$ is independent of the choice of~$\lambda(\mu)$.
  
  In the previous paragraph, we prove, that~$h$ gives the same value for every choice of~$\lambda(\mu).$ Now we need to show that~$h$ is a decreasing function for the arbitrary choice of~$\lambda(\mu).$ Fix any~$\mu_2 > \mu_1 > 0$. From~\eqref{eq: toppushk family init alg 2} we have
  \begin{align}
   \sum_{j=1}^{\ntil} \clip{0}{\mu_1}{\beta_j^0 + \lambda(\mu_1)} - K \mu_1 & = 0,
    \label{eq:system2_proof1} \\
  \sum_{j=1}^{\ntil} \clip{0}{\mu_2}{\beta_j^0 + \lambda(\mu_2)} - K \mu_2 & = 0.
    \label{eq:system2_proof2}
  \end{align}
  Equation~\eqref{eq:system2_proof1} implies that at most~$K$ values of~$\beta_j^0 + \lambda(\mu_1)$ are greater or equal than~$\mu_1$. If we increase the upper bound in the projection, at most~$K$ values can increase, which results in
  \begin{equation}\label{eq:system2_proof3}
    \sum_{j=1}^{\ntil} \clip{0}{\mu_2}{\beta_j^0 + \lambda(\mu_1)}
      \le \sum_{j=1}^{\ntil} \clip{0}{\mu_1}{\beta_j^0 + \lambda(\mu_1)} + K(\mu_2 - \mu_1)
      = K \mu_2,
  \end{equation}
  where the equality follows from~\eqref{eq:system2_proof1}. Comparing~\eqref{eq:system2_proof2} and~\eqref{eq:system2_proof3} yields~$\lambda(\mu_2) \ge \lambda(\mu_1)$.
  
  Now define
  \begin{equation*}
    J = \Set{j}{\beta_j^0 + \lambda(\mu_1) \ge 0}
  \end{equation*}
  and observe that due to~\eqref{eq:system2_proof1} we have~$\abs{J} \ge K$. Moreover, the definition of $J$ and~\eqref{eq:system2_proof1} yields
  \begin{equation}\label{eq:system2_proof4}
    \sum_{j \in J} \clip{0}{\mu_1}{\beta_j^0 + \lambda(\mu_1)} - K\mu_1
      = \sum_{j=1}^{\ntil} \clip{0}{\mu_1}{\beta_j^0 + \lambda(\mu_1)} - K\mu_1
      = 0.
  \end{equation}
  Then we have
  \begin{align*}
    \sum_{j=1}^{\ntil} \clip{0}{\mu_2}{\beta_j^0 + \lambda(\mu_1) + \mu_2 - \mu_1}
      & \ge \sum_{j \in J} \clip{0}{\mu_2}{\beta_j^0 + \lambda(\mu_1) + \mu_2 - \mu_1} \\
      &  = \sum_{j \in J} \clip{\mu_2 - \mu_1}{\mu_2}{\beta_j^0 + \lambda(\mu_1) + \mu_2 - \mu_1} \\
      & = \sum_{j \in J} \clip{0}{\mu_1}{\beta_j^0 + \lambda(\mu_1)} + \abs{J}(\mu_2 - \mu_1) \\
      &  = K\mu_1 + \abs{J}(\mu_2 - \mu_1)
      \ge K\mu_1 + K(\mu_2 - \mu_1)
      = K\mu_2,
  \end{align*}
  where the first equality follows from the definition of~$J$ and the second equality is a shift by a~$\mu_2- \mu_1.$ The third equality follows from~\eqref{eq:system2_proof4} and finally, the last inequality follows from~$\abs{J} \ge K$. The chain above together with~\eqref{eq:system2_proof2} implies~$\lambda(\mu_2) - \mu_2 \le \lambda(\mu_1)- \mu_1$. Combining this with~$\mu_2 > \mu_1$ and~$\lambda(\mu_2) \ge \lambda(\mu_1)$, this implies that~$h$ from~\eqref{eq: toppushk family h} is non-increasing which is precisely the lemma statement.
\end{proof}

\subsection{Family of \PatMat Formulations}\label{sec: Pat coordinate descent}

In this section, we derive a coordinate descent algorithm for solving dual formulation~\eqref{eq: patmat family dual} for the family of \PatMat formulations. We follow the same approach as for \TopPushK family in Section\ref{sec: Top coordinate descent}, i.e. we use update rules~\eqref{eq: update rules}. In this case, we must also consider the third primary variable~$\delta.$ Then the dual formulation~\eqref{eq: patmat family dual}  can be rewritten as a one-dimensional quadratic problem
\begin{maxi*}{\Delta}{
  -\frac{1}{2} a(\bm{\alpha}, \bm{\beta}, \delta) \Delta^2
  - b(\bm{\alpha}, \bm{\beta}, \delta) \Delta
  - c(\bm{\alpha}, \bm{\beta}, \delta)
  }{}{}
  \addConstraint{\Delta_{lb}(\bm{\alpha}, \bm{\beta}, \delta)}{\leq \Delta \leq \Delta_{ub}(\bm{\alpha}, \bm{\beta}, \delta)}
\end{maxi*}
where~$a,$~$b,$~$c,$~$\Delta_{lb},$~$\Delta_{ub}$ are constants with respect to~$\Delta.$ The form of the optimal solution is the same as for problem~\eqref{eq: toppushk family dual} and reads
\begin{equation*}
  \Delta^{\star} = \clip{\Delta_{lb}}{\Delta_{ub}}{\gamma}.
\end{equation*}
Since we assume one of the update rule~\eqref{eq: update rules}, the constrain~\eqref{eq: patmat family dual c1} is always satisfied after the update. The exact form of the update rules depends on the surrogate function. Moreover, the form of optimal~$\delta$ also depends on the surrogate function. The upcoming text follows the same order as in the previous section. Therefore, we introduce concrete forms of update rules for hinge and quadratic hinge loss function and then show how to find an initial feasible solution.

\subsubsection{Hinge Loss}

We again start with the hinge loss function from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate hinge} of the hinge loss into the dual formulation~\eqref{eq: patmat family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  + \frac{1}{\vartheta} \sum_{j = 1}^{\ntil} \beta_j 
  - \delta \ntil \tau
  }{\label{eq: Pat dual hinge}}{\label{eq: Pat dual hinge L}}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j \label{eq: Pat dual hinge c1}}
  \addConstraint{0 \leq \alpha_i}{\leq C,}{i = 1, 2, \ldots, \npos \label{eq: Pat dual hinge c2}}
  \addConstraint{0 \leq \beta_j}{\leq \delta \vartheta, \quad}{j = 1, 2, \ldots, \ntil \label{eq: Pat dual hinge c3}}
  \addConstraint{\delta }{\geq 0. \label{eq: Pat dual hinge c4}}
\end{maxi!}
Since we know the form of the optimal solution~\eqref{eq: Delta optimal}, we only need to show how to compute~$\Delta_{lb},$~$\Delta_{ub}$ and~$\gamma$ for all update rules~\eqref{eq: update rules}. However, in this case, constants~$\Delta_{lb},$~$\Delta_{ub}$ and~$\gamma$ also depend on the third dual variable~$\delta$. We do not perform a joint maximization in~$(\alphak, \; \betal, \; \delta)$ but perform a maximization with respect to~$(\alphak, \; \betal)$, update these two values and then optimize the objective with respect to~$\delta$. Then for fixed feasible solution~$\bm{\alpha}$ and~$\bm{\beta},$ maximizing objective function~\eqref{eq: Pat dual hinge L} with respect to~$\delta$ yields
\begin{maxi*}{\delta}{
  - \ntil \tau \delta
  }{}{}
  \addConstraint{0 \leq \beta_j}{\leq \delta \vartheta, \quad}{j = 1, 2, \ldots, \ntil}
  \addConstraint{\delta \geq 0.}
\end{maxi*}
Since~$\ntil \tau \geq 0,$ we have to find the smallest possible~$\delta$ that satisfies constraints above. Such~$\delta$ is in the following form
\begin{equation}\label{eq: Pat dual hinge optimal delta}
  \delta^* = \frac{1}{\vartheta} \max_{j \in \{1, 2, \ldots, \ntil \}} \beta_j.
\end{equation}

The following three propositions provide closed-form formulas for all three update rules.

\begin{proposition}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Pat dual hinge}]\label{thm: patmat family hinge update a,a}
  Consider problem~\eqref{eq: Pat dual hinge}, update rule~\eqref{eq: update rule a,a}, indices~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos$  and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \min\{- \alphak,\; \alphal - C\}, &
    \Delta_{ub} & = \max\{C - \alphak,\; \alphal\}, \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
    \delta^{\star} & = \delta.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Pat dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Pat dual hinge c3} is also always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Pat dual hinge c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta \leq C
    & \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak \\
    0 \leq \alphal - \Delta \leq C
    & \quad \implies \quad
    \alphal - C \leq \Delta \leq \alphal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$
  
  Using the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}. Finally, since optimal~$\delta$ is given by~\eqref{eq: Pat dual hinge optimal delta} and no~$\beta_j$ was updated, the optimal~$\delta$ does not change.
\end{proof}

\begin{proposition}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Pat dual hinge}]\label{thm: patmat family hinge update a,b}
  Consider problem~\eqref{eq: Pat dual hinge}, update rule~\eqref{eq: update rule a,b}, indices~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil$  and Notation~\ref{not: dual update rules}. Let us define
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the bounds from~\eqref{eq: Delta optimal} are defined as~$\Delta_{lb} = \max\{- \alphak,\; -\betal \}$ and~$\Delta_{ub} = C - \alphak$ and there are two possible solutions
  \begin{enumerate}
    \item $\Delta^{\star}_1$ is feasible if~$\betal + \Delta^{\star}_1 \leq \beta_{\max}$ and is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k + s_l - 1 - \frac{1}{\vartheta}}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}, &
      \delta^{*}_1
        & = \frac{\beta_{\max}}{\vartheta}.
    \end{align*}
    \item $\Delta^{\star}_2$ is feasible if~$\betal + \Delta^{\star}_2 \geq \beta_{\max}$ and is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k + s_l - 1 - \frac{1 - \ntil \tau}{\vartheta}}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}, &
      \delta^{*}_2
        & = \frac{\betal + \Delta^{\star}_2}{\vartheta}.
    \end{align*}
  \end{enumerate}
  The optimal solution~$\Delta^{\star}$ is equal to one of them, which maximizes the original objective and is feasible.
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Pat dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraint~\eqref{eq: Pat dual hinge c2} reads~$- \alphak \leq \Delta \leq C - \alphak.$ Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Pat dual hinge c3} reads~$\beta_{\max} \leq \delta \vartheta$ and~$0 \leq \betal + \Delta \leq \delta \vartheta.$ Since the optimal~$\delta$ is given by~\eqref{eq: Pat dual hinge optimal delta}, there are only two possible choices: $\delta^{\star}_1 = \frac{\beta_{\max}}{\vartheta}$ and~$\delta^{\star}_2 = \frac{\betal + \Delta}{\vartheta}.$ If~$\delta$ is feasible, all upper bounds in constraint~\eqref{eq: Pat dual hinge c3} hold. Therefore, we can simplify the constraints to~$- \betal \leq \Delta,$ which in combination with bounds for~$\alphak$ gives the lower and upper bound of~$\Delta.$ Now let us discuss how to select optimal~$\delta:$
  \begin{enumerate}
    \item Using~$\delta^{\star}_1$ and the update rule~\eqref{eq: update rule a,b}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}} \Delta^2
      - \Brac[s]{s_k + s_l - 1 - \frac{1}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_1$ is given by~\eqref{eq: Delta optimal} and is feasible if~$\betal + \Delta^{\star}_1 \leq \beta_{\max}$.

    \item Using~$\delta^{\star}_2$ and the update rule~\eqref{eq: update rule a,b}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}} \Delta^2
      - \Brac[s]{s_k + s_l - 1 - \frac{1 - \ntil \tau}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_2$ is given by~\eqref{eq: Delta optimal} and is feasible if~$\betal + \Delta^{\star}_2 \geq \beta_{\max}$.
  \end{enumerate}
  The optimal solution is the one, which maximizes the objective~\eqref{eq: Pat dual hinge L} and is feasible.
\end{proof}

\begin{proposition}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Pat dual hinge}]\label{thm: patmat family hinge update b,b}
  Consider problem~\eqref{eq: Pat dual hinge}, update rule~\eqref{eq: update rule b,b}, indices~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Let us define
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{k}, \hat{l}\}} \beta_j.
  \end{equation*}
  Then the bounds from~\eqref{eq: Delta optimal} are defined as~$\Delta_{lb} = - \betak$ and~$\Delta_{ub} = \betal$ and there are three possible solutions
  \begin{enumerate}
    \item $\Delta^{\star}_1$ is feasible if~$\beta_{\max} \geq \max\{\betak + \Delta^{\star}_1, \betal - \Delta^{\star}_1\}$ and is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
      \delta^{*}_1
        & = \frac{\beta_{\max}}{\vartheta}.
    \end{align*}
    \item $\Delta^{\star}_2$ is feasible if~$\betak + \Delta^{\star}_2 \geq \max\{\beta_{\max} , \betal - \Delta^{\star}_2\}$ and is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k - s_l + \frac{\ntil \tau}{\vartheta}}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
      \delta^{*}_2
        & = \frac{\betak + \Delta^{\star}_2}{\vartheta}.
    \end{align*}
    \item $\Delta^{\star}_3$ is feasible if~$\betal - \Delta^{\star}_3 \geq \max\{\betak + \Delta^{\star}_3, \beta_{\max}\}$ and is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k - s_l - \frac{\ntil \tau}{\vartheta}}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
      \delta^{*}_3
        & = \frac{\betal - \Delta^{\star}_3}{\vartheta}.
    \end{align*}
  \end{enumerate}
  The optimal solution~$\Delta^{\star}$ is equal to one of them, which maximizes the original objective and is feasible.
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Pat dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Pat dual hinge c2} is also always satisfied since no~$\alpha_i$ is updated. Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Pat dual hinge c3} reads
  \begin{align*}
    \beta_{\max} & \leq \delta \vartheta, \\
    0 \leq \betak + \Delta & \leq \delta \vartheta, \\
    0 \leq \betal - \Delta & \leq \delta \vartheta.
  \end{align*}
  Since the optimal~$\delta$ is given by~\eqref{eq: Pat dual hinge optimal delta}, there are only two possible choices
  \begin{align}\label{eq: Pat dual hinge b, b proof delta}
    \delta^{\star}_1 & = \frac{\beta_{\max}}{\vartheta}, &
    \delta^{\star}_2 & = \frac{\betak + \Delta}{\vartheta}, &
    \delta^{\star}_3 & = \frac{\betal - \Delta}{\vartheta}.
  \end{align}
  If we use any of these choices which is feasible, all upper bounds in constraint~\eqref{eq: Pat dual hinge c3} hold, i.e. we can simplify the constraints to
  \begin{align*}
    0 \leq \betak + \Delta
    & \quad \implies \quad
    - \betak \leq \Delta, \\
    0 \leq \betal - \Delta
    & \quad \implies \quad
    \Delta \leq \betal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Now let us discuss how to select optimal~$\delta:$
  \begin{enumerate}
    \item Using~$\delta^{\star}_1$ from~\eqref{eq: Pat dual hinge b, b proof delta} and the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_1$ is given by~\eqref{eq: Delta optimal} and is feasible if
    \begin{equation*}
      \beta_{\max} \geq \max\{\betak + \Delta^{\star}_1, \; \betal - \Delta^{\star}_1\}.
    \end{equation*}

    \item Using~$\delta^{\star}_2$ from~\eqref{eq: Pat dual hinge b, b proof delta} and the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l + \frac{\ntil \tau}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_2$ is given by~\eqref{eq: Delta optimal} and is feasible if
    \begin{equation*}
      \betak + \Delta^{\star}_2 \geq \max\{\beta_{\max} , \betal - \Delta^{\star}_2\}.
    \end{equation*}

    \item Using~$\delta^{\star}_3$ from~\eqref{eq: Pat dual hinge b, b proof delta} and the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l - \frac{\ntil \tau}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_3$ is given by~\eqref{eq: Delta optimal} and is feasible if
    \begin{equation*}
      \betal - \Delta^{\star}_3 \geq \max\{\beta_{\max}, \betak + \Delta^{\star}_3\}.
    \end{equation*}
  \end{enumerate}
  The optimal solution is the one, which maximizes the objective~\eqref{eq: Pat dual hinge L} and is feasible.
\end{proof}

\subsubsection{Quadratic Hinge Loss}

The second considered surrogate function is the quadratic hinge loss from Notation~\ref{not: surrogates}. Plugging the conjugate~\eqref{eq: conjugate quadratic hinge} of the quadratic hinge loss into the dual formulation~\eqref{eq: patmat family dual} yields
\begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
  - \frac{1}{2} \vecab^\top \K \vecab
  + \sum_{i = 1}^{\npos} \alpha_i
  - \frac{1}{4C} \sum_{i = 1}^{\npos} \alpha_i^2
  }{\label{eq: Pat dual quadratic}}{\label{eq: Pat dual quadratic L1}}
  \breakObjective{
    + \frac{1}{\vartheta} \sum_{j = 1}^{\ntil} \beta_j 
    - \frac{1}{4 \delta \vartheta^2} \sum_{j = 1}^{\ntil} \beta_j^2
    - \delta \ntil \tau \label{eq: Pat dual quadratic L2}
  }
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j
  \label{eq: Pat dual quadratic c1}}
  \addConstraint{\alpha_i}{\geq 0,}{i = 1, 2, \ldots, \npos
  \label{eq: Pat dual quadratic c2}}
  \addConstraint{\beta_j}{\geq 0,}{j = 1, 2, \ldots, \ntil
  \label{eq: Pat dual quadratic c3}}
  \addConstraint{\delta }{\geq 0,
  \label{eq: Pat dual quadratic c4}}
\end{maxi!}
Similar to the previous case, we perform maximization only with respect to~$(\alphak, \; \betal).$ Then for fixed feasible solution~$\bm{\alpha},$~$\bm{\beta},$ we need to maximize the objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) with respect to~$\delta$, which leads to the following problem
\begin{maxi*}{\delta}{
  - (\ntil \tau) \delta - \Brac{\frac{1}{4\vartheta^2} \sum_{j = 1}^{\ntil} \beta_j^2} \frac{1}{\delta}
  }{}{}
  \addConstraint{\delta \geq 0,}
\end{maxi*}
with the optimal solution that equals to
\begin{equation}\label{eq: Pat dual quadratic optimal delta}
  \delta^* = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \sum_{j = 1}^{\ntil} \beta_j^2}.
\end{equation}

The following three propositions provide closed-form formulas for all three update rules.

\begin{proposition}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Pat dual quadratic}]\label{thm: patmat family quadratic update a,a}
  Consider problem~\eqref{eq: Pat dual quadratic}, update rule~\eqref{eq: update rule a,a}, indices~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos$  and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = -\alphak, \\
    \Delta_{ub} & = \alphal, \\
    \gamma & = -\frac{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}}, \\
    \delta^{\star}  & = \delta.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Pat dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Pat dual quadratic c3} is also always satisfied since no~$\beta_j$ was updated. Constraint~\eqref{eq: Pat dual quadratic c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta
    & \quad \implies \quad
    - \alphak \leq \Delta, \\
    0 \leq \alphal - \Delta
    & \quad \implies \quad
    \Delta \leq \alphal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$
  
  Using the update rule~\eqref{eq: update rule a,a}, objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}} \Delta^2
    - \Brac[s]{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}. Finally, since optimal~$\delta$ is given by~\eqref{eq: Pat dual quadratic optimal delta} and no~$\beta_j$ was updated, the optimal~$\delta$ does not change.
\end{proof}

\begin{proposition}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Pat dual quadratic}]\label{thm: patmat family quadratic update a,b}
  Consider problem~\eqref{eq: Pat dual quadratic}, update rule~\eqref{eq: update rule a,b}, indices~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \max\{- \alphak, - \betal\}, \\
    \Delta_{ub} & = +\infty, \\
    \gamma      & = -\frac{s_k + s_l  - 1 + \frac{\alphak}{2C} - \frac{1}{\vartheta} + \frac{\betal}{2 \delta \vartheta^2}}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C} + \frac{1}{2 \delta \vartheta^2}}, \\
    \delta^{\star}  & = \sqrt{\delta^2 + \frac{1}{4 \vartheta^2 \ntil \tau}({\Delta^{\star}}^2 + 2 \Delta^{\star} \betal)}.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Pat dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraints~\eqref{eq: Pat dual quadratic c2} and~\eqref{eq: Pat dual quadratic c3} reads
  \begin{align*}
    0 \leq \alphak + \Delta
    & \quad \implies \quad
    - \alphak \leq \Delta, \\
    0 \leq \betal + \Delta
    & \quad \implies \quad
    - \betal \leq \Delta, \\
  \end{align*}
  which gives the lower bound of~$\Delta.$ In this case, $\Delta$ has no upper bound.
  
  Using the update rule~\eqref{eq: update rule a,b}, objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) can be rewritten as a quadratic function with respect to~$\Delta$
  \begin{align*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C} + \frac{1}{2 \delta \vartheta^2}} & \Delta^2 \\
    - \Brac[s]{s_k + s_l - 1 + \frac{\alphak}{2C} - \frac{1}{\vartheta} + \frac{\betal}{2\delta\vartheta^2}} & \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{align*}

  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}. We know that the optimal~$\delta^*$ is given by~\eqref{eq: Pat dual quadratic optimal delta}, then
  \begin{equation*}
    \delta^*
      = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \Brac{\sum_{j\neq \hat{l}} \beta_j^2 + (\betal + \Delta^\star)^2}}
      = \sqrt{\delta^2 + \frac{1}{4\vartheta^2 \ntil \tau} (\Delta^{\star2} + 2\Delta^\star \betal)}.
  \end{equation*}
\end{proof}

\begin{proposition}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Pat dual quadratic}]\label{thm: patmat family quadratic update b,b}
  Consider problem~\eqref{eq: Pat dual quadratic}, update rule~\eqref{eq: update rule b,b} indices~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil$ and Notation~\ref{not: dual update rules}. Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = - \betak, \\
    \Delta_{ub} & = \betal, \\
    \gamma      & = -\frac{s_k - s_l + \frac{1}{2\delta \vartheta^2}(\betak - \betal)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{\delta \vartheta^2}}, \\
    \delta^{\star}  & = \sqrt{\delta^2 + \frac{1}{2 \vartheta^2 \ntil \tau}({\Delta^{\star}}^2 + \Delta^{\star} (\betak - \betal))}.
  \end{align*}
\end{proposition}

\begin{proof}
  Constraint~\eqref{eq: Pat dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Pat dual quadratic c2} is also always satisfied since no~$\alpha_i$ is updated. Constraint~\eqref{eq: Pat dual quadratic c3} reads
  \begin{align*}
    0 \leq \betak + \Delta
    & \quad \implies \quad
    - \betak \leq \Delta, \\
    0 \leq \betal - \Delta
    & \quad \implies \quad
    \Delta \leq \betal, \\
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$
  
  Using the update rule~\eqref{eq: update rule b,b}, objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{\delta\vartheta^2}} \Delta^2
    - \Brac[s]{s_k - s_l + \frac{1}{2\delta\vartheta^2}(\betak - \betal)} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.   We know that the optimal~$\delta^*$ is given by~\eqref{eq: Pat dual quadratic optimal delta}, then
  \begin{equation*}
    \delta^*
      = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \Brac{\sum_{j \notin \{\hat{l}, \hat{k}\}} \beta_j^2 + (\betak + \Delta^\star)^2 + (\betal - \Delta^\star)^2}} 
      = \sqrt{\delta^2 + \frac{1}{2\vartheta^2 \ntil \tau} (\Delta^{\star2} + \Delta^\star (\betak - \betal))}.
  \end{equation*}
\end{proof}

\pagebreak

\subsubsection{Initialization}

As in the case of problem~\eqref{eq: toppushk family dual}, all update rules~\eqref{eq: update rules} assume that the current solution~$\bm{\alpha},$~$\bm{\beta},$~$\delta$ is feasible. So to create an iterative algorithm that solves problem~\eqref{eq: Pat dual hinge} or~\eqref{eq: Pat dual quadratic}, we need to have a way how to obtain an initial feasible solution. Such a task can be formally written as a projection of random variables~$\bm{\alpha}^0,$~$\bm{\beta}^0,$~$\delta^0$ to the feasible set of solutions
\begin{mini}{\bm{\alpha}, \bm{\beta}, \delta}{
  \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
  + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
  + \frac{1}{2} (\delta - \delta^0)^2
  }{\label{eq: patmat family initialization}}{}
  \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\ntil} \beta_j}
  \addConstraint{0 \leq \alpha_i}{\leq C_1, \quad i = 1, 2, \ldots, \npos}
  \addConstraint{0 \leq \beta_j}{\leq C_2 \delta, \quad j = 1, 2, \ldots, \ntil,}
  \addConstraint{\delta }{\geq 0,}
\end{mini}
where the upper bounds in the second and third constraints depend on the used surrogate function and are defined as follows
\begin{align*}
  C_1 & = \begin{cases}
    C & \text{for hinge loss}, \\
    +\infty & \text{for quadratic hinge loss},
  \end{cases} &
  C_2 & = \begin{cases}
    \vartheta & \text{for hinge loss}, \\
    +\infty & \text{for quadratic hinge loss}.
  \end{cases}
\end{align*}
Again, we will follow the same approach as in~\cite{adam2020projections} to solve this optimization problem. In the following theorem, we show that problem~\eqref{eq: patmat family initialization} can be written as a system of two equations of two variables~$\lambda$ and~$\mu.$ The theorem also shows the concrete form of feasible solution~$\bm{\alpha},$~$\bm{\beta},$~$\delta$ that depends only on~$\lambda$ and~$\mu.$

\begin{theorem}\label{thm: patmat family initialization}
  Consider problem~\eqref{eq: patmat family initialization} and some initial solution~$\bm{\alpha}^0,$~$\bm{\beta}^0$ and~$\delta^0.$ Then if the following condition holds
  \begin{equation}\label{eq: patmat family init alg condition}
    \delta^0 \le - C_2 \sum_{j = 1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0}.
  \end{equation}
  the optimal solution of~\eqref{eq: patmat family initialization} amounts to~$\bm{\alpha} = \bm{\beta} = \bm{0}$ and~$\delta^0 = 0.$ In the opposite case, the following system of two equations
  \begin{subequations}\label{eq: patmat family init alg}
    \begin{align}
    0
      & = \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda} - \sum_{j=1}^{\ntil} \clip{0}{\lambda + \mu}{\beta_j^0 + \lambda},
    \label{eq: patmat family init alg 1} \\
    \lambda
      & = C_2 \delta^0 + C_2^2 \sum_{j=1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 - \mu} - \mu.
    \label{eq: patmat family init alg 2}
    \end{align}
  \end{subequations}
  has a solution $(\lambda,\mu)$ with $\lambda+\mu>0$ and the optimal solution of~\eqref{eq: patmat family initialization} is equal to
  \begin{align*}
    \alpha_i & = \clip{0}{C_1}{\alpha_i^0 - \lambda}, \\
    \beta_j & = \clip{0}{\lambda + \mu}{\beta_j^0 + \lambda}, \\
    C_2 \delta &= \lambda + \mu.
  \end{align*}
\end{theorem}

\begin{proof}
  The Lagrangian of~\eqref{eq: patmat family initialization} reads
  \begin{align*}
    \mathcal{L}(\bm{\alpha}, \bm{\beta}; \lambda, \bm{p}, \bm{q}, \bm{u}, \bm{v})
      = \frac{1}{2} \norm{\bm{\alpha} - \bm{\alpha}^0}^2
      + \frac{1}{2} \norm{\bm{\beta} - \bm{\beta}^0}^2
      + \frac{1}{2} (\delta - \delta^0)^2
     + \lambda \Brac{\sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\ntil} \beta_j} \\
     - \sum_{i = 1}^{\npos} p_i \alpha_i
     + \sum_{i = 1}^{\npos} q_i (\alpha_i - C_1)
     - \sum_{j = 1}^{\ntil} u_j \beta_j
     + \sum_{j = 1}^{\ntil} v_j (\beta_j - C_2 \delta).
  \end{align*}
  The KKT conditions then amount to the optimality conditions
  \begin{subequations}\label{eq:problem2_KKT}
    \begin{align}
      \frac{\partial \mathcal{L}}{\partial \alpha_i}
        & = \alpha_i - \alpha_i^0 + \lambda - p_i + q_i = 0,
        && \quad i = 1, 2, \ldots, \npos, \label{eq:problem2_KKT_opt1} \\
      \frac{\partial \mathcal{L}(\cdot)}{\partial \beta_j}
        & = \beta_j - \beta_j^0 - \lambda - u_j + v_j = 0,
        && \quad j = 1, 2, \ldots, \ntil, \label{eq:problem2_KKT_opt2} \\
      \frac{\partial \mathcal{L}(\cdot)}{\partial \delta}
        & = \delta - \delta^0 - C_2 \sum_{j = 1}^{\ntil} v_j = 0,
        \label{eq:problem2_KKT_opt3}
    \end{align}
  the primal feasibility conditions~\eqref{eq: patmat family initialization}, the dual feasibility conditions~$\lambda \in \R$, $p_i \ge 0$, $q_i\ge0$, $u_j \ge 0$, $v_j \ge 0$ and finally the complementarity conditions
  \begin{align}
    p_i \alpha_i & = 0,
      && \quad i = 1, 2, \ldots, \npos, \label{eq:problem2_KKT_comp1} \\
    q_i \Brac{\alpha_i - C_1} & = 0,
      && \quad i = 1, 2, \ldots, \npos, \label{eq:problem2_KKT_comp2} \\
    u_j \beta_j & = 0,
      && \quad j = 1, 2, \ldots, \ntil, \label{eq:problem2_KKT_comp3} \\
    v_j \Brac{\beta_j - C_2 \delta} & =0,
      && \quad j = 1, 2, \ldots, \ntil. \label{eq:problem2_KKT_comp4}
  \end{align}
  \end{subequations}

  \paragraph*{Case 1:} The first case concerns when the optimal solution satisfies~$\delta=0$. From the primal feasibility conditions, we immediately get~$\alpha_i = 0$ for all~$i$ and~$\beta_j = 0$ for all~$j$. Then~\eqref{eq:problem2_KKT_comp2} implies~$q_i=0$ and all complementarity conditions are satisfied. Moreover,~\eqref{eq:problem2_KKT_opt1} implies for all~$i$
  \begin{equation*}
    \lambda = \alpha_i^0 + p_i.
  \end{equation*}
  Since the only condition on~$p_i$ is the non-negativity, this implies~$\lambda \ge \max_i \alpha_i^0$.
  
  Similarly, from~\eqref{eq:problem2_KKT_opt2} we deduce
  \begin{equation*}
    v_j
      = \beta_j^0 +\lambda + u_j
      \ge \beta_j^0 + \lambda
      \ge \beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0.
  \end{equation*}
  Since we also have the non-negativity constraint on $v_j$, this implies
  \begin{equation*}
    v_j \ge \clip[u]{0}{+\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0}.
  \end{equation*}
  
  Condition~\eqref{eq:problem2_KKT_opt3} implies
  \begin{equation*}
    \delta^0
      = -C_2 \sum_{j = 1}^{\ntil} v_j
      \le -C_2 \sum_{j = 1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \max_{i=1,\dots,\npos} \alpha_i^0}.
  \end{equation*}
  This corresponds to the first case in the theorem statement and the violation of condition~\eqref{eq: patmat family init alg condition}.

  \paragraph*{Case 2:} If~\eqref{eq: patmat family init alg condition} holds true, then from the discussion above we obtain that the optimal solution satisfies~$\delta > 0$. For any fixed~$i$, the standard trick is to combine the optimality condition~\eqref{eq:problem2_KKT_opt1} with the primal feasibility condition~$0 \le \alpha_i \le C_1$, the dual feasibility conditions~$p_i \ge 0$, $q_i \ge 0$ and the complementarity conditions~(\ref{eq:problem2_KKT_comp1}, \ref{eq:problem2_KKT_comp2}) to obtain
  \begin{equation}\label{eq:problem2_alpha}
    \alpha_i = \clip{0}{C_1}{\alpha_i^0 - \lambda}.
  \end{equation}

  Similarly for any fixed~$j$, we combine the optimality condition~\eqref{eq:problem2_KKT_opt2} with the primal feasibility condition~$0 \le \beta_j \le C_2 \delta$, the dual feasibility conditions~$u_j \ge 0,$ $v_j \ge 0$ and the complementarity conditions~(\ref{eq:problem2_KKT_comp3}, \ref{eq:problem2_KKT_comp4}) to obtain
  \begin{align}
    \beta_j & = \clip{0}{C_2 \delta}{\beta_j^0 + \lambda}, \label{eq:problem2_beta} \\
    v_j & = \clip[u]{0}{+\infty}{\beta_j^0 + \lambda - C_2 \delta}. \label{eq:problem2_rho}
  \end{align}

  Note that we now obtain the following system
  \begin{align*}
    \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda} - \sum_{j = 1}^{\ntil} \clip{0}{C_2 \delta}{\beta_j^0 + \lambda}
      & = 0, \\
    \delta - \delta^0 - C_2 \sum_{j = 1}^{\ntil} \clip[u]{0}{+\infty}{\beta_j^0 + \lambda - C_2 \delta}
      & = 0.
  \end{align*}
  Here, the first equation follows from plugging~\eqref{eq:problem2_alpha} and~\eqref{eq:problem2_beta} into the feasibility condition~$\sum_i \alpha_i =\sum_j \beta_j$ while the second equation follows from plugging~\eqref{eq:problem2_rho} into~\eqref{eq:problem2_KKT_opt3}. Finally, system~\eqref{eq: patmat family init alg} follows after making the substitution $C_2 \delta = \lambda + \mu$.
\end{proof}

System~\eqref{eq: patmat family init alg} is relatively simple to solve, since equation~\eqref{eq: patmat family init alg 2} provides an explicit formula for~$\lambda$. Let us denote it as $\lambda(\mu)$, then we denote the right-hand side of~\eqref{eq: patmat family init alg 1} as
\begin{equation}\label{eq: patmat family init alg h}
  h(\mu) :=
    \sum_{i=1}^{\npos} \clip{0}{C_1}{\alpha_i^0 - \lambda(\mu)} - \sum_{j=1}^{\ntil} \clip{0}{\lambda(\mu) + \mu}{\beta_j^0 + \lambda(\mu)}.
\end{equation}
Then the system of equations~\eqref{eq: patmat family init alg} is equivalent to solving~$h(\mu) = 0$. The following lemma states that~$h$ is a non-decreasing function in~$\mu$ on~$(0,\infty)$ and thus the equation~$h(\mu) = 0$ is simple to solve using any root-finding method. Note that if~$\delta^0 < 0$, then it may happen that~$\lambda + \mu < 0$ if the initial~$\mu$ is chosen large. In such a case, it suffices to decrease~$\mu$ until~$\lambda + \mu$ is positive.

\begin{lemma}\label{lemma: patmat family initialization h}
  Function $h$ is non-decreasing in~$\mu$ on~$(0,\infty)$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma: patmat family initialization h} on page~\pageref{lemma: patmat family initialization h}]
  Consider any~$\mu_1 < \mu_2$. Then from~\eqref{eq: patmat family init alg 2} we obtain both~$\lambda(\mu_1) \ge \lambda(\mu_2)$ and~$\mu_1+\lambda(\mu_1) \ge \mu_2 + \lambda(\mu_2)$. The statement then follows from the definition of~$h$ in~\eqref{eq: patmat family init alg h}.
\end{proof}
