\chapter{Appendix for Chapter~\ref{chap: dual}}\label{app: dual}

In this chapter we provide proofs and additional results for the Chapter~\ref{chap: dual}. In the first part, we introduce concept of conjugate functions. In the second part, we derive dual formulation to the formulations from Table~\ref{tab: summary formulations}. Finally, the last part focuses on how to efficiently solve these dual formulations.

\section{Convex Conjugate}
\begin{definition}[Convex conjugate~\cite{boyd2004convex}]\label{def: conjugate}
  Let~$l \colon \R^n \to \R.$ The function~$l^{\star} \colon \R^n \to \R,$ defined as
  \begin{equation*}
    l^{\star} (\bm{y})
      =  \sup_{\bm{x} \in \domain l} \{\bm{y}^{\top}\bm{x} - l(\bm{x})\}
      = -\inf_{\bm{x} \in \domain l} \{l(\bm{x}) - \bm{y}^{\top}\bm{x}\}.
  \end{equation*}
  is called conjugate function of~$l.$
\end{definition}
Recall the hinge loss and quadratic hinge loss function defined in Notation~\ref{not: surrogates} as follows
\begin{equation*}
  \begin{aligned}
    l_{\text{hinge}}(s) & = \max\Brac[c]{0, 1 + s}, \\
    l_{\text{quadratic}}(s) & = \Brac{\max\Brac[c]{0, 1 + s}}^2.\\
  \end{aligned}
\end{equation*}
The conjugate for the hinge loss can be found in~\cite{shnlev2014accelerated} and has the following form
\begin{equation}\label{eq: conjugate hinge}
  l_{\text{hinge}}^{\star}(y) =
  \begin{cases}
    -y & \text{if } y \in [0, 1], \\
    \infty & \text{otherwise.}
  \end{cases}  
\end{equation}
Similarly, the conjugate for the quadratic hinge is defuined in~\cite{kanamori2013conjugate} as
\begin{equation}\label{eq: conjugate quadratic hinge}
  l_{\text{quadratic}}^{\star}(y) =
  \begin{cases}
    \frac{y^2}{4} - y & \text{if } y \geq 0, \\
    \infty & \text{otherwise.}
  \end{cases}
\end{equation}

\section{Dual formulations}

In Section~\ref{sec:Derivation of dual problems} we divided all formulations from Table~\ref{tab: summary formulations} into two families. All formulations in these families use the same objective function and also use the same form of the decision threshold. In Theorems~\ref{thm: toppushk family dual} and~\ref{thm: patmat family dual} we derived the dual formulation for these two families. In this section, we derive dual formulations for formulations from Table~\ref{tab: summary formulations}. Then the Theorems~\ref{thm: toppushk family dual} and~\ref{thm: patmat family dual} are direct consequence of the theorems presented in the following sections. 

\subsection{Ranking Problems}

In this section, we derive the dual formulation of \TopPushK. Table~\ref{tab: summary formulations} shows, that \TopPush is a special is a special case of the \TopPushK for~$K = 1.$ Therefore, it is sufficient to show the dual form only for \TopPushK.

\begin{lemma}[\TopPushK alternative formulation.]\label{lem: TopPushK primal alternative}
  The problem~\eqref{eq: toppushK surrogate} with linear classifier can be equivalently written as follows
  \begin{maxi}{\bm{w}, t, \bm{y}, \bm{z}}{
    \frac{1}{2} \norm{\bm{w}}_{2}^{2}+ C \sum_{i = 1}^{\npos} l(y_i)
    }{\label{eq: TopPushK primal alternative}}{}
    \addConstraint{y_i}{= t + \frac{1}{K} \sum_{j = 1}^{\nneg} z_j - \bm{w}^{\top} \bm{x}^+_i, \quad}{i = 1, \; 2, \ldots, \; \npos.}
    \addConstraint{z_j}{\geq \bm{w}^{\top} \bm{x}^-_j - t,}{j = 1, \; 2, \ldots, \; \nneg}
    \addConstraint{z_j}{\geq 0,}{j = 1, \; 2, \ldots, \; \nneg}
  \end{maxi}
\end{lemma}
\begin{proof}
  Firstly, we rewrite the formula for the decision threshold from~\eqref{eq: toppushK surrogate} using the Lemma~1 from~\cite{ogryczak2003minimizing}
  \begin{equation*}
    \sum_{j = 1}^{K} s^{-}_{[j]} = \min_{t} \Brac[c]{Kt + \sum_{j = 1}^{\nneg} \max\{0, \; s^-_j - t\}}.
  \end{equation*}
  Substituing this formula into the objective function from~\eqref{eq: toppushK surrogate} we get
  \begin{align*}
    \sum_{i = 1}^{\npos} l\Brac{\frac{1}{K}\sum_{j = 1}^{K} s^{-}_{[j]} - s^+_{i}}
      & = \sum_{i = 1}^{\npos} l\Brac{ \frac{1}{K} \min_{t} \Brac[c]{Kt + \sum_{j = 1}^{\nneg} \max\Brac[c]{0, \; s^-_j - t}} - s^+_{i}} \\
      & = \min_{t} \; \sum_{i = 1}^{\npos} l\Brac{t + \frac{1}{K} \sum_{j = 1}^{\nneg} \max\Brac[c]{0, \; s^-_j - t} - s^+_{i}}.
  \end{align*}
  where the last equality follows from the fact, that the surrogate function is~$l$ is non-decreasing. The max operator can be replaced using auxiliary variable~$\bm{z} \in \R^{\nneg}$ which for all~$j = 1, \; 2, \ldots, \; \nneg$ fullfills~$z _j \geq s^-_j - t$ and at the same time~$z _j \geq 0.$ Moreover, we introduce new variable~$\bm{y} \in \R^{\nneg}$ defined for all~$i = 1, \; 2, \ldots, \; \npos$ as
  \begin{equation*}
    y_i = t + \frac{1}{K} \sum_{j = 1}^{\nneg} z_j - s^+_i.
  \end{equation*}
  Altogether, we get the formulation~\eqref{eq: TopPushK primal alternative}, where we use the fact, that we have linear model and therefore~$s^-_j = \bm{w}^{\top} \bm{x}^-_j$ for all~$j = 1, \; 2, \ldots, \; \nneg$ and ~$s^+_i = \bm{w}^{\top} \bm{x}^+_i$ for all~$i = 1, \; 2, \ldots, \; \npos$.
\end{proof}

\pagebreak

\begin{theorem}[Dual formulation of \TopPush and \TopPushK ]\label{thm: TopPushK dual}
  Consider \TopPushK formulation~\eqref{eq: toppush surrogate} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}}{
    - \frac{1}{2} \vecab^\top \Kneg \vecab - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    }{\label{eq: TopPushK dual}}{\label{eq: TopPushK dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\nneg} \beta_j \label{eq: TopPushK dual c1}}
    \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \nneg, \label{eq: TopPushK dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l.$ If~$K = 1,$ the upper bound in the second constrainet vanishes due to the first constraint and we get the dual form of \TopPush.
\end{theorem}
\begin{proof}
  In Lemma~\ref{lem: TopPushK primal alternative} we derived alternative fomrulation of \TopPushK with Lagrangian in the following form
  \begin{align*}
    \mathcal{L}(\bm{w}, t, \bm{y}, \bm{z}; \bm{\alpha}, \bm{\beta}, \bm{\gamma})
     & = \frac{1}{2} \norm{\bm{w}}_{2}^{2}
       + C \sum_{i = 1}^{\npos} l(y_i)
       + \sum_{i = 1}^{\npos} \alpha_i \Brac{t + \frac{1}{K} \sum_{j = 1}^{\nneg} z_j - \bm{w}^{\top} \bm{x}^+_i - y_i} \\
     & + \sum_{j = 1}^{\nneg} \beta_j \Brac{\bm{w}^{\top} \bm{x}^-_j - t - z_j}
       + \sum_{j = 1}^{\nneg} \gamma_j z_j,
  \end{align*}
  with feasibility conditions~$\beta_j \geq 0$ and~$\gamma_j \geq 0$ for all~$j = 1, \; 2, \ldots, \; \nneg.$ Then the corresponding dual objective function reads
  \begin{equation*}
    g(\bm{\alpha}, \bm{\beta}, \bm{\gamma})
      = \min_{\bm{w}, t, \bm{y}, \bm{z}} \; \mathcal{L}(\bm{w}, t, \bm{z}; \bm{\alpha}, \bm{\beta}, \bm{\gamma}),
  \end{equation*}
  Since the Lagrangian~$\mathcal{L}$ is separable in primal variables, it can be minimized with respect to each variable separately, i.e., the dual function can be rewritten as follows
  \begin{equation}\label{eq: TopPushK dual function}
    \begin{aligned}
      g(\bm{\alpha}, \bm{\beta}, \bm{\gamma})
        & = \min_{\bm{w}} \; \frac{1}{2} \norm{\bm{w}}_{2}^{2}
          - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j} \\
        & + \min_{t} \; t \Brac{\sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nneg} \beta_j} \\
        & + \min_{\bm{y}} \; C \sum_{i = 1}^{\npos} \Brac{l(y_i) - \frac{\alpha_i}{C}y_i} \\
        & + \min_{\bm{z}} \; \sum_{j = 1}^{\nneg} \Brac{\sum_{i = 1}^{\npos} \alpha_i - \beta_j - \gamma_j}z_j
    \end{aligned}
  \end{equation}
  From optimality conditions with respect to~$\bm{w}$ we deduce 
  \begin{equation*}
    \bm{w}
        = \sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j
        = \Matrix{\X^+ \\ - \X^-}^\top \vecab,
  \end{equation*}
  where we use Notation~\ref{not: kernel matrix}. Using this relation, we get the first part of the objective function~\eqref{eq: TopPushK dual L} 
  \begin{equation*}
    \frac{1}{2} \norm{\bm{w}}_{2}^{2} - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nneg} \beta_j \bm{x}^-_j}
      = - \frac{1}{2} \norm{\bm{w}}_{2}^{2}
      = - \frac{1}{2} \bm{w}^{\top} \bm{w}
      = - \frac{1}{2} \vecab^{\top} \Kneg \vecab,
  \end{equation*}
  where~$\Kneg$ is defined in Notation~\ref{not: kernel matrix}. Optimality condition with respect to~$t$ reads 
  \begin{equation*}
    \sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nneg} \beta_j = 0,
  \end{equation*}
  and implies constrain in~\eqref{eq: TopPushK dual c1}. Similarly, Optimality condition with respect to~$\bm{z}$ reads for all $j = 1, \; 2, \ldots, \; \nneg$ as 
  \begin{equation*}
    \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \beta_j - \gamma_j = 0.
  \end{equation*}
  Plugging the feasibility condition~$\gamma_j \geq 0$ into this equality and combining it with the feasibility conditions~$\beta_j \geq 0$ yields constraint~\eqref{eq: TopPushK dual c2}. Finally, minimization of the Lagrangian with respect to~$\bm{y}$ yields for all $i = 1, \; 2, \ldots, \; \npos$ 
  \begin{equation*}
    C \min_{y_i} \Brac{l(y_i) - \frac{\alpha_i}{C} y_i} = - C l^{\star} \Brac{\frac{\alpha_i}{C}}.
  \end{equation*}
  where the equality follows from Definition~\ref{def: conjugate}. Plugging this back into the Lagrange function yields the second part of the objective function~\eqref{eq: TopPushK dual L}. For \TopPush, we have~$K = 1.$ From~\eqref{eq: TopPushK dual c1} and non-negativity of~$\beta_j$ we deduce, that the upper bound in~\eqref{eq: TopPushK dual c2} is always fulfilled and therefore can be ommited, which finishes the proof. 
\end{proof}

\subsection{Accuracy at the Top}

In Section~\ref{sec: aatp} we derived three formulations that fall into our framework~\eqref{eq: aatp surrogate}. In this section, we focus only on two of them that are convex for linear classifer as showed in Chapter~\ref{chap: linear}. Namely, we focus on \TopMeanK and \PatMat.

\begin{theorem}[Dual formulation of \TopMeanK]\label{thm: TopMeanK dual}
  Consider \TopMeanK formulation~\eqref{eq: topmeank} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi*}{\bm{\alpha}, \bm{\beta}}{
    - \frac{1}{2} \vecab^\top \Kall \vecab - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    }{}{}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\nall} \beta_j}
    \addConstraint{0 \leq \beta_j}{\leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i, \quad j = 1, 2, \ldots, \nall,}
  \end{maxi*}
  where~$l^{\star}$ is conjugate function of~$l$ and~$K = \nall \tau.$
\end{theorem}
\begin{proof}
  \TopMeanK formulation is similar to the \TopPushK and therefore also dual formulations are similar. The main difference is, that the decision threshold for \TopMeanK is computed from all socres and not only from the negative ones as for \TopPushK. Due to that, the dual variable~$\bm{\beta}$ has different size and the kernel matrix has slightly different form as can be seen in Notation~\ref{not: kernel matrix}. Besides that dual formulations of \TopMeanK and \TopMeanK are identical and the proof of Theorem~\ref{thm: TopMeanK dual} is almost identical to the proof of Theorem~\ref{thm: TopPushK dual}.
\end{proof}

\begin{theorem}[Dual formulation of \PatMat]\label{thm: PatMat dual}
  Consider \PatMat formulation~\eqref{eq: patmat} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi!}{\bm{\alpha}, \bm{\beta}, \delta}{
    - \frac{1}{2} \vecab^\top \Kall \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    - \delta \sum_{j = 1}^{\nall} l^{\star} \Brac{\frac{\beta_j}{\delta\vartheta}}
    - \delta \nall \tau
    }{\label{eq: PatMat dual}}{\label{eq: PatMat dual L}}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\nall} \beta_j \label{eq: PatMat dual c1}}
    \addConstraint{\delta }{\geq 0, \label{eq: PatMat dual c2}}
  \end{maxi!}
  where~$l^{\star}$ is conjugate function of~$l$ and~$\vartheta > 0$ is a scaling parameter.
\end{theorem}
\begin{proof}
  Let us first realize tha \PatMat formulation~\eqref{eq: patmat} with linear model is equivalent to
  \begin{mini*}{\bm{w}, t, \bm{y}, \bm{z}}{
    \frac{1}{2} \norm{\bm{w}}_{2}^{2}+ C \sum_{i = 1}^{\npos} l(y_i)
    }{}{}
    \addConstraint{\sum_{j = 1}^{\nall} l(\vartheta z_i)}{\leq \nall \tau}{}
    \addConstraint{y_i}{= t - \bm{w}^{\top} \bm{x}^+_i,}{i = 1, \; 2, \ldots, \; \npos.}
    \addConstraint{z_j}{= \bm{w}^{\top} \bm{x}_j - t, \quad}{j = 1, \; 2, \ldots, \; \nall}
  \end{mini*}
  Corresponding Lagrangian is in the following form
  \begin{align*}
    \mathcal{L}(\bm{w}, t, \bm{y}, \bm{z}; \bm{\alpha}, \bm{\beta}, \delta)
    & = \frac{1}{2} \norm{\bm{w}}_{2}^{2}
      + C \sum_{i = 1}^{\npos} l(y_i)
      + \sum_{i = 1}^{\npos} \alpha_i (t - \bm{w}^{\top}\bm{x}^+_{i} - y_i) \\
    & + \sum_{j = 1}^{\nall} \beta_j(\bm{w}^{\top}\bm{x}_j - t - z_j)
      + \delta \Brac{\sum_{j = 1}^{\nall} l(\vartheta z_j) - \nall \tau}.
  \end{align*}
  with feasibility condition~$\delta \geq 0.$ Then the corresponding dual objective function reads
  \begin{equation*}
    g(\bm{\alpha}, \bm{\beta}, \delta)
      = \min_{\bm{w}, t, \bm{y}, \bm{z}} \; \mathcal{L}(\bm{w}, t, \bm{y}, \bm{z}; \bm{\alpha}, \bm{\beta}, \delta),
  \end{equation*}
  Since the Lagrangian~$\mathcal{L}$ is separable in primal variables, it can be minimized with respect to each variable separately, i.e., the dual function can be rewritten as follows
  \begin{align*}
    g(\bm{\alpha}, \bm{\beta}, \delta)
      & = \min_{\bm{w}} \; \frac{1}{2} \norm{\bm{w}}_{2}^{2}
        - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nall} \beta_j \bm{x}_j} \\
      & + \min_{t} \; t \Brac{\sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nall} \beta_j} \\
      & + \min_{\bm{y}} \; C \sum_{i = 1}^{\npos} \Brac{l(y_i) - \frac{\alpha_i}{C}y_i} \\
      & + \min_{\bm{z}} \; \delta \sum_{j = 1}^{\nall} \Brac{l(\vartheta z_j) - \frac{\beta_j}{\delta}z_j} \\
      & - \delta \nall \tau.
  \end{align*}
  Note that resulting dual function is very similar to the dual function~\eqref{eq: TopPushK dual function} for \TopPushK, i.e. minimization of the Lagrangian with respect to~$\bm{w}$,~$t$ and~$\bm{y}$ yields similar results. From optimality conditions with respect to~$\bm{w}$ we deduce 
  \begin{equation*}
    \bm{w}
        = \sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nall} \beta_j \bm{x}_j
        = \Matrix{\X^+ \\ - \X}^\top \vecab,
  \end{equation*}
  where we use Notation~\ref{not: kernel matrix}. Using this relation, we get the first part of the objective function~\eqref{eq: PatMat dual L} 
  \begin{equation*}
    \frac{1}{2} \norm{\bm{w}}_{2}^{2} - \bm{w}^{\top} \Brac{\sum_{i = 1}^{\npos} \alpha_i \bm{x}^+_i - \sum_{j = 1}^{\nall} \beta_j \bm{x}_j}
      = - \frac{1}{2} \norm{\bm{w}}_{2}^{2}
      = - \frac{1}{2} \bm{w}^{\top} \bm{w}
      = - \frac{1}{2} \vecab^{\top} \Kall \vecab,
  \end{equation*}
  where~$\Kall$ is defined in Notation~\ref{not: kernel matrix}. Optimality condition with respect to~$t$ reads 
  \begin{equation*}
    \sum_{i = 1}^{\npos} \alpha_i - \sum_{j = 1}^{\nall} \beta_j = 0,
  \end{equation*}
  and implies constrain in~\eqref{eq: PatMat dual c1}. The optimality condition with respect to~$\bm{y}$ is identical to the one in the proof of Theorem~\ref{thm: TopPushK dual}. Finally, inimization of the Lagrangian with respect to~$\bm{z}$ yields for all $j = 1, \; 2, \ldots, \; \nall$ 
  \begin{equation*}
    \delta \min_{\bm{z}} \; \Brac{l(\vartheta z_j) - \frac{\beta_j}{\delta\vartheta } \vartheta z_j} = - \delta l^{\star} \Brac{\frac{\beta_i}{\delta\vartheta }},
  \end{equation*}
  where the equality follows from Definition~\ref{def: conjugate}. Plugging this back into the Lagrange function yields the second part of the objective function~\eqref{eq: PatMat dual L}, which finishes the proof.
\end{proof}

\subsection{Hypothesis Testing}

In Section~\ref{sec: Neyman-Pearson} we derived three problem formulations that fall into our framework~\eqref{eq: aatp surrogate}. Namely: \GrillNP, \tauFPL and \PatMatNP. Similarly to the previous section, we focus only on \tauFPL and \PatMatNP. Since \tauFPL is a special case of \TopPushK for~$K = \nneg \tau,$ the dual formulation is identical to the one in~\ref{thm: TopPushK dual}.

\begin{theorem}[Dual formulation of \PatMatNP]\label{thm: PatMatNP dual}
  Consider \PatMatNP formulation~\eqref{eq: patmat np} with linear model, surrogate function~$l$ and Notation~\ref{not: kernel matrix}. Then the corresponding dual problem has the following form
  \begin{maxi*}{\bm{\alpha}, \bm{\beta}, \delta}{
    - \frac{1}{2} \vecab^\top \Kneg \vecab
    - C \sum_{i = 1}^{\npos} l^{\star}\Brac{\frac{\alpha_i}{C}}
    - \delta \sum_{j = 1}^{\nneg} l^{\star} \Brac{\frac{\beta_j}{\delta\vartheta}}
    - \delta \nneg \tau
    }{}{}
    \addConstraint{\sum_{i = 1}^{\npos} \alpha_i}{= \sum_{j = 1}^{\nneg} \beta_j}
    \addConstraint{\delta }{\geq 0,}
  \end{maxi*}
  where~$l^{\star}$ is conjugate function of~$l$ and~$\vartheta > 0$ is a scaling parameter.
\end{theorem}
\begin{proof}
  \PatMatNP formulation is similar to the \PatMat and therefore also dual formulations are similar. The main difference is, that the decision threshold for \PatMatNP is computed from all socres and not only from the negative ones as for \PatMat. Due to that, the dual variable~$\bm{\beta}$ has different size and the kernel matrix has slightly different form as can be seen in Notation~\ref{not: kernel matrix}. Besides that dual formulations of \PatMatNP and \PatMat are identical and the proof of Theorem~\ref{thm: PatMatNP dual} is almost identical to the proof of Theorem~\ref{thm: PatMat dual}.
\end{proof}

\section{New Coordinate descent Algorithm}

\subsection{Family of \TopPushK Formulations}

\subsubsection{Hinge Loss}

\topruleaa*
\begin{proof}[Proof Lemma~\ref{thm: toppushk family hinge update a,a} on page~\pageref{thm: toppushk family hinge update a,a}]
  Constraint~\eqref{eq: Top dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Top dual hinge c3} is always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Top dual hinge c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta \leq C
    & \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak \\
    0 \leq \alphal - \Delta \leq C
    & \quad \implies \quad
    \alphal - C \leq \Delta \leq \alphal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Using the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Top dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\topruleab*
\begin{proof}[Proof Lemma~\ref{thm: toppushk family hinge update a,b} on page~\pageref{thm: toppushk family hinge update a,b}]
  Constraint~\eqref{eq: Top dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraint~\eqref{eq: Top dual hinge c2} reads
  \begin{equation*}
    0 \leq \alphak + \Delta \leq C
    \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak.
  \end{equation*}
  Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Top dual hinge c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \beta_{\max} \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K} 
    & \quad \implies \quad
    K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i \leq \Delta \\
    0 \leq \betal + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K}
    & \quad \implies \quad
    -\betal \leq \Delta \quad \land \quad \Delta \leq \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}
  \end{align*}
  Combination of these bounds yealds the lower bound~$\Delta_{lb}$ and upper bound~$\Delta_{ub}.$ If~$K = 1,$ the upper bounds in~\eqref{eq: Top dual hinge c3} is always satisfied due to~\eqref{eq: Top dual hinge c1} and the lower and upper bound of~$\Delta$ can be simplified. Using the update rule~\eqref{eq: update rule a,b}, objective function~\eqref{eq: Top dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}} \Delta^2
    - \Brac[s]{s_k + s_l - 1} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\toprulebb*
\begin{proof}[Proof Lemma~\ref{thm: toppushk family hinge update b,b} on page~\pageref{thm: toppushk family hinge update b,b}]
  Constraint~\eqref{eq: Top dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Top dual hinge c2} is also always satisfied since no~$\alpha_i$ is updated. Constraint~\eqref{eq: Top dual hinge c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \betak + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i 
    & \quad \implies \quad
    -\betak \leq \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak \\
    0 \leq \betal - \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i
    & \quad \implies \quad
    \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i \leq \Delta \leq \betal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ If~$K = 1,$ the upper bounds in~\eqref{eq: Top dual hinge c3} is always satisfied due to~\eqref{eq: Top dual hinge c1} and the lower and upper bound of~$\Delta$ can be simplified. Using the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Top dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\subsubsection{Quadratic Hinge Loss}

\begin{lemma}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Top dual quadratic}]\label{thm: toppushk family quadratic update a,a}
  Consider problem~\eqref{eq: Top dual quadratic}, update rule~\eqref{eq: update rule a,a} and~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos.$ Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = -\alphak, &
    \Delta_{ub} & = \alphal, &
    \gamma & = -\frac{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}}.
  \end{align*}
\end{lemma}

\begin{proof}
  Constraint~\eqref{eq: Top dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Top dual quadratic c3} is also always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Top dual quadratic c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta
    & \quad \implies \quad
    - \alphak \leq \Delta \\
    0 \leq \alphal - \Delta
    & \quad \implies \quad
    \Delta \leq \alphal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Using the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Top dual quadratic L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}} \Delta^2
    - \Brac[s]{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\begin{lemma}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Top dual quadratic}]\label{thm: toppushk family quadratic update a,b}
  Consider problem~\eqref{eq: Top dual quadratic}, update rule~\eqref{eq: update rule a,b} and~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil.$ Let us define~$\hat{l} = l - \npos$ and
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        \max \Brac[c]{- \alphak,\;  -\betal} & K = 1, \\
        \max \Brac[c]{- \alphak,\;  -\betal, \; K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        + \infty & K = 1, \\
        \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal} & \textrm{otherwise},
      \end{cases*} \\
    \gamma & = -\frac{s_k + s_l - 1 + \frac{1}{2C} \alphak}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C}}.
  \end{align*}
\end{lemma}

\begin{proof}
  Constraint~\eqref{eq: Top dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraint~\eqref{eq: Top dual quadratic c2} reads
  \begin{equation*}
    0 \leq \alphak + \Delta
    \quad \implies \quad
    - \alphak \leq \Delta.
  \end{equation*}
  Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Top dual quadratic c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \beta_{\max} \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K} 
    & \quad \implies \quad
    K\beta_{\max} - \sum_{i = 1}^{\npos} \alpha_i \leq \Delta \\
    0 \leq \betal + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i + \frac{\Delta}{K}
    & \quad \implies \quad
    -\betal \leq \Delta \quad \land \quad \Delta \leq \frac{1}{K-1}\Brac{\sum_{i = 1}^{\npos} \alpha_i - K \betal}
  \end{align*}
  Combination of these bounds yealds the lower bound~$\Delta_{lb}$ and upper bound~$\Delta_{ub}.$ If~$K = 1,$ the upper bounds in~\eqref{eq: Top dual quadratic c3} is always satisfied due to~\eqref{eq: Top dual quadratic c1} and the lower and upper bound of~$\Delta$ can be simplified. Using the update rule~\eqref{eq: update rule a,b}, objective function~\eqref{eq: Top dual quadratic L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C}} \Delta^2
    - \Brac[s]{s_k + s_l - 1 + \frac{1}{2C} \alphak} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\begin{lemma}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Top dual quadratic}]\label{thm: toppushk family quadratic update b,b}
  Consider problem~\eqref{eq: Top dual quadratic}, update rule~\eqref{eq: update rule b,b} and~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil.$ Let us define~$\hat{k} = k - \npos$ and~$\hat{l} = l - \npos.$ Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = 
      \begin{cases*}
        -\betak & K = 1, \\
        \max \Brac[c]{- \betak,\; \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i} & \textrm{otherwise},
      \end{cases*} \\
    \Delta_{ub} & = 
      \begin{cases*}
        \betal & K = 1, \\
        \min \Brac[c]{\betal,\; \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak} & \textrm{otherwise},
      \end{cases*} \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}.
  \end{align*}
\end{lemma}

\begin{proof}
  Constraint~\eqref{eq: Top dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Top dual quadratic c2} is also always satisfied since no~$\alpha_i$ is updated. Constraint~\eqref{eq: Top dual quadratic c3} for any~$K \geq 2$ reads
  \begin{align*}
    0 \leq \betak + \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i 
    & \quad \implies \quad
    -\betak \leq \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i - \betak \\
    0 \leq \betal - \Delta \leq \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i
    & \quad \implies \quad
    \betal - \frac{1}{K} \sum_{i = 1}^{\npos} \alpha_i \leq \Delta \leq \betal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ If~$K = 1,$ the upper bounds in~\eqref{eq: Top dual quadratic c3} is always satisfied due to~\eqref{eq: Top dual quadratic c1} and the lower and upper bound of~$\Delta$ can be simplified. Using the update rule~\eqref{eq: update rule b,b}, objective function~\eqref{eq: Top dual quadratic L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  Finally, the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.
\end{proof}

\subsubsection{Initialization}

\subsection{Family of \PatMat Formulations}

\subsubsection{Hinge Loss}

\begin{lemma}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Pat dual hinge}]
  Consider problem~\eqref{eq: Pat dual hinge}, update rule~\eqref{eq: update rule a,a} and~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos.$ Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \min\{- \alphak,\; \alphal - C\}, &
    \Delta_{ub} & = \max\{C - \alphak,\; \alphal\}, \\
    \gamma & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
    \delta^{\star} & = \delta.
  \end{align*}
\end{lemma}

\begin{proof}
  Constraint~\eqref{eq: Pat dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Pat dual hinge c3} is also always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Pat dual hinge c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta \leq C
    & \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak \\
    0 \leq \alphal - \Delta \leq C
    & \quad \implies \quad
    \alphal - C \leq \Delta \leq \alphal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Using the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
    - \Brac[s]{s_k - s_l} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}. Finally, since optimal~$\delta$ is given by~\eqref{eq: Pat dual hinge optimal delta} and no~$\beta_j$ was updated, the optimal~$\delta$ does not change.
\end{proof}

\begin{lemma}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Pat dual hinge}]
  Consider problem~\eqref{eq: Pat dual hinge}, update rule~\eqref{eq: update rule a,b} and~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil.$ Let us define~$\hat{l} = l - \npos$ and
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{l}\}} \beta_j.
  \end{equation*}
  Then the bounds from~\eqref{eq: Delta optimal} are defined as follows
  \begin{align*}
    \Delta_{lb} & = \max\{- \alphak,\; -\betal \}, &
    \Delta_{ub} & = C - \alphak,
  \end{align*}
  and there are two possible solutions
  \begin{enumerate}
    \item The first solution~$\Delta^{\star}_1$ is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k + s_l - 1 - \frac{1}{\vartheta}}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}, &
      \delta^{*}_1
        & = \frac{\beta_{\max}}{\vartheta}.
    \end{align*}
    and is feasible if~$\betal + \Delta^{\star}_1 \leq \beta_{\max}.$
    \item The second solution~$\Delta^{\star}_2$ is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k + s_l - 1 - \frac{1 - \ntil \tau}{\vartheta}}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk}}, &
      \delta^{*}_2
        & = \frac{\betal + \Delta^{\star}_2}{\vartheta}.
    \end{align*}
    and is feasible if~$\betal + \Delta^{\star}_2 \geq \beta_{\max}.$
  \end{enumerate}
  The optimal solution~$\Delta^{\star}$ is equal to the one of them which maximizes the original objective and is feasible.
\end{lemma}

\begin{proof}
  Constraint~\eqref{eq: Pat dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraint~\eqref{eq: Pat dual hinge c2} reads
  \begin{equation}\label{eq: Pat dual hinge c2 update a,a}
    0 \leq \alphak + \Delta \leq C
    \quad \implies \quad
    - \alphak \leq \Delta \leq C - \alphak.
  \end{equation}
  Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Pat dual hinge c3} reads
  \begin{align*}
    \beta_{\max} & \leq \delta \vartheta \\
    0 \leq \betal + \Delta & \leq \delta \vartheta
  \end{align*}
  Since the optimal~$\delta$ is given by~\eqref{eq: Pat dual hinge optimal delta}, there are only two possible choices
  \begin{align}\label{eq: Pat dual hinge a, b proof delta}
    \delta & = \frac{\beta_{\max}}{\vartheta}, &
    \delta & = \frac{\betal + \Delta}{\vartheta}.
  \end{align}
  If we use any of these choices which is feasible, all upper bounds in constraint~\eqref{eq: Pat dual hinge c3} hold, i.e. we can simplify the constraints to
  \begin{equation*}
    0 \leq \betal + \Delta
    \quad \implies \quad
    - \betal \leq \Delta,
  \end{equation*}
  which in combination with~\eqref{eq: Pat dual hinge c2 update a,a} gives the lower and upper bound of~$\Delta.$ Now let us discuss how to select optimal~$\delta:$
  \begin{enumerate}
    \item Using~$\delta^{\star}_1$ from~\eqref{eq: Pat dual hinge a, b proof delta} and the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l - 1 - \frac{1}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_1$ is given by~\eqref{eq: Delta optimal} and is feasible if~$\betal + \Delta^{\star}_1 \leq \beta_{\max}$.

    \item Using~$\delta^{\star}_2$ from~\eqref{eq: Pat dual hinge a, b proof delta} and the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l - 1 - \frac{1 - \ntil \tau}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_2$ is given by~\eqref{eq: Delta optimal} and is feasible if~$\betal + \Delta^{\star}_2 \geq \beta_{\max}$.
  \end{enumerate}
  The final optimal solution is the one that is feasible and that maximizes the original objective function~\eqref{eq: Pat dual hinge L}.
\end{proof}

\begin{lemma}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Pat dual hinge}]
  Consider problem~\eqref{eq: Pat dual hinge}, update rule~\eqref{eq: update rule b,b} and~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil.$ Let us define~$\hat{k} = k - \npos,$~$\hat{l} = l - \npos$ and
  \begin{equation*}
    \beta_{\max} = \max_{j \in \{1, 2, \ldots, \ntil \} \setminus \{\hat{k}, \hat{l}\}} \beta_j.
  \end{equation*}
  Then the bounds from~\eqref{eq: Delta optimal} are defined as follows
  \begin{align*}
    \Delta_{lb} & = - \betak, &
    \Delta_{ub} & = \betal,
  \end{align*}
  and there are three possible solutions
  \begin{enumerate}
    \item The first solution~$\Delta^{\star}_1$ is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k - s_l}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
      \delta^{*}_1
        & = \frac{\beta_{\max}}{\vartheta}.
    \end{align*}
    and is feasible if~$\beta_{\max} \geq \max\{\betak + \Delta^{\star}_1, \betal - \Delta^{\star}_1\}.$
    \item The second solution~$\Delta^{\star}_2$ is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k - s_l + \frac{\ntil \tau}{\vartheta}}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
      \delta^{*}_2
        & = \frac{\betak + \Delta^{\star}_2}{\vartheta}.
    \end{align*}
    and is feasible if~$\betak + \Delta^{\star}_2 \geq \max\{\beta_{\max} , \betal - \Delta^{\star}_2\},$
    \item The third solution~$\Delta^{\star}_3$ is given by~\eqref{eq: Delta optimal} where
    \begin{align*}
      \gamma
        & = -\frac{s_k - s_l - \frac{\ntil \tau}{\vartheta}}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}}, &
      \delta^{*}_3
        & = \frac{\betak - \Delta^{\star}_3}{\vartheta}.
    \end{align*}
    and is feasible if~$\betal - \Delta^{\star}_3 \geq \max\{\betak + \Delta^{\star}_3, \beta_{\max}\}$
  \end{enumerate}
  The optimal solution~$\Delta^{\star}$ is equal to the one of them which maximizes the original objective and is feasible.
\end{lemma}

\begin{proof}
  Constraint~\eqref{eq: Pat dual hinge c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Pat dual hinge c2} is also always satisfied since no~$\alpha_i$ is updated. Using the definition of~$\beta_{\max},$ constraint~\eqref{eq: Pat dual hinge c3} reads
  \begin{align*}
    \beta_{\max} & \leq \delta \vartheta, \\
    0 \leq \betak + \Delta & \leq \delta \vartheta, \\
    0 \leq \betal - \Delta & \leq \delta \vartheta.
  \end{align*}
  Since the optimal~$\delta$ is given by~\eqref{eq: Pat dual hinge optimal delta}, there are only two possible choices
  \begin{align}\label{eq: Pat dual hinge b, b proof delta}
    \delta^{\star}_1 & = \frac{\beta_{\max}}{\vartheta}, &
    \delta^{\star}_2 & = \frac{\betak + \Delta}{\vartheta}, &
    \delta^{\star}_3 & = \frac{\betal - \Delta}{\vartheta}.
  \end{align}
  If we use any of these choices which is feasible, all upper bounds in constraint~\eqref{eq: Pat dual hinge c3} hold, i.e. we can simplify the constraints to
  \begin{align*}
    0 \leq \betak + \Delta
    & \quad \implies \quad
    - \betak \leq \Delta, \\
    0 \leq \betal - \Delta
    & \quad \implies \quad
    \Delta \leq \betal,
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Now let us discuss how to select optimal~$\delta:$
  \begin{enumerate}
    \item Using~$\delta^{\star}_1$ from~\eqref{eq: Pat dual hinge b, b proof delta} and the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_1$ is given by~\eqref{eq: Delta optimal} and is feasible if
    \begin{equation*}
      \beta_{\max} \geq \max\{\betak + \Delta^{\star}_1, \; \betal - \Delta^{\star}_1\}.
    \end{equation*}

    \item Using~$\delta^{\star}_2$ from~\eqref{eq: Pat dual hinge b, b proof delta} and the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l + \frac{\ntil \tau}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_2$ is given by~\eqref{eq: Delta optimal} and is feasible if
    \begin{equation*}
      \betak + \Delta^{\star}_2 \geq \max\{\beta_{\max} , \betal - \Delta^{\star}_2\}.
    \end{equation*}

    \item Using~$\delta^{\star}_3$ from~\eqref{eq: Pat dual hinge b, b proof delta} and the update rule~\eqref{eq: update rule a,a}, objective function~\eqref{eq: Pat dual hinge L} can be rewritten as a quadratic function with respect to~$\Delta$ as
    \begin{equation*}
      - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk}} \Delta^2
      - \Brac[s]{s_k - s_l - \frac{\ntil \tau}{\vartheta}} \Delta
      - c(\bm{\alpha}, \bm{\beta}).
    \end{equation*}
    The optimal solution~$\Delta^{\star}_3$ is given by~\eqref{eq: Delta optimal} and is feasible if
    \begin{equation*}
      \betal - \Delta^{\star}_3 \geq \max\{\beta_{\max}, \betak + \Delta^{\star}_3\}.
    \end{equation*}
  \end{enumerate}
  The final optimal solution is the one that is feasible and that maximizes the original objective function~\eqref{eq: Pat dual hinge L}.
\end{proof}

\subsubsection{Quadratic Hinge Loss}

\begin{lemma}[Update rule~\eqref{eq: update rule a,a} for problem~\eqref{eq: Pat dual quadratic}]\label{thm: patmat family quadratic update a,a}
  Consider problem~\eqref{eq: Pat dual quadratic}, update rule~\eqref{eq: update rule a,a} and~$1 \leq k \leq \npos$ and~$1 \leq l \leq \npos.$ Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = -\alphak, \\
    \Delta_{ub} & = \alphal, \\
    \gamma & = -\frac{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}}, \\
    \delta^{\star}  & = \delta.
  \end{align*}
\end{lemma}

\begin{proof}[Proof Lemma~\ref{thm: patmat family quadratic update a,a} on page~\pageref{thm: patmat family quadratic update a,a}]
  Constraint~\eqref{eq: Pat dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,a}. Constraint~\eqref{eq: Pat dual quadratic c3} is also always satisfied since no~$\beta_j$ was updated and the sum of all~$\alpha_i$ did not change. Constraint~\eqref{eq: Pat dual quadratic c2} reads
  \begin{align*}
    0 \leq \alphak + \Delta
    & \quad \implies \quad
    - \alphak \leq \Delta \\
    0 \leq \alphal - \Delta
    & \quad \implies \quad
    \Delta \leq \alphal
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Using the update rule~\eqref{eq: update rule a,a}, objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{C}} \Delta^2
    - \Brac[s]{s_k - s_l + \frac{1}{2C}(\alphak - \alphal)} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}. Finally, since optimal~$\delta$ is given by~\eqref{eq: Pat dual quadratic optimal delta} and no~$\beta_j$ was updated, the optimal~$\delta$ does not change.
\end{proof}

\begin{lemma}[Update rule~\eqref{eq: update rule a,b} for problem~\eqref{eq: Pat dual quadratic}]\label{thm: patmat family quadratic update a,b}
  Consider problem~\eqref{eq: Pat dual quadratic}, update rule~\eqref{eq: update rule a,b} and~$1 \leq k \leq \npos$ and~$\npos + 1 \leq l \leq \ntil.$ Let us define~$\hat{l} = l - \npos.$ Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = \max\{- \alphak, - \betal\}, \\
    \Delta_{ub} & = +\infty, \\
    \gamma      & = -\frac{s_k + s_l  - 1 + \frac{\alphak}{2C} - \frac{1}{\vartheta} + \frac{\betal}{2 \delta \vartheta^2}}{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C} + \frac{1}{2 \delta \vartheta^2}}, \\
    \delta^{\star}  & = \sqrt{\delta^2 + \frac{1}{4 \vartheta \ntil \tau}({\Delta^{\star}}^2 + 2 \Delta^{\star} \betal)}.
  \end{align*}
\end{lemma}

\begin{proof}[Proof Lemma~\ref{thm: patmat family quadratic update a,b} on page~\pageref{thm: patmat family quadratic update a,b}]
  Constraint~\eqref{eq: Pat dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule a,b}. Constraints~\eqref{eq: Pat dual quadratic c2} and~\eqref{eq: Pat dual quadratic c3} reads
  \begin{align*}
    0 \leq \alphak + \Delta
    & \quad \implies \quad
    - \alphak \leq \Delta, \\
    0 \leq \betal + \Delta
    & \quad \implies \quad
    - \betal \leq \Delta, \\
  \end{align*}
  which gives the lower bound of~$\Delta.$ In this case, $\Delta$ has no upper bound. Using the update rule~\eqref{eq: update rule a,b}, objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{align*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} + \K_{kl} + \K_{lk} + \frac{1}{2C} + \frac{1}{2 \delta \vartheta^2}} & \Delta^2 \\
    - \Brac[s]{s_k + s_l - 1 + \frac{\alphak}{2C} - \frac{1}{\vartheta} + \frac{\betal}{2\delta\vartheta^2}} & \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{align*}
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}. We know that the optimal~$\delta^*$ is given by~\eqref{eq: Pat dual quadratic optimal delta}, then
  \begin{equation*}
    \delta^*
      = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \Brac{\sum_{j\neq \hat{l}} \beta_j^2 + (\betal + \Delta^\star)^2}}
      = \sqrt{\delta^2 + \frac{1}{4\vartheta^2 \ntil \tau} (\Delta^{\star2} + 2\Delta^\star \betal)}.
  \end{equation*}
\end{proof}

\begin{lemma}[Update rule~\eqref{eq: update rule b,b} for problem~\eqref{eq: Pat dual quadratic}]\label{thm: patmat family quadratic update b,b}
  Consider problem~\eqref{eq: Pat dual quadratic}, update rule~\eqref{eq: update rule b,b} and~$\npos + 1 \leq k \leq \ntil$ and~$\npos + 1 \leq l \leq \ntil.$ Let us define~$\hat{k} = k - \npos$ and~$\hat{l} = l - \npos.$ Then the optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal} where
  \begin{align*}
    \Delta_{lb} & = - \betak, \\
    \Delta_{ub} & = \betal, \\
    \gamma      & = -\frac{s_k - s_l + \frac{1}{2\delta \vartheta^2}(\betak - \betal)}{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{\delta \vartheta^2}}, \\
    \delta^{\star}  & = \sqrt{\delta^2 + \frac{1}{2 \vartheta \ntil \tau}({\Delta^{\star}}^2 + \Delta^{\star} (\betak - \betal))}.
  \end{align*}
\end{lemma}

\begin{proof}[Proof Lemma~\ref{thm: patmat family quadratic update b,b} on page~\pageref{thm: patmat family quadratic update b,b}]
  Constraint~\eqref{eq: Pat dual quadratic c1} is always satisfied from the definition of the update rule~\eqref{eq: update rule b,b}. Constraint~\eqref{eq: Pat dual quadratic c2} is also always satisfied since no~$\alpha_i$ is updated. Constraint~\eqref{eq: Pat dual quadratic c3} reads
  \begin{align*}
    0 \leq \betak + \Delta
    & \quad \implies \quad
    - \betak \leq \Delta, \\
    0 \leq \betal + \Delta
    & \quad \implies \quad
    - \betal \leq \Delta, \\
  \end{align*}
  which gives the lower and upper bound of~$\Delta.$ Using the update rule~\eqref{eq: update rule b,b}, objective function~(\ref{eq: Pat dual quadratic L1}-\ref{eq: Pat dual quadratic L2}) can be rewritten as a quadratic function with respect to~$\Delta$ as
  \begin{equation*}
    - \frac{1}{2} \Brac[s]{\K_{kk} + \K_{ll} - \K_{kl} - \K_{lk} + \frac{1}{2\delta\vartheta^2}} \Delta^2
    - \Brac[s]{s_k - s_l + \frac{1}{\delta\vartheta^2}(\betak - \betal)} \Delta
    - c(\bm{\alpha}, \bm{\beta}).
  \end{equation*}
  The optimal solution~$\Delta^{\star}$ is given by~\eqref{eq: Delta optimal}.   We know that the optimal~$\delta^*$ is given by~\eqref{eq: Pat dual quadratic optimal delta}, then
  \begin{equation*}
    \delta^*
      = \sqrt{\frac{1}{4\vartheta^2 \ntil \tau} \Brac{\sum_{j \notin \{\hat{l}, \hat{k}\}} \beta_j^2 + (\betak + \Delta^\star)^2 + (\betal - \Delta^\star)^2}} 
      = \sqrt{\delta + \frac{1}{2\vartheta^2 \ntil \tau} (\Delta^{\star2} + \Delta^\star (\betak - \betal))}.
  \end{equation*}
\end{proof}

\subsubsection{Initialization}