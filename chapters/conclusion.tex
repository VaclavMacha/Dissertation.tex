\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In this work, we studied the problem of classification at the top, which is closely related to binary classification. We showed that many well-known categories of problems, such as ranking, accuracy at the top, or hypothesis testing, are closely related to classification at the top. In Chapter~\ref{chap: framework}, we studied these three categories in detail and showed that they could be formulated similarly. This leads us to introduce a unified framework for classification at the top~\eqref{eq: aatp surrogate}. We showed that several known formulations (\TopPush, \Grill, \tauFPL) fell into our framework and derived some completely new formulations (\PatMat, \PatMatNP). The summary of all presented formulations is in Table~\ref{tab: summary formulations}.

In Chapter~\ref{chap: linear}, we performed a theoretical analysis of the presented formulations when the linear model is used. We showed that known formulations suffer from certain disadvantages. While \TopPush and \tauFPL are sensitive to outliers, \Grill is non-convex. On the other hand, we showed that newly introduced \PatMat and \PatMatNP formulations are robust and convex. We also proved the global convergence of the stochastic gradient descent for \PatMat and \PatMatNP.

In Chapter~\ref{chap: dual}, we extended the framework~\eqref{eq: aatp surrogate} to nonlinear problems. We showed that all presented formulations (except for \Grill and \GrillNP) could be divided into two families based on the form of the constraints, namely \TopPushK and \PatMat family of formulations. We derived dual formulations for \TopPushK and \PatMat family of formulations. Moreover, we proposed a new coordinate descent algorithm for solving the resulting dual problems. We also derived the closed-form formulas for selected surrogate functions needed in the coordinate descent algorithm. Since the algorithm needs a feasible solution for initialization, we also showed how to find such a solution.

In Chapter~\ref{chap: deep}, we study the primal formulations with non-linear models. We showed that when we use a non-linear model, the resulting formulations are non-decomposable. This property is caused by the special threshold constraint in~\eqref{eq: aatp surrogate} and prevents us from using stochastic gradient descent in a standard way. We introduce modified stochastic gradient descent for our formulations. Unfortunately, we showed that stochastic gradient descent leads to a biased estimate of the true gradient. We suggested that this can mitigate by using a large minibatch. However, such an approach is often not possible. For such cases, we proposed \DeepTopPush as an efficient alternative to \TopPush formulation that does not suffer from this issue. For \DeepTopPush, we implicitly removed some optimization variables, created an unconstrained end-to-end network, and used the stochastic gradient descent to train it. We modified the minibatch so that the sampled threshold (computed on a minibatch) is a good estimate of the true threshold (computed on all samples). We showed both theoretically and numerically that this procedure reduces the bias of the sampled gradient.

In Chapter~\ref{chap: experiments}, we performed a numerical comparison of all presented formulations. We showed a good performance of our newly introduced formulation \PatMatNP, when used in its primal form. We also showed that \DeepTopPush formulation could be very useful, especially for very large real-world datasets. We demonstrated that standard formulations provide poor results at very low false-positive rates on steganalysis datasets and malware detection datasets, while the formulations proposed in this work provide well results.
