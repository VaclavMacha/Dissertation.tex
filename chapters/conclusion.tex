\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

binary classification is focused on classification performance on all samples. In Chapter~\ref{chap: binary classification}, we discussed many problems in which the performance on all samples is not desired and only performance on a small amout of samples is important.  In this work, we studied the problem of classification at the top, which focus on such problems. In the following list, we provide the main contributions of this work:
\begin{itemize}
  \item \textbf{Introduction of unified framework for classification at the top:} In Chapter~\ref{chap: binary classification} we showed that many well-known categories of problems, such as ranking, accuracy at the top, or hypothesis testing, are closely related to classification at the top. This leads us to introduce a unified framework for classification at the top in Chapter~\ref{chap: framework}. We showed that several known formulations (\TopPush, \Grill, \tauFPL) fell into our framework and derived some completely new formulations (\PatMat, \PatMatNP). The summary of all presented formulations is in Table~\ref{tab: summary formulations}.
  \item \textbf{Introduction of \PatMat and \PatMatNP formulations:} In Chapter~\ref{chap: framework}, we introduced \PatMat formulation as an alternative to \TopMeanK formulation. These two formulations differ only in the used approximation of the decision threshold. \PatMat formulation uses surrogate approximation of the true quantile while \TopMeanK approximates the true quantile using mean. We showed, that \PatMat provides worse approximation, but better theoretical properties. Similarly, we introduced \PatMatNP formulation as an alternative to \tauFPL formulation.
  \item \textbf{Derivation of theoretical properties for linear classifier:} In Chapter~\ref{chap: linear}, we performed a theoretical analysis of the presented formulations when the linear model is used. We showed that known formulations suffer from certain disadvantages. While \TopPush and \tauFPL are sensitive to outliers, \Grill is non-convex. On the other hand, we showed that newly introduced \PatMat and \PatMatNP formulations are robust and convex. We also proved the global convergence of the stochastic gradient descent for \PatMat and \PatMatNP.
  \item \textbf{Derivation of dual forms and use of non-linear kernels:} In Chapter~\ref{chap: dual}, we showed that all presented formulations (except for \Grill and \GrillNP) could be divided into two families based on the form of the constraints, namely \TopPushK and \PatMat family of formulations. We derived dual forms for \TopPushK and \PatMat family of formulations. Moreover, for both these formulations we show how to incorporate non-linear kernels.
  \item \textbf{Derivation of efficient algorithm for solving dual forms:} In Chapter~\ref{chap: dual}, we proposed a new coordinate descent algorithm for solving dual forms of \TopPushK and \PatMat family of formulations. The resulting algorithm depends on the used surrogate function. Therefore, we derived the closed-form formulas for selected surrogate functions. Since the algorithm needs a feasible solution for initialization, we also showed how to find such a solution.
  \item \textbf{Introduction of modified stochastic gradient descent:} In Chapter~\ref{chap: deep}, we study the primal formulations with non-linear models. More precisely, we used neural-networks. We showed that when we use a non-linear model, the resulting formulations are non-decomposable. This property is caused by the special threshold constraint in~\eqref{eq: aatp surrogate} and prevents us from using stochastic gradient descent in a standard way. We introduce modified stochastic gradient descent for our formulations. Unfortunatelly, we showed that modified stochastic gradient descent leads to a biased estimate of the true gradient. 
  \item \textbf{Introduction of \DeepTopPush formulation:} As mentioned above, the proposed modified stochastic gradient descent leads to a biased estimate of the true gradient. In Chapter~\ref{chap: deep}, we suggested that this can mitigate by using a large minibatch. However, such an approach is often not possible. For such cases, we proposed \DeepTopPush as an efficient alternative to \TopPush formulation that does not suffer from this issue. For \DeepTopPush, we implicitly removed some optimization variables, created an unconstrained end-to-end network, and used the stochastic gradient descent to train it. We modified the minibatch so that the sampled threshold (computed on a minibatch) is a good estimate of the true threshold (computed on all samples). We showed both theoretically and numerically that this procedure reduces the bias of the sampled gradient.
  \item \textbf{Numerical comparison:} In Chapter~\ref{chap: experiments}, we performed a numerical comparison of all presented formulations. We showed a good performance of our newly introduced formulation \PatMatNP, when used in its primal form. We also showed that \DeepTopPush formulation could be very useful, especially for very large real-world datasets. We demonstrated that standard formulations provide poor results at very low false-positive rates on steganalysis datasets and malware detection datasets, while the formulations proposed in this work provide good results.
\end{itemize}

\section*{Future Work}
