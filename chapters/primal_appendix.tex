\chapter{Appendix for Chapter~\ref{chap: linear}}

Firstly we recall definitions of the thresholds defined in Section~\ref{sec: aatp}
\begin{equation*}
  \begin{aligned}
    t_1(\bm{w}) & = \max \Set{t}{\frac{1}{n} \sum_{i \in \I} \Iverson{s_i \ge t} \ge \tau} \\
    t_2(\bm{w}) & = \frac{1}{K} \sum_{i=1}^{K} s_{[i]} \\
    t_3(\bm{w}) & \quad \text{solves} \quad \frac{1}{n} \sum_{i \in \I} l\Brac{\vartheta(s_i - t)} = \tau, \\
  \end{aligned}
\end{equation*}
and also recall, that we assume linear classifier, i.e. scores are defined for all~$i \in \I$ as~$s_i = \bm{w}^{\top} \bm{x}_i.$

\section{Convexity}

\propconvex*
\begin{proof}[Proof Proposition~\ref{prop:convex} on page~\pageref{prop:convex}]
  It is easy to show that the quantile~$t_1$ is not convex. Due to~\cite{lapin2015top}, the mean of the~$K$ highest values of a vector is a convex function and therefore,~$t_2$ is a convex function. It remains to analyze~$t_3$. Let us dfefine function~$g$ as follows
  \begin{equation*}
    g(\bm{w},t) := \frac{1}{n} \sum_{i \in \I} l(\bm{w}^\top \bm{x}_i - t) - \tau.
  \end{equation*}
  where we for simplicity set~$\vartheta = 1.$ Then~$t_3$ is defined via an implicit equation~$g(\bm{w},t) = 0.$ Moreover, since~$l$ is convex, we immediately obtain that~$g$ is jointly convex in both variables. To show the convexity, consider~$\bm{w}, \; \hat{\bm{w}} \in \R^d$ and the corresponding thresholds~$t = t_3(\bm{w})$,~$\hat{t} = t_3(\hat{\bm{w}})$. Then for any~$\lambda\in[0,1]$ we have 
  \begin{equation}\label{eq:proof_conv1}
    g\Brac{\lambda \bm{w} + (1 - \lambda)\hat{\bm{w}}, \;\lambda t + (1 - \lambda)\hat{t}}
    \le \lambda g(\bm{w}, t) + (1 - \lambda) g(\hat{\bm{w}}, \hat{t}) = 0,
  \end{equation}
  where the inequality follows from the convexity of~$g$ and the equality from
  \begin{equation*}
    g(\bm{w}, t) = g(\hat{\bm{w}}, \hat{t}) = 0,
  \end{equation*}
  which holds true since both~$t$ and $\hat{t}$ solves~\eqref{eq: aatp quantile surrogate}. From the definition of the surrogate quantile function~$t_3$ we have
  \begin{equation}\label{eq:proof_conv2}
    g(\lambda\bm{w} + (1-\lambda)\hat{\bm{w}}, t_3(\lambda\bm{w} + (1-\lambda)\hat{\bm{w}})) = 0.
  \end{equation}
  Since~$g$ is non-increasing in the second variable, from~\eqref{eq:proof_conv1} and~\eqref{eq:proof_conv2} we deduce
  \begin{equation*}
    t_3(\lambda\bm{w} + (1-\lambda)\hat{\bm{w}})
    \le \lambda t + (1-\lambda)\hat{t}
    =   \lambda t_3(\bm{w})+(1-\lambda) t_3(\hat{\bm{w}}),
  \end{equation*}
  which implies that function~$\bm{w}\mapsto t_3(\bm{w})$ is convex.
\end{proof}

\thmconvex*
\begin{proof}[Proof of Theorem~\ref{thm:convex} on page~\pageref{thm:convex}]
  Due to the definition of the surrogate counts~\eqref{eq: confusion counts surrogate}, the function~$L$ equals to
  \begin{equation*}
    L(\bm{w}) = \fns(\bm{s}, t(\bm{w})) = \sum_{i \in \Ipos} l \Brac{t(\bm{w}) - \bm{w}^\top \bm{x}_i}.
  \end{equation*}
  Here we write~$t(\bm{w})$ to stress the dependence of~$t$ on~$\bm{w}$. Since~$\bm{w}\mapsto t(\bm{w})$ is a convex function, we also have that~$\bm{w} \mapsto t(\bm{w}) - \bm{w}^\top \bm{x}$ is a convex function. From its definition, the surrogate function~$l$ is convex and non-decreasing. Since a composition of a convex function with a non-decreasing convex function is a convex function, this finishes the proof.
\end{proof}

\section{Differentiability}

\derivative* 
\begin{proof}[Proof of Theorem~\ref{thm:derivative} on page~\pageref{thm:derivative}]
  The result for~$t_3$ follows directly from the implicit function theorem. The non-differentiability of~$t_1$ and~$t_2$ happens whenever the threshold value is achieved at two different scores.
\end{proof}

\pagebreak

\section{Stability}\label{app: stability}

In this section, we derive the results presented from Section~\ref{sec: stability} more properly. 
\degeneratebehavior*
\noindent Moreover, we assume that~$n$ is large and the outlier may be ignored for the computation of thresholds which require a large number of points. Since the computation is simple for other formulations, we show it only for \PatMat. For~$\bm{w}_0 = (0,0)$, we get
\begin{equation*}
  \tau
  = \frac{1}{n}\sum_{i \in \I} l\Brac{\vartheta(\bm{w}_0^\top \bm{x}_i - t)}
  = l(0 - \vartheta t) = 1 - \vartheta t,
\end{equation*}
which implies
\begin{equation*}\label{eq: PatMat threshold 0}
  t = \nicefrac{1-\tau}{\vartheta}
\end{equation*}
and consequently the value of the objective function is
\begin{equation}\label{eq: PatMat objective 0}
  L(\bm{w}_0)
    = \frac{1}{\npos} \sum_{i \in \Ipos} l(t - \bm{w}_0^\top \bm{x}_i)
    = l(t - 0)
    = 1 + t,
\end{equation}
where the last equality follows from definition of the hinge loss function and the fact that~$t \ge 0.$ This finishes the computation for~$\bm{w}_0$. For~$\bm{w}_1 = (1,0)$ the computation goes similar. Since~$\bm{w}_1^\top \bm{x}$  for~$i \in \I$ has the uniform distribution on~$[-1,1],$ we have
\begin{equation*}
  \tau
    = \frac{1}{n} \sum_{i \in \I} l\Brac{\vartheta (\bm{w}_1^\top \bm{x}_i - t)}
    \approx \frac{1}{2}\int_{-1}^{1} l\Brac{\vartheta(s-t)} \dd{s}
    = \frac{1}{2} \int_{-1}^{1} \max\{0, 1 + \vartheta(s - t)\}\dd{s}
\end{equation*}
If~$\vartheta \le \tau$, then
\begin{equation*}
  1 + \vartheta(s - t)
    \ge 1 + \vartheta(-1 - t)
    = 1 - \vartheta - 1 + \tau
    = \tau - \beta
    \ge 0.
\end{equation*}
Using this inequality, we can ignore the max operator in the relation for the~$\tau$ above and get
\begin{equation}\label{eq:example1}
  \tau
    =\frac{1}{2} \int_{-1}^{1} (1+\vartheta(s - t))\dd{s}
    = 1 - \vartheta t + \frac{\vartheta}{2}\int_{-1}^{1}s\dd{s}
    = 1 - \vartheta t,
\end{equation}
and thus again~$t = \nicefrac{1-\tau}{\vartheta}$. Finally, since~$\bm{w}_1^\top \bm{x}$  for~$i \in \Ipos$ has the uniform distribution on~$[0,1],$ we have
\begin{equation*}
  L(\bm{w}_1)
    = \frac{1}{\npos} \sum_{i \in \Ipos} l(t - \bm{w}_1^\top \bm{x}_i)
    \approx \int_{0}^{1} l(t-s)\dd{s}
    = \int_{0}^{1} (1 + t - s)\dd{s}
    = 0.5 + t.
\end{equation*}
Results for \PatMatNP can be obtained in a similar way. In the rest of the section we provide proofs for all the theorems from Section~\ref{sec: stability}.

\larget*
\begin{proof}[Proof of Theorem~\ref{thm:large_t} on page~\pageref{thm:large_t}]
  All mentioned formulations use surogate approximation of the false-negative rate as the objective function~$L.$ For the linear classifier, the objective function has the following form
  \begin{equation*}
    L(\bm{w})
      = \frac{1}{\npos}\sum_{i \in \Ipos}l(t - \bm{w}^\top \bm{x}_i)
  \end{equation*}
  Due to~$l(0) = 1$ and the convexity of~$l$ we have~$l(s) \ge 1 + cs$, where~$c$ equals to the derivative of~$l$ at~$0$. Then we have
  \begin{equation*}
    L(\bm{w}) 
      \ge \frac{1}{\npos} \sum_{i \in \Ipos}(1 + c(t-\bm{w}^\top \bm{x}_i))
      = 1 + c\Brac{t - \frac{1}{\npos}\sum_{i \in \Ipos}\bm{w}^\top \bm{x}}
      \ge 1,
  \end{equation*}
  where the last inequality follows from assumption~\eqref{eq:w_zero_nn}. Now we realize that for any formulation from the statement, the corresponding threshold for~$\bm{w}=0$ equals to~$t=0$, and thus~$L(\bm{0})=1$. But then~$L(\bm{0}) \le L(\bm{w})$. The second part of the result follows from the form of thresholds~$t(\bm{w})$.
\end{proof}

\patmatzero*
\begin{proof}[Proof of Theorem~\ref{thm:patmat_zero} on page~\pageref{thm:patmat_zero}]
  Firstly recall thewe use linear model and Notation~\ref{not: scores} and define the following auxilliary variables
  \begin{equation*}
    \begin{aligned}
      s_{\min} & = \min \Set{s_i}{i \in \I}, \qquad
      s_{\max} & = \max \Set{s_i}{i \in \I}, \qquad
      \bar{s} & = \frac{1}{n} \sum_{i \in \I} s_i. \\
    \end{aligned}
  \end{equation*}
  Using the definition of~$\bar{s}$ we get the following relation
  \begin{equation}\label{eq:patmat_zero_aux0}
    \bar{s}
      = \frac{1}{n} \sum_{i \in \I} s_i
      = \frac{1}{n}\sum_{i \in \Ipos} s_i + \frac{1}{n} \sum_{i \in \Ineg} s_i
      < \frac{1}{n} \sum_{i \in \Ipos} s_i + \frac{\nneg}{n\npos} \sum_{i \in \Ipos} s_i
      = \frac{1}{\npos} \sum_{i \in \Ipos} s_i,
  \end{equation}
  where the inequality follows from~\eqref{eq:patmat_zero} and the last equality follows from
  \begin{equation*}
    \frac{1}{n} + \frac{\nneg}{n\npos}
      = \frac{1}{n} \Brac{1 + \frac{\nneg}{\npos}} 
      = \frac{1}{n} \frac{\npos + \nneg}{\npos}
      = \frac{1}{n} \frac{n}{\npos}
      = \frac{1}{\npos}.
  \end{equation*}
  Moreover, since the average of elements of the  vector is smaller or equal to the maximum of elements of the same vector, we get the following relation
  \begin{equation*}
    \bar{s}
      < \frac{1}{\npos} \sum_{i \in \Ipos} s_i
      \le \max \Set{s_i}{i \in \Ipos}
      \le \max \Set{s_i}{i \in \I}
      = s_{\max}
  \end{equation*}
  where the first inequality follows from~\eqref{eq:patmat_zero_aux0}. The lower bound for~$\bar{s}$ can be computed in a similar way. Altogether, we have~$s_{\min} < \bar{s} < s_{\max}$. Then we can define
  \begin{equation*}
    \vartheta_0 = \min\Brac[c]{\frac{\tau}{\bar{s} - s_{\min}}, \; \frac{1-\tau}{s_{\max}-\bar{s}}, \; \tau},
  \end{equation*}
  observe that~$\vartheta_0 > 0$, fix any~$\vartheta \in (0, \vartheta_0)$ and define
  \begin{equation*}
    t = \frac{1 - \tau}{\vartheta} + \bar{s}.
  \end{equation*}
  Then we obtain for any~$i \in \I$
  \begin{equation*}
    1 + \vartheta(s_i - t)
      \ge 1 + \vartheta(s_{\min} - t)
      = 1 + \vartheta s_{\min} - 1 + \tau - \vartheta\bar{s}
      = \tau - \vartheta (\bar{s} - s_{\min}),
  \end{equation*}
  where the first equality follows from the definition of~$t.$ From the definition~$\vartheta_0$ we known the following
  \begin{equation*}
    0 < \vartheta \le \vartheta_0 \le \frac{\tau}{\bar{s} - s_{\min}}.
  \end{equation*}
  Since~$\bar{s} - s_{\min} > 0,$ we get the following inequality
  \begin{equation}\label{eq:patmat_zero_aux1}
    1 + \vartheta(s_i - t)
      = \tau - \vartheta (\bar{s} - s_{\min})
      \ge \tau - \frac{\tau}{\bar{s} - s_{\min}} (\bar{s} - s_{\min})
      = 0
  \end{equation}
  Moreover, combining the definition of the hinge loss function in Notation~\ref{not: surrogates} and the inequality above, we have
  \begin{equation*}
    l\Brac{\vartheta (s_i - t)} = \max\{0, 1 + \vartheta (s_i - t), 0\} = \Brac{1 + \vartheta(s_i - t)}.
  \end{equation*}
  Finally, replacing the hinge loss in the left hand side of~\eqref{eq: aatp quantile surrogate} leads to
  \begin{equation*}
    \begin{aligned}
      \frac{1}{n} \sum_{i \in \I} l\Brac{\vartheta (s_i - t)}
      & = \frac{1}{n}\sum_{i \in \I}\Brac{1 + \vartheta(s_i - t)} \\
      & = 1 - \vartheta t + \frac{\vartheta}{n} \sum_{i \in \I} s_i \\
      & = 1 - \vartheta \Brac{\frac{1 - \tau}{\vartheta} + \bar{s}} + \vartheta \bar{s} \\
      & = \tau,
    \end{aligned}
  \end{equation*}
  where the third equality employs the definition of~$\bar{s}$ and~$t$. But this means that~$t$ is the threshold corresponding to~$\bm{w}$, i.e. it solves~\eqref{eq: aatp quantile surrogate}. Similarly to~\eqref{eq:patmat_zero_aux1} we get
  \begin{equation}\label{eq:patmat_zero_aux2}
    1 + t - s_i
    \ge 1 + t-s_{\max}
    =   1 + \frac{1-\tau}{\vartheta} + \bar{s} - s_{\max}
    \ge \frac{1-\tau}{\vartheta} + \bar{s} - s_{\max}
    \ge 0,
  \end{equation}
  where the last inequality follows from the definition of~$\vartheta_0$. Then for the objective we have
  \begin{equation*}
    \begin{aligned}
      L(\bm{w}) = \frac{1}{\npos}\sum_{i \in \Ipos}l(t-s_i)
      & = \frac{1}{\npos}\sum_{i \in \Ipos}\Brac{1+t-s_i} \\
      & = 1 + t - \frac{1}{\npos}\sum_{i \in \Ipos} s_i \\
      & < 1 + \Brac{\frac{1 - \tau}{\vartheta} + \bar{s}} - \bar{s} \\
      & = 1 + \frac{1-\tau}{\vartheta} \\
      & = L(\bm{0}),\\
    \end{aligned}
  \end{equation*}
  where the second equality follows from~\eqref{eq:patmat_zero_aux2}, the only inequality from~\eqref{eq:patmat_zero_aux0} and the last equality from~\eqref{eq: PatMat threshold 0} and~\eqref{eq: PatMat objective 0}. Thus, we finished the proof for \PatMat. The proof for \PatMatNP can be performed in an identical way by replacing in the definition of~$\bar{s}$ the mean with respect to all samples by the mean with respect to all negative samples.
\end{proof}

\section{Threshold comparison}\label{app:relations}

Whenever the objective contains only false-negatives, a lower threshold~$t$ means a lower objective function. Therefore, a lower threshold is preferred. The two following lemmas compares thresholds defined in Chapter~\ref{chap: framework} in terms of approximation quality.
\begin{lemma}[Thresholds relation~\cite{zhang2018tau}]\label{prop: threholds}
  We always have
  \begin{equation*}
    t_1(\bm{s}) \le t_2(\bm{s}) \le t_3(\bm{s}).
  \end{equation*}
\end{lemma}

\pagebreak

\begin{lemma}\label{lemma:thresholds2}
  Consider the \Grill, \GrillNP, \TopMeanK and \tauFPL formulations and the notation from Notation~\ref{not: scores}. Then we have the following statements:
  \begin{equation*}
    \begin{aligned}
      s_{[\npos\tau]}^+ > s_{[\nneg\tau]}^-
        & \implies \Grill \text{ has larger threshold than }\GrillNP, \\
      \frac{1}{\npos\tau}\sum_{i=1}^{\npos\tau} s_{[i]}^+
      > \frac{1}{\nneg\tau}\sum_{i=1}^{\nneg\tau} s_{[i]}^-
        & \implies \TopMeanK \text{ has larger threshold than }\tauFPL. \\
    \end{aligned}
  \end{equation*}
\end{lemma}

\begin{proof}
  Since~$\bm{s}^+$ and~$\bm{s}^-$ are computed on disjunctive indices, we have
  \begin{equation*}
    s_{[n\tau]} \ge \min\{s_{[\npos\tau]}^+, \; s_{[\nneg\tau]}^-\}.
  \end{equation*}
  Since~$s_{[n\tau]}$ is the threshold for \Grill and~$s_{[\nneg\tau]}^-$ is the threshold for \GrillNP, the first statement follows. The second part can be shown in a similar way.
\end{proof}
  
\noindent Since the goal of the presented formulations is to push~$s^+$ above~$s^-$, we may expect that the conditions in Lemma~\ref{lemma:thresholds2} hold true. 

\section{Computing the threshold for \PatMat}\label{app:threshold}

We show how to efficiently compute the threshold~\eqref{eq: aatp quantile surrogate} for \PatMat with linear model and the hinge surrogate from Notation~\ref{not: surrogates}. Consider function
\begin{equation}\label{eq:defin_h}
  h(t) = \sum_{i \in \I} l\Brac{\vartheta(s_i - t)} - n\tau.
\end{equation}
Then solving~\eqref{eq:update_t} is equivalent to looking for~$\hat{t}$ such that~$h(\hat{t}) = 0$. Function~$h$ is continuous and strictly decreasing (until it hits the global minimum) with~$h(t) \to \infty$ as~$t \to -\infty$ and~$h(t) \to -n\tau$ as~$t \to \infty$. Thus, there is a unique solution to the equation~$h(t) = 0$. For sorted data, the following lemma gives advice on how to solve equation~$h(t) = 0$. 

\begin{lemma}
  Consider vector of scores~$\bm{s}$ and its sorted version~$\bm{s}_{[\cdot]}$ with decreasing elements as defined in Notation~\ref{not: scores}. Define~$\gamma = \nicefrac{1}{\vartheta}$. Then 
  \begin{equation}\label{eq:update_h}
    h(s_{[j]} + \gamma) = h(s_{[j - 1]} + \gamma) + (j - 1) \vartheta(s_{[j - 1]} - s_{[j]})
  \end{equation}
  for all~$i = 2, \; 3, \ldots, n$ with the initial condition~$h(s_{[1]} + \gamma) = -n\tau$.
\end{lemma}
\begin{proof}
Observe first that
\begin{equation*}
  \begin{aligned}
    h(s_{[j]}+\gamma)
      & = \sum_{i \in \I} l\Brac{\vartheta(s_i - (s_{[j]} + \gamma))} - n\tau \\
      & = \sum_{i \in \I} \max\Brac[c]{0,\; 1 + \vartheta\Brac{s_i - s_{[j]} - \frac{1}{\gamma}}} - n\tau \\
      & = \sum_{i = 1}^{j - 1} \vartheta(s_{[i]} - s_{[j]}) - n\tau,
  \end{aligned}
\end{equation*}
where the last equality holds since~$\vartheta > 0$ and~$s_{[i]} - s_{[j]} \le 0$ for all~$i \geq j$
From here, we obtain~$h(s_{[1]} + \gamma) = -n\tau$. Moreover, we have
\begin{equation*}
  \begin{aligned}
    h(s_{[j]} + \gamma)
    & = \sum_{i = 1}^{j - 1} \vartheta(s_{[i]} - s_{[j]}) - n\tau \\
    & = \sum_{i = 1}^{j - 2} \vartheta(s_{[i]} - s_{[j]}) + \vartheta(s_{[j-1]} - s_{[j]}) - n\tau \\
    & = \sum_{i = 1}^{j - 2} \vartheta(s_{[i]} - s_{[j]} \pm s_{[j - 1]}) + \vartheta(s_{[j-1]} - s_{[j]}) - n\tau \\
    & = \sum_{i = 1}^{j - 2} \vartheta(s_{[i]} - s_{[j - 1]}) + \sum_{i = 1}^{j - 2} \vartheta(s_{[j - 1]} - s_{[j]}) + \vartheta(s_{[j - 1]} - s_{[j]}) - n\tau \\
    & = h(s_{[j - 1]} + \gamma) + (j - 1) \vartheta(s_{[j - 1]} - s_{[j]}),
  \end{aligned}
\end{equation*}
which finishes the proof.
\end{proof}

\noindent Thus, to solve~$h(t) = 0$ with the hinge surrogate, we start with~$t_1 = s_{[1]}  + \gamma$ and~$h(t_1) = -n\tau$. Then we start decreasing~$t$ according to~\eqref{eq:update_h} until we find some~$t_i = s_{[i]} + \gamma$ such that~$h(t_i) > 0$. The desired~$t$ then lies between~$t_i$ and~$t_{i-1}$. Since~$h$ is a piecewise linear function with
\begin{equation*}
  h(t) = h(t_{i-1}) + \frac{t - t_{i-1}}{t_{i} - t_{i-1}}\Brac{h(t_{i}) - h(t_{i-1})}
\end{equation*}
for~$t \in [t_{i-1}, \; t_{i}]$, the precise value of~$\hat{t}$ can be computed by a simple interpolation
\begin{equation*}
  \hat{t}
    = t_{i-1} - h(t_{i-1})\frac{t_{i} - t_{i-1}}{h(t_{i}) - h(t_{i-1})}
    = t_{i-1} - h(t_{i-1})\frac{t_{i} - t_{i-1}}{-(i-1)\vartheta(t_{i} - t_{i-1})}
    = t_{i-1} + \frac{h(t_{i-1})}{\vartheta(i-1)}.
\end{equation*}

\section{Convergence of stochastic gradient descent}

The proof is divided into three parts. In Section~\ref{app:sgd1}, we prove a general statement for convergence of stochastic gradient descent with a convex objective. In Section~\ref{app:sgd2} we apply it to Theorem~\ref{thm:sgd}. The proof is based on auxiliary results from Section~\ref{app:sgd3}.

\subsection{General result}\label{app:sgd1}

Consider a differentiable objective function~$L$ and the optimization method
\begin{equation}\label{eq:update}
  \bm{w}^{k+1} = \bm{w}^k - \alpha^k g(\bm{w}^k),
\end{equation}
where~$\alpha^k > 0$ is a stepsize and~$g(\bm{w}^k)$ is an approximation of the gradient~$\nabla L(\bm{w}^k)$. Assume the following:
\begin{enumerate}[label={(A\arabic*)}]
  \item \label{ass_convex}~$L$ is differentiable, convex and attains a global minimum;
  \item \label{ass_gbound}~$\norm{g(\bm{w}^k)}\le B$ for all~$k$;
  \item \label{ass_alpha1} the stepsize is non-increasing and satisfies~$\sum_{k=0}^\infty \alpha^k = \infty$;
  \item \label{ass_alpha2} the stepsize satisfies~$\sum_{k=0}^\infty (\alpha^k)^2<\infty$;
  \item \label{ass_alpha3} the stepsize satisfies~$\sum_{k=0}^\infty \norm{\alpha^{k+1}-  \alpha^k}<\infty$.
\end{enumerate}
Assumptions~\ref{ass_alpha1}-\ref{ass_alpha3} are satisfied for example for~$\alpha^k = \nicefrac{\alpha^0}{k+1}$. We start with the general result.

\begin{theorem}\label{thm:convergence}
  Assume that~\ref{ass_convex}-\ref{ass_alpha2} is satisfied. If there exists some~$C$ such that for some global minimum of~$\bm{w}^*$ of~$L$ we have
  \begin{equation}\label{eq:nec_cond}
    \sum_{k=0}^\infty \alpha^k \inner{g(\bm{w}^k) - \nabla L(\bm{w}^k)}{\bm{w}^* - \bm{w}^k} \le C,
  \end{equation}
  then the sequence~$\{\bm{w}^k\}$ generated by~\eqref{eq:update} is bounded and~$L(\bm{w}^k) \to L(\bm{w}^*)$. Thus, all its convergent subsequences converge to some global minimum of~$L$.
\end{theorem}
\begin{proof}
  Note first that the convexity of~$L$ from~\ref{ass_convex} implies
  \begin{equation}\label{eq:convex_estimate}
    \inner{\nabla L(\bm{w}^k)}{\bm{w}^* - \bm{w}^k} \le L(\bm{w}^*) - L(\bm{w}^k).
  \end{equation}
  Then we have
  \begin{equation*}
    \begin{aligned}
      \norm{\bm{w}^{k+1} - \bm{w}^*}^2
        = \; & \norm{\bm{w}^k - \alpha^k g(\bm{w}^k) - \bm{w}^*}^2 \\
        = \; &\norm{\bm{w}^k - \bm{w}^*}^2 + 2\alpha^k\inner{g(\bm{w}^k)}{\bm{w}^* - \bm{w}^k} + (\alpha^k)^2 \norm{g(\bm{w}^k)}^2 \\
        \le \; &\norm{\bm{w}^k - \bm{w}^*}^2 + 2\alpha^k\inner{g(\bm{w}^k) \pm \nabla L(\bm{w}^k)}{\bm{w}^* - \bm{w}^k} + (\alpha^k)^2 B^2\\
        \le \; & \norm{\bm{w}^k - \bm{w}^*}^2 + 2 \alpha^k \inner{g(\bm{w}^k) - \nabla L(\bm{w}^k)}{\bm{w}^* - \bm{w}^k} \\
        & + 2 \alpha^k \Brac{L(\bm{w}^*) - L(\bm{w}^k)} + (\alpha^k)^2 B^2,
    \end{aligned}
  \end{equation*}
  where the first inequality follows from assumption~\ref{ass_gbound} and the second on from the properties of inner product and~\eqref{eq:convex_estimate}. Summing this expression for all~$k$ and using~\eqref{eq:nec_cond} leads to
  \begin{equation*}
    \limsup_{k \rightarrow \infty} \; \norm{\bm{w}^k - \bm{w}^*}^2
      \le \norm{\bm{w}^0 - \bm{w}^*}^2 + 2C + 2 \sum_{k=0}^\infty \alpha^k (L(\bm{w}^*) - L(\bm{w}^k)) + \sum_{k=0}^ \infty (\alpha^k)^2 B^2.
\end{equation*}
  Using assumption~\ref{ass_alpha2} results in the existence of some~$\hat{C}$ such that
  \begin{equation}\label{eq:general_bound}
  \limsup_{k \rightarrow \infty} \;\norm{\bm{w}^k - \bm{w}^*}^2 + 2\sum_{k=0}^\infty \alpha^k \Brac{L(\bm{w}^k) - L(\bm{w}^*)} \le 2 \hat{C}.
  \end{equation}
  Since~$\alpha^k > 0$ and~$L(\bm{w}^k) \ge L(\bm{w}^*)$ as~$\bm{w}^*$ is a global minimum of~$L$, we infer that sequence~$\{\bm{w}^k\}$ is bounded and~\eqref{eq:general_bound} implies
  \begin{equation*}
    \sum_{k=0}^\infty \alpha^k \Brac{L(\bm{w}^k) - L(\bm{w}^*)} \le \hat{C}.
  \end{equation*}
  Since~$L(\bm{w}^k) - L(\bm{w}^*) \ge 0$, due to assumption~\ref{ass_alpha1} we obtain
  \begin{equation*}
    \lim_{k \to \infty} L(\bm{w}^k) = L(\bm{w}^*),
  \end{equation*}
  which implies the theorem statement.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:sgd}}\label{app:sgd2}

For the proof, we will consider a general surrogate which satisfies:
\begin{enumerate}[label={(S\arabic*)}]
  \item \label{surr_basic1} $l(s)\ge 0$ for all~$s\in\R$, $l(0)=1$ and~$l(s)\to 0$ as~$s\to-\infty$;
  \item \label{surr_basic2} $l$ is convex and strictly increasing function on~$(s_0,\infty)$, where~$s_0:=\sup\{s \mid l(s)=0\}$;
  \item \label{surr_ratio} $\nicefrac{l'}{l}$ is a decreasing function on~$(s_0,\infty)$;
  \item \label{surr_der1} $l'$ is a bounded function;
  \item \label{surr_der2} $l'$ is a Lipschitz continuous function with Lipschitz constant~$D$.
\end{enumerate}
All these reguirements are satisfied for the surrogate logistic or by the Huber loss, which is the hinge surrogate which is smoothened on an~$\eps$-neighborhood of zero.

\sgd*
\begin{proof}[Proof of Theorem~\ref{thm:sgd} on page~\pageref{thm:sgd}]
  We intend to apply Theorem~\ref{thm:convergence} and thus, we need to verify its assumptions. Assumption~\ref{ass_convex} is satisfied as~$L$ is convex due to Theorem~\ref{thm:convex}. Assumption~\ref{ass_gbound} follows directly from Lemma~\ref{lemma:bound_g}. Assumptions~\ref{ass_alpha1},\ref{ass_alpha2} and~\ref{ass_alpha3} are imposed directly in the statement of this theorem. It remains to verify~\eqref{eq:nec_cond}.

  For simplicity, we will do so only for~$\vartheta = 1$ and for~$2$ minibatches of the same size. However, the proof would be identical for other values. This implies that there are some~$\Imb^k$ and~$\Imb^{k+1}$ which are pairwise disjoint, they cover all samples and that~$\Imb^k = \Imb^{k+2}$ for all~$k$. The assumptions imply that the number of positive samples in each minibatch equal to~$\nmbpos^k = \nicefrac{\npos}{2}$, where~$\npos$ is the total number of positive samples.

  First we estimate the difference between~$s_i^k$ defined in~\eqref{eq:defin_z} and~$\bm{x}_i^\top \bm{w}^k$. For any~$i \in \Imb^k$ we have
  \begin{equation*}
    s_i^k = \bm{x}_i^\top \bm{w}^k
  \end{equation*}
  and since we have two disjoint minibatches, due to the construction~\eqref{eq:defin_z} we get
  \begin{equation}\label{eq:sgd_estimate_z1}
    \begin{aligned}
      s_i^{k-1}
          = s_i^{k-2}
        & = \bm{x}_i^\top \bm{w}^{k-2} \\
        & = \bm{x}_i^\top \Brac{\bm{w}^k + \alpha^{k-2}g(\bm{w}^{k-2}) + \alpha^{k-1} g(\bm{w}^{k-1})} \\
        & = \bm{x}_i^\top \bm{w}^k + \alpha^{k-2}\bm{x}_i^\top g(\bm{w}^{k-2}) + \alpha^{k-1}\bm{x}_i^\top g(\bm{w}^{k-1}).
    \end{aligned}
  \end{equation}
  Similarly, due to the construction~\eqref{eq:defin_z}, for~$i \notin \Imb^k$ we have
  \begin{equation}\label{eq:sgd_estimate_z2}
    s_i^k
    = s_i^{k-1}
    = \bm{x}_i^\top \bm{w}^{k-1}
    = \bm{x}_i^\top (\bm{w}^k+\alpha^{k-1}g(\bm{w}^{k-1}))
    = \bm{x}_i^\top \bm{w}^k + \alpha^{k-1}\bm{x}_i^\top g(\bm{w}^{k-1}).
  \end{equation}
  Recall that we already verified~\ref{ass_convex}-\ref{ass_alpha3}. Combining~\ref{ass_gbound} with~\eqref{eq:sgd_estimate_z1} and~\eqref{eq:sgd_estimate_z2} yields the existence of some~$C_2$ such that for all~$i \in \I$ we have
  \begin{equation}\label{eq:estimate_diff_z}
    \begin{aligned}
      \norm{s_i^k - \bm{x}_i^\top \bm{w}^k} &\le C_2\alpha^{k-1}, \\
      \norm{s_i^{k-1} - \bm{x}_i^\top \bm{w}^k} &\le C_2\Brac{\alpha^{k-1}+\alpha^{k-2}}. \\
    \end{aligned}
  \end{equation}
  This also immediately implies
  \begin{equation}\label{eq:estimate_diff_t}
    \begin{aligned}
      \norm{t^k - t(\bm{w}^k)}     & \le C_2\alpha^{k-1}, \\
      \norm{t^{k-1} - t(\bm{w}^k)} & \le C_2\Brac{\alpha^{k-1}+\alpha^{k-2}}. \\
    \end{aligned}
  \end{equation}
  Since~$l'$ is Lipschitz continuous with Lipschitz constant~$D$ according to~\ref{surr_der2}, due to~\eqref{eq:estimate_diff_z} and~\eqref{eq:estimate_diff_t} we get
  \begin{equation}\label{eq:sgd_lipschitz1}
    \begin{aligned}
      \norm{l'(t^k-s_i^k) - l'(t(\bm{w}^k)-\bm{x}_i^\top \bm{w}^k)}
        & \le D \norm{t^k-s_i^k - t(\bm{w}^k)+ \bm{x}_i^\top \bm{w}^k}
        \le  2C_2 D \alpha^{k-1}.
    \end{aligned}
  \end{equation}
  In an identical way we can show
  \begin{equation}\label{eq:sgd_lipschitz2}
    \begin{aligned}
      \norm{l'(t^{k-1}-s_i^{k-1}) - l'(t(\bm{w}^k)-\bm{x}_i^\top \bm{w}^k)}
        & \le 2C_2D\Brac{\alpha^{k-1}+\alpha^{k-2}}, \\
      \norm{l'(s_i^k-t^k) - l'(\bm{x}_i^\top \bm{w}^k-t(\bm{w}^k))}
        & \le 2C_2D\alpha^{k-1}, \\
      \norm{l'(s_i^{k-1}-t^{k-1}) - l'(\bm{x}_i^\top \bm{w}^k-t(\bm{w}^k))}
        & \le 2C_2D\Brac{\alpha^{k-1}+\alpha^{k-2}}.
    \end{aligned}
  \end{equation}
  Now we need to estimate the distance between~$\nabla t(\bm{w}^k)$ and~$\nabla t^k$. From~\eqref{eq:update_nablat} and~\eqref{eq:update_a}, we have
  \begin{equation*}
    \nabla t^k
      = \frac{\sum_{i \in \Imb^k} l'(s_i^k - t^k)\bm{x}_i + \sum_{i \in \Imb^{k-1}} l'(s_i^{k-1} - t^{k-1}) \bm{x}_i}{\sum_{i \in \I} l'(s_i^k - t^k)}.
  \end{equation*}
  Moreover, using Theorem~\ref{thm:derivative} and the fact that we have only two minibatches and therefore for any~$k$ we have~$\I = \Imb^k \cup \Imb^{k-1}$, we get
  \begin{equation*}
    \nabla t(\bm{w}^k)
      = \frac{\sum_{i \in \Imb^k} l'(\bm{x}_i^\top \bm{w}^k - t(\bm{w}^k))\bm{x}_i + \sum_{i \in \Imb^{k-1}} l'(\bm{x}_i^\top \bm{w}^k - t(\bm{w}^k))\bm{x}_i}{\sum_{i \in \I} l'(\bm{x}_i^\top \bm{w}^k - t(\bm{w}^k))}.
  \end{equation*}
  From Lemma~\ref{lemma:bound_zero} we deduce that the denominators in the relations above are bounded away from zero uniformly in~$k$. Assumption~\ref{ass_alpha2} implies ~$\alpha^k \to 0$. This allows us to use Lemma~\ref{lemma:ratio} which together with~\eqref{eq:sgd_lipschitz2} implies that there is some~$C_3$ such that for all sufficiently large~$k$ we have
  \begin{equation}\label{eq:sgd_nablat_diff}
    \norm{\nabla t^k - \nabla t(\bm{w}^k)} \le C_3\Brac{\alpha^{k-1} + \alpha^{k-2}}.
  \end{equation}
  Using the assumptions above, we can simplify the terms for~$g(\bm{w}^k)$ and~$\nabla L(\bm{w}^k)$ to
  \begin{equation*}
    \begin{aligned}
      g(\bm{w}^k)
        & = \frac{2}{\npos} \sum_{i \in \Imbpos^k} l'(t^k - s_i^k)(\nabla t^k - \bm{x}_i), \\
      g(\bm{w}^{k+1})
        & = \frac{2}{\npos} \sum_{i \in \Imbpos^{k+1}} l'(t^{k+1}-s_i^{k+1})(\nabla t^{k+1} - \bm{x}_i), \\
      \nabla L(\bm{w}^k)
        & = \frac{1}{\npos} \sum_{i \in \Ipos} l'(t(\bm{w}^k) - \bm{x}_i^\top \bm{w}^k)(\nabla t(\bm{w}^k) - \bm{x}_i), \\
      \nabla L(\bm{w}^{k+1})
        & = \frac{1}{\npos} \sum_{i \in \Ipos} l'(t(\bm{w}^{k+1}) - \bm{x}_i^\top \bm{w}^{k+1})(\nabla t(\bm{w}^{k+1}) - \bm{x}_i).
    \end{aligned}
  \end{equation*}
  Due to the assumptions, we have~$\Ipos = \Imbpos^k \cup \Imbpos^{k+1}$ and~$\emptyset = \Imbpos^k \cap \Imbpos^{k+1}$, which allows us to write
  \begin{subequations}\label{eq:sgd_sum}
    \begin{align}
    \label{eq:sgd_sum1}
    \npos & \Brac{g(\bm{w}^k) + g(\bm{w}^{k+1}) - \nabla f(\bm{w}^k)-\nabla f(\bm{w}^{k+1})}\\
    \label{eq:sgd_sum2}
    & = \sum_{i \in \Imbpos^k} l'(t^k - s_i^k)(\nabla t^k - \bm{x}_i) - \sum_{i \in \Imbpos^k} l'(t(\bm{w}^k) - \bm{x}_i^\top \bm{w}^k)(\nabla t(\bm{w}^k) - \bm{x}_i) \\
    \label{eq:sgd_sum3}
    & + \sum_{i \in \Imbpos^k} l'(t^k - s_i^k)(\nabla t^k - \bm{x}_i) - \sum_{i \in \Imbpos^k} l'(t(\bm{w}^{k+1}) - \bm{x}_i^\top \bm{w}^{k+1})(\nabla t(\bm{w}^{k+1}) - \bm{x}_i)\\
    \label{eq:sgd_sum4}
    & + \sum_{i \in \Imbpos^{k+1}} l'(t^{k+1} - s_i^{k+1})(\nabla t^{k+1} - \bm{x}_i) - \sum_{i\in \Imbpos^{k+1}}l'(t(\bm{w}^k) - \bm{x}_i^\top \bm{w}^k)(\nabla t(\bm{w}^k) - \bm{x}_i) \\
    \label{eq:sgd_sum5}
    & + \sum_{i \in \Imbpos^{k+1}} l'(t^{k+1} - s_i^{k+1})(\nabla t^{k+1} - \bm{x}_i)  - \sum_{i \in \Imbpos^{k+1}} l'(t(\bm{w}^{k+1}) - \bm{x}_i^\top \bm{w}^{k+1})(\nabla t(\bm{w}^{k+1}) - \bm{x}_i).
    \end{align}
  \end{subequations}
  Then relations~\eqref{eq:sgd_nablat_diff} and~\eqref{eq:sgd_lipschitz1} applied to Lemma~\ref{lemma:product} imply
  \begin{multline*}
    \norm{\sum_{i \in \Imbpos^k} l'(t^k - s_i^k)(\nabla t^k - \bm{x}_i) - \sum_{i \in \Imbpos^k} l'(t(\bm{w}^k) - \bm{x}_i^\top \bm{w}^k)(\nabla t(\bm{w}^k) - \bm{x}_i)}\\
      \le C_4 \Brac{\alpha^{k-1} + \alpha^{k-2}}
  \end{multline*}
  for some~$C_4$, which gives a bound for~\eqref{eq:sgd_sum2}. Bound for~\eqref{eq:sgd_sum5} is obtained by increasing~$k$ by one. Bounds for~\eqref{eq:sgd_sum3} and~\eqref{eq:sgd_sum4} can be find similarly using~\eqref{eq:sgd_lipschitz2}. Altogether, we showed
  \begin{equation}\label{eq:nec_cond3}
    \norm{g(\bm{w}^k) + g(\bm{w}^{k+1}) - \nabla L(\bm{w}^k) - \nabla L(\bm{w}^{k+1})}
      \le C_1(\alpha^{k-2} + \alpha^{k-1} + \alpha^{k} + \alpha^{k+1})
  \end{equation}
  for some~$C_1$. We now estimate
  \begin{equation}\label{eq:proof_est1}
    \begin{aligned}
      \alpha^k
      & \inner{ g(\bm{w}^{k})-\nabla L(\bm{w}^{k})}{\bm{w}^*-\bm{w}^{k}} + \alpha^{k+1}\inner{ g(\bm{w}^{k+1})-\nabla L(\bm{w}^{k+1})}{\bm{w}^*-\bm{w}^{k+1}} \\
      & = \inner{ g(\bm{w}^{k})-\nabla L(\bm{w}^{k})}{\alpha^k(\bm{w}^*-\bm{w}^{k})}
        + \inner{ g(\bm{w}^{k+1})-\nabla L(\bm{w}^{k+1})}{\alpha^{k+1}(\bm{w}^*-\bm{w}^{k+1})} \\
      & = \inner{ g(\bm{w}^{k})-\nabla L(\bm{w}^{k}) + g(\bm{w}^{k+1})-\nabla L(\bm{w}^{k+1})}{\alpha^k(\bm{w}^*-\bm{w}^{k})} \\
      & + \inner{ g(\bm{w}^{k+1})-\nabla L(\bm{w}^{k+1})}{\alpha^{k+1}(\bm{w}^*-\bm{w}^{k+1})-\alpha^k(\bm{w}^*-\bm{w}^{k})}.
    \end{aligned}
  \end{equation}
  To estimate the second part of the right hand side of~\eqref{eq:proof_est1}, we make use of Lemma~\ref{lemma:bound_g} to obtain the existence of some~$C_5$ such that
  \begin{equation}\label{eq:proof_est2}
    \begin{aligned}
    & \inner{ g(\bm{w}^{k+1})
    -\nabla L(\bm{w}^{k+1})}{\alpha^{k+1}(\bm{w}^*-\bm{w}^{k+1})-\alpha^k(\bm{w}^*-\bm{w}^{k})} \\
    & \le 2B\norm{\alpha^{k+1}(\bm{w}^*-\bm{w}^{k+1})-\alpha^k(\bm{w}^*-\bm{w}^{k})} \\
    & = 2B\norm{\alpha^{k+1}(\bm{w}^*-\bm{w}^k+\alpha^kg(\bm{w}^k))-\alpha^k(\bm{w}^*-\bm{w}^{k})} \\
    & = 2B\norm{(\alpha^{k+1}-\alpha^k)\bm{w}^* + (\alpha^k-\alpha^{k+1})\bm{w}^k +\alpha^k\alpha^{k+1} g(\bm{w}^k)} \\
    & \le C_5 \norm{\alpha^{k+1}-\alpha^k} + C_5(\alpha^k)^2 + C_5(\alpha^{k+1})^2.
    \end{aligned}
  \end{equation}
  In the last inequality we used the inequality~$2ab\le a^2+b^2$. To estimate the first part of the right hand side of~\eqref{eq:proof_est1}, we can apply~\eqref{eq:nec_cond3} together with the boundedness of~$\{\bm{w}^k\}$ to obtain the existence of some~$C_6$ such that
  \begin{multline}\label{eq:proof_est3}
    \inner{ g(\bm{w}^{k}) -\nabla L(\bm{w}^{k}) + g(\bm{w}^{k+1})-\nabla L(\bm{w}^{k+1})}{\alpha^k(\bm{w}^* - \bm{w}^{k})} \\
      \le C_6(\alpha^{k-2})^2 + C_6(\alpha^{k-1})^2 + C_6(\alpha^{k})^2 + C_6(\alpha^{k+1})^2.
  \end{multline}
  Plugging~\eqref{eq:proof_est2} and~\eqref{eq:proof_est3} into~\eqref{eq:proof_est1} and summing the terms yields~\eqref{eq:nec_cond}. Then the assumptions of Theorem~\ref{thm:convergence} are verified and the theorem statement follows.
\end{proof}

\subsection{Auxiliary results}\label{app:sgd3}

\begin{lemma}\label{lemma:bound_zero}
  Let~$l$ satisfy~\ref{surr_basic1}-\ref{surr_ratio}. Then there exists some~$\hat{C} > 0$ such that for all~$k$ we have
  \begin{equation*}
    \begin{aligned}
      \hat{C} \le & \sum_{i \in \I} l'(s_i^k - t^k), \\
      \hat{C} \le & \sum_{i \in \I} l'(\bm{x}_i^\top \bm{w}^k - t(\bm{w}^k)).
    \end{aligned}
  \end{equation*}
\end{lemma}
\begin{proof}
  First, we will find an upper bound of~$s_i^k-t^k$. Fix any index~$i_0$. Since~$l$ is nonnegative due to~\ref{surr_basic1}, equation~\eqref{eq:update_t} implies
  \begin{equation*}
    n\tau = \sum_{i \in \I} l(s_i^k - t^k) \ge l(s_{i_0}^k - t^k).
  \end{equation*}
  Moreover, as~$l$ is a strictly increasing function due to~\ref{surr_basic2} and~$n\tau>0$, this means 
  \begin{equation}\label{eq:sigma_bound}
    l^{-1}(n\tau) \ge s_{i_0}^k-t^k.
  \end{equation}
  Since~$i_0$ was an arbitrary index, it holds true for all indices. Then~\ref{surr_ratio} which leads to a further estimate
  \begin{equation*}
    \begin{aligned}
    \sum_{i \in \I} l'(s_i^k - t^k)
      & = \sum_{i\in \I} l(s_i^k-t^k) \frac{l'(s_i^k-t^k)}{l(s_i^k-t^k)} \\
      & \ge \sum_{i \in \I} l(s_i^k - t^k) \frac{l'(l^{-1}(n\tau))}{l(l^{-1}(n\tau))} \\
      & = n\tau \frac{l'(l^{-1}(n\tau))}{l(l^{-1}(n\tau))} \\
      & = l'(l^{-1}(n\tau)),
    \end{aligned}
  \end{equation*}
  where the inequality follows from~\eqref{eq:sigma_bound} and the following equality from~\eqref{eq:update_t}. Due to~\ref{surr_basic2} we obtain that~$l'(l^{-1}(n\tau))$ is a positive number, which finishes the proof of the first part. The second part can be obtained in an identical way.
\end{proof}

\pagebreak

\begin{lemma}\label{lemma:bound_g}
  Let~$l$ satisfy~\ref{surr_basic1}-\ref{surr_der1}. Then there exists some~$B$ such that for all~$k$ we have
  \begin{equation*}
    \begin{aligned}
      \norm{\nabla L(\bm{w}^k)} & \le B, \\
      \norm{g(\bm{w}^k)} & \le B.
    \end{aligned}
  \end{equation*}
\end{lemma}
\begin{proof}
  Due to~\ref{surr_der1} the derivative~$l'$ is bounded by some~$\hat{B}$. Then Theorem~\ref{thm:derivative} and Lemma~\ref{lemma:bound_zero} imply
  \begin{equation*}
    \norm{\nabla t(\bm{w}^k)}
      \le \frac{\hat{B} \sum_{i \in \I} \norm{\bm{x}_i}}{\sum_{i \in \I} l'(\bm{x}_i^\top \bm{w} - t(\bm{w}))}
      \le \frac{\hat{B}}{\hat{C}} \sum_{i\in \I} \norm{\bm{x}_i},
  \end{equation*}
  which is independent of~$k$. Then~\eqref{eq:derivatives} and again the boundedness of~$l'$ imply the existence of some~$B$ such that~$\norm{\nabla L(\bm{w}^k)} \le B$ for all~$k$. The proof for~$g(\bm{w}^k)$ can be performed identically.
\end{proof}

\begin{lemma}\label{lemma:ratio}
  Consider uniformly bounded positive sequences~$c_1^k,$~$c_2^k,$~$d_1^k,$~$d_2^k,$~$\alpha^k$ and positive constants~$C_1$,~$C_2$ such that for all~$k$ we have
  \begin{equation*}
    \begin{aligned}
      \norm{c_1^k-c_2^k} & \le C_1\alpha^k, \quad &
      \norm{d_1^k-d_2^k} & \le C_1\alpha^k, \quad &
      d_1^k & \ge C_2, \quad &
      d_2^k & \ge C_2.
    \end{aligned}
  \end{equation*}
  If~$\alpha^k \to 0$, then there exists a constant~$C_3$ such that for all sufficiently large~$k$ we have
  \begin{equation*}
    \norm{\frac{c_1^k}{d_1^k} - \frac{c_2^k}{d_2^k}} \le C_3\alpha^k.
  \end{equation*}
\end{lemma}

\begin{proof}
  Since~$d_1^k$ and~$d_2^k$ are bounded away from zero and since~$\alpha^k \to 0$, we have
  \begin{equation*}
    \norm{\frac{c_1^k}{d_1^k} - \frac{c_2^k}{d_2^k}}
      \le \max\Brac[c]{\frac{c_1^k}{d_1^k} - \frac{c_1^k+C_1\alpha^k}{d_1^k-C_1\alpha^k}, \; \frac{c_1^k}{d_1^k} - \frac{c_1^k-C_1\alpha^k}{d_1^k+C_1\alpha^k}}.
  \end{equation*}
  The first term can be estimated as
  \begin{equation*}
    \norm{\frac{c_1^k}{d_1^k} - \frac{c_1^k+C_1\alpha^k}{d_1^k-C_1\alpha^k}}
    = \norm{\frac{(c_1^k+d_1^k)C_1\alpha^k}{d_1^k(d_1^k-C_1\alpha^k)}}
    \le \frac{(c_1^k+d_1^k)C_1\alpha^k}{C_2|d_1^k-C_1\alpha^k|}.
  \end{equation*}
  Since~$\alpha^k\to 0$ by assumption, for large~$k$ we have~$\norm{d_1^k-C_1\alpha^k}\ge \frac{1}{2}C_2$. Since the sequences are uniformly bounded, the statement follows.
\end{proof}

\pagebreak

\begin{lemma}\label{lemma:product}
  Consider scalars~$a_i,$~$c_i$ and vectors~$b_i,$~$d_i.$ If there is some~$\hat{C}$ such that~$\norm{a_i} \le \hat{C}$ and~$\norm{d_i} \le \hat{C}$, then
  \begin{equation*}
    \norm{\sum_{i=1}^n a_ib_i - \sum_{i=1}^n c_id_i}
      \le \hat{C}\sum_{i=1}^n \Brac{\norm{a_i-c_i} + \norm{b_i-d_i}}.
  \end{equation*}
\end{lemma}
\begin{proof}
  It is simple to verify
  \begin{equation*}
    \norm{\sum_{i=1}^n a_ib_i - \sum_{i=1}^n c_id_i} \le \sum_{i=1}^n \norm{d_i}\norm{a_i-c_i} + \sum_{i=1}^n \norm{a_i}\norm{b_i-d_i},
  \end{equation*}
  from which the statement follows.
\end{proof}