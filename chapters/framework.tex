\chapter{Binary Classiciation at the Top}\label{chap: framework}

In the previous chapter, we introduced the general formulation of the binary classification problem~\eqref{eq: Binary classification counts} and basic ways how to evaluate the performance of binary classification. Furthermore, we introduced three problems that are closely related to binary classification, but focus on specific performance criteria. Namely: accuracy at the top, ranking problems and problem of hypothesis testing. Since these problems are usually considered separately, we will show, that they have one important thing in common: all three problems can be formulated as a minimization of the number of misclassified samples below (or above) a certain threshold. Moreover, we provide a unified framework for their handling and present several classification problems falling into this framework. Specificaly, all mentioned problems falls into the following formulation
\begin{mini}{\bm{w}}{
  \lambda_1 \cdot \fp(\bm{s}, t) + \lambda_2 \cdot \fn(\bm{s}, t)
}{\label{eq: aatp counts}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad}{i \in \I}
  \addConstraint{t}{= G(\Brac[c]{\bm{s}_i; y_i}_{i \in \I}),}
\end{mini}
where function~$G \colon \R^n \times \{0, 1\}^n \to \R$ takes the scores and labels of all samples and computes the decision threshold. In other words, all mentioned problems can be formulated as a binary classification problem with special condition on the decision threshold. Then the problems only differ in the way they define the function~$G$. The important distinction from standard binary classification is that the decision threshold is no longer fixed (as in case of neural networks) or trained independently (as in case of SVM), but is a function of scores of all samples. Therefore, the minimization in the problem~\eqref{eq: aatp counts} is performed only with respect to the one variable~$\bm{w}$.

\begin{figure}[t]
  \centering
  \includegraphics[width = 0.95\linewidth]{images/surrogates.pdf}
  \caption{Surrogates}
  \label{fig: surrogates}
\end{figure}

The problem~\eqref{eq: aatp counts} is difficult to handle, since the objective function contains Iverson function~\eqref{eq: iverson} and therefore is discontinuous. The usual approach how to getting a continuous objective function is to employ a surrogate function to replace the Iverson function~\cite{li2014top, grill2016learning}. In the text below, the symbol~$l$ denotes any convex non-negative non-decreasing function with~$l(0) = 1$. As examples of such function we can mention the hinge loss function or the quadratic hinge loss functions defined as follows
\begin{equation}\label{eq: surrogates}
  \begin{aligned}
    l_{\text{hinge}}(s) & = \max\Brac[c]{0, 1 + \vartheta \cdot s}, \\
    l_{\text{quadratic}}(s) & = \Brac{\max\Brac[c]{0, 1 + \vartheta \cdot s}}^2,\\
  \end{aligned}
\end{equation}
where~$\vartheta > 0$ is a scaling parameter. Figure~\ref{fig: surrogates} compares Iverson function with hinge loss and quadratic hinge with different scaling parameters. If the scaling parameter is larger than 1, then the surrogate functions approximate the Iverson function better on the interval~$(-\infty, 0]$. However, the approximation on the interval~$[0, \infty)$ is worse. Even though the scaling parameter affects a lot the quality of the approximation, the usual choice is 1. Besides that, the surrogate function always provides an upper approximation of the Iverson function, i.e.~$l(s) \ge \Iverson{s \ge 0}$ for all~$s \in \R.$ 

To follow the notation from the previous chapter, we will define surrogate approximations of the counts defined in equation~\eqref{eq: confusion counts}. Using surrogate function~$l$ that fulfill the requirements mentioned above, the counts~\eqref{eq: confusion counts} may be approximated by their surrogate counterparts defined as follows
\begin{equation}\label{eq: confusion counts surrogate}
  \begin{aligned}
    \tps(\bm{s}, t) & = \sum_{i \in \I^+}l(s_i - t), & \qquad
    \fns(\bm{s}, t) & = \sum_{i \in \I^+}l(t - s_i), \\
    \tns(\bm{s}, t) & = \sum_{i \in \I^-}l(t - s_i), &
    \fps(\bm{s}, t) & = \sum_{i \in \I^-}l(s_i - t).
  \end{aligned}
\end{equation}
Since the surrogate function provides upper approximation of the Iverson function, the surrogate counts~\eqref{eq: confusion counts surrogate} provide upper approximations of the true counts~\eqref{eq: confusion counts}. Replacing the counts in~\eqref{eq: aatp counts} by their surrogate counterparts and adding a regularization results in
\begin{mini}{\bm{w}}{
  \frac{\lambda_0}{2} \norm{\bm{w}}^2 + \lambda_1 \cdot \fps(\bm{s}, t) + \lambda_2 \cdot \fns(\bm{s}, t)
  }{\label{eq: aatp surrogate}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{= G(\Brac[c]{\bm{s}_i; y_i}_{i \in \I}).}
\end{mini}
The regularization is not necessary, but it is usually added for better numerical stability. The resulting optimization problem is easier to solve than problem~\eqref{eq: aatp counts} since the objective function is continuous. To derive other theoretical properties, we need to know the form of the function~$G$ as well as the form of the used classifier~$f.$ Different forms of the function~$G$ will be discussed in the rest of this chapter and different forms of classifier~$f$ will be discussed in Chapters~\ref{chap: linear} - \ref{chap: deep}. 

In the rest of the chapter, we will show, that accuracy at the top, ranking problems and the problem of hypothesis testing can be formulated in such a way, that fall  into our general framework of~\eqref{eq: aatp counts} and~\eqref{eq: aatp surrogate}. Most of the problems that we will be referring to are originally defined for the linear classifier only, i.e. for~$f(\bm{x}; \bm{w}) = \bm{w}^{\top} \bm{x}.$ However, we will formulate all the problems for general classifier~$f$ and then in Chapters~\ref{chap: linear} - \ref{chap: deep} we will discuss their properties based on the used classifier.  

To make the presentation as simple as possible, we first define a special notation. Section~\ref{sec: general formulation}, we defined vector~$\bm{s} \in \R^n$ of scores of all samples with components defined as
\begin{equation}\label{eq: scores}
  s_i = f(\bm{x}_i; \bm{w}), \quad i \in \I.
\end{equation}
To simplify the upcoming sections, let us define sorted version of the vector of scores~$\bm{s}_{[\cdot]}$ with decreasing components, i.e.
\begin{equation*}
  s_{[1]}   \ge s_{[2]} \ge \dots \ge s_{[n - 1]} \ge s_{[n]}.
\end{equation*}
Similarly, let us define vectors~$\bm{s}^- \in \R^{n^-},$ $\bm{s}^+ \in \R^{n^+}$ of scores of all positive and negative samples with components defined as
\begin{equation}\label{eq: scores neg/pos}
  \begin{aligned}
    s^-_i & = f(\bm{x}_i; \bm{w}), \quad i \in \I^-, \\
    s^+_i & = f(\bm{x}_i; \bm{w}), \quad i \in \I^+,
  \end{aligned}
\end{equation}
and their sorted versions~$\bm{s}^-_{[\cdot]}$, $\bm{s}^+_{[\cdot]}$ with decreasing components.

\section{Methods based on pushing positives to the top}\label{sec:obj1}

The first category of formulations falling into our framework~\eqref{eq: aatp counts} and~\eqref{eq: aatp surrogate} are ranking methods which attempt to put as many positive (relevant) samples to the top as possible. The number of positive samples on top equals to the number of positive samples with classification score the highest classicication score corresponding to negative sample. This amounts to maximizing the number of true-positive samples or, equivalently, minimizing the number of false-negative negative samples, which may be written as
\begin{mini}{\bm{w}}{
  \frac{1}{n^+} \fn(\bm{s}, t)
  }{\label{eq: toppush}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{= s_{[1]}^-}
\end{mini}
Since the threshold~$t$ is a function of score, the problem~\eqref{eq: toppush} is a special case of~\eqref{eq: aatp counts} for~$\lambda_1 = 0$ and~$\lambda_2 = \frac{1}{n^+}$. The authors in~\cite{li2014top} proposed an efficient method to solve the formulation above and called it \TopPush. They replaced the number of false-negative samples with its surrogate counterpart in the objective function of~\eqref{eq: toppush} and added the regularization term to arrive at
\begin{mini}{\bm{w}}{
  \frac{\lambda_0}{2} \norm{\bm{w}}^2 + \frac{1}{n^+} \fns(\bm{s}, t)
  }{\label{eq: toppush surrogate}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{= s_{[1]}^-}
\end{mini}
Similarly to the original problem, this surrogate approximation falls into our framework of~\eqref{eq: aatp surrogate} for~$\lambda_1 = 0$ and~$\lambda_2 = \frac{1}{n^+}$. However, \TopPush can be very sensitive to outliers. Especially when the linear classifier is used, as shown in Section~\ref{sec: stability}. To robustify \TopPush method, we follow the idea from~\cite{lapin2015top} and propose to replace the largest negative score by the mean of~$K$ largest negative scores. We call this approach \TopPushK and the resulting formulation is as follows
\begin{mini}{\bm{w}}{
  \frac{\lambda_0}{2} \norm{\bm{w}}^2 + \frac{1}{n^+} \fns(\bm{s}, t)
  }{\label{eq: toppushK surrogate}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{= \sum_{i = 1}^{K} s_{[i]}^-}
\end{mini}
We used the mean of highest~$K$ negative scores instead of the value of the~$K$-th negative score to preserve convexity as shown in Section~\ref{sec:convexity}. 

\section{Accuracy at the Top}\label{sec: aatp}

The previous category considers formulations which minimize the number of positive samples below the highest-ranked negative sample, i.e. minimize the number of false-negative samples. Accuracy at the Top~\cite{boyd2012accuracy} takes a different approach and minimizes the number of negative samples with the score above the top~$\tau$-quantile of all scores defined by
\begin{equation}\label{eq: aatp quantile} 
  t_1(\bm{s}) = \max \Set{t}{\tp(\bm{s}, t) + \fp(\bm{s}, t) \ge n \tau}.
\end{equation}
Then the Accuracy at the Top problem is defined by
\begin{mini}{\bm{w}}{
  \frac{1}{n^-} \fp(\bm{s}, t)
  }{\label{eq: aatp original}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{\text{ is the top } \tau\text{-quantile: it solves~\eqref{eq: aatp quantile}.}}
\end{mini}
This formulation already falls into our framework~~\eqref{eq: aatp counts} for~$\lambda_1 = \frac{1}{n^-}$ and~$\lambda_2 = 0$. However, this formulation can be rewritten using the following lemma.
\begin{restatable}{lemma}{lemmaequivalence}\label{lemma:fnfp_equivalence}
  Denote by~$t$ the exact quantile from~\eqref{eq: aatp quantile}. Then for all~$\mu \in [0,1]$ we have
  \begin{equation}\label{eq: fn fp equivalence}
      \fp(\bm{s},t) = \mu \fp(\bm{s},t) + (1-\mu)\fn(\bm{s},t) + (1-\mu)(n\tau - n^+) + (1-\mu)(q - 1),
  \end{equation}
  where~$q$ represents the number of samples which score is equal to~$t$
  \begin{equation*}
      q := \# \Set{i \in \I}{ \bm{s}_i = t}.
  \end{equation*}
\end{restatable}
\noindent The right-hand side of~\eqref{eq: fn fp equivalence} consists of three parts. The first one is a convex combination of false-positive and false-negative counts. The second one is a constant term which has no impact on optimization. Finally, the third term~$(1-\mu)\Brac{q - 1}$ equals the number of samples for which their score equals the quantile. However, this term is small in comparison with the true-positive and the false-negative counts and can be neglected. Moreover, when the data are ``truly'' random such as when measurement errors are present, then~$q = 1$ and this term vanishes completely. Altogether, we get the following (almost) equivalent formulation to the problem~\eqref{eq: aatp original}
\begin{mini}{\bm{w}}{
  \mu \fp(\bm{s},t) + (1 - \mu)\fn(\bm{s},t)
  }{\label{eq: aatp original combination}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{\text{ is the top } \tau\text{-quantile: it solves~\eqref{eq: aatp quantile},}}
\end{mini}
where~$\mu \in [0,1]$. This problem with~$\mu = 0$ equals to~\eqref{eq: aatp original}, while with~$\mu = \frac{n^-}{n}$ it corresponds to the original definition (without regularization) from~\cite{boyd2012accuracy}.

Paper~\cite{grill2016learning} builds on the Accuracy at the Top problem~\eqref{eq: aatp original combination}, where it replaces false-positive and false-negative counts in the objective function by their surrogate counterparts. This leads to
\begin{mini}{\bm{w}}{
  \frac{\lambda_0}{2} \norm{\bm{w}}^2 + \frac{1}{n^{-}}\fps(\bm{s}, t) + \frac{1}{n^{+}} \fns(\bm{s}, t)
  }{\label{eq: grill}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{\text{ is the top } \tau\text{-quantile: it solves~\eqref{eq: aatp quantile},}}
\end{mini}
Based on the first author, we name this formulation \Grill. This formulation falls into our framework~\eqref{eq: aatp surrogate} for~$\lambda_1 = \frac{1}{n^-}$ and~$\lambda_2 = \frac{1}{n^+}$.

Apart from the quantile~\eqref{eq: aatp quantile}, there are two other possible choices of the threshold. The first on is simple approximation of the the true quantile by mean of the~$n\tau$ largest scores 
\begin{equation}\label{eq: aatp quantile mean} 
  t_2(\bm{s}) = \frac{1}{n\tau} \sum_{i=1}^{n\tau} s_{[i]}.
\end{equation}
For simplicity, for the rest of the paper we assume, that~$n\tau$ is an integer. The quantile~\eqref{eq: aatp quantile} is sometimes denoted as VaR (value at risk) and~\eqref{eq: aatp quantile mean} as CVaR (conditional value of risk).The main purpose of~\eqref{eq: aatp quantile mean} is to provide a convex approximation of the non-convex quantile~\eqref{eq: aatp quantile}. In fact, it is known is that it is the tightest convex approximation of~\eqref{eq: aatp quantile}. Putting~\eqref{eq: aatp quantile mean} into the constraint results in the following problem, which we call \TopMeanK
\begin{mini}{\bm{w}}{
  \frac{\lambda_0}{2} \norm{\bm{w}}^2 + \frac{1}{n^{+}} \fns(\bm{s}, t)
  }{\label{eq: topmeank}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{= \frac{1}{K} \sum_{i=1}^{K} s_{[i]},}
\end{mini}
where~$K = n\tau.$ Note that this formulation is very similar to the \TopPushK formulation from the previous section. The only difference is, that the threshold is computed from scores of all samples and not only from the negative ones as for \TopPushK. 

The second option for the threshold, is to use surrogate counterparts to replace counts in~\eqref{eq: aatp quantile} and solve the following equality
\begin{equation}\label{eq: aatp quantile surrogate}
  t_3(\bm{s}) \quad \text{solves} \quad \frac{1}{n}\sum_{i \in \I} l(s_i - t) = \tau. 
\end{equation}
Since this threshold uses the surrogate approximation, we will sometimes denote~\eqref{eq: aatp quantile surrogate} as surrogate top~$\tau$-quantile. Similarly to the previous case, we can use the top~$\tau$-quantile in the constraint to arrive at
\begin{mini}{\bm{w}}{
  \frac{\lambda_0}{2} \norm{\bm{w}}^2 + \frac{1}{n^{+}} \fns(\bm{s}, t)
  }{\label{eq: patmat}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I}
  \addConstraint{t}{\text{ is the surrogate top } \tau\text{-quantile: it solves~\eqref{eq: aatp quantile surrogate}.}}{~}
\end{mini}
Note that \Grill minimizes the convex combination of false-positives and false-negatives while~\eqref{eq: topmeank} and~\eqref{eq: patmat} minimize only the false-negatives. The reason for this will be evident in Section~\ref{sec:convexity} and amounts to preservation of convexity. Moreover, as will see later, problem~\eqref{eq: patmat} provides a good approximation to the Accuracy at the Top problem and for linear classifier is easily solvable due to convexity and requires almost no tuning, we named it \PatMat (Precision At the Top \& Mostly Automated Tuning). 

The following proposition compares the thresholds defined in this section in terms of approximation quality.
\begin{proposition}[Thresholds relation~\cite{zhang2018tau}]\label{prop: threholds}
  We always have
  \begin{equation*}
    t_1(\bm{s}) \le t_2(\bm{s}) \le t_3(\bm{s}).
  \end{equation*}
\end{proposition}
Whenever the objective contains only false-negatives, a lower threshold~$t$ means a lower objective function. Therefore, a lower threshold is preferred.

\section{Methods optimizing the Neyman-Pearson criterion}\label{sec: Neyman-Pearson}

Another category falling into the framework of~\eqref{eq: aatp counts} and~\eqref{eq: aatp surrogate} is the Neyman-Pearson problem which is closely related to hypothesis testing, where null~$H_0$ and alternative~$H_1$ hypotheses are given. Type~I error occurs when~$H_0$ is true but is rejected, and type II error happens when~$H_0$ is false, but it fails to be rejected. The standard technique is to minimize Type II error while a bound for Type I error is given.

In the Neyman-Pearson problem, the null hypothesis~$H_0$ states that a sample~$\bm{x}$ has the negative label. Then Type I error corresponds to false-positives while Type II error to false-negatives. If the bound on Type I error equals~$\tau$, we may write this as
\begin{equation}\label{eq:defin_quantile_np} 
  t_1^{\rm NP}(\bm{w}) = \max\Brac[c]{t \mid \fp(\bm{w},t) \ge n^- \tau}.
\end{equation}
Then, we may write the Neyman-Pearson problem as
\begin{equation}\label{eq:problem_np}
  \begin{aligned}
    \minimize{}
    & \quad \frac{1}{n^{+}}\fn(\bm{w},t) \\
    \st
    & \quad t \text{ is Type I error at level \ensuremath{\tau}: it solves }\eqref{eq:defin_quantile_np}.
  \end{aligned}
\end{equation}
Since~\eqref{eq:problem_np} differs from~\eqref{eq: aatp original combination} only by counting only the false-positives in~\eqref{eq:defin_quantile_np} instead of counting all positives in~\eqref{eq: aatp quantile}, we can derive its approximations in exactly the same way as in Section~\ref{sec: aatp}. We therefore provide only their brief description and start with approximations of~\eqref{eq:defin_quantile_np}
\begin{align}
  \label{eq:defin_quantile1_np} t_2^{\rm NP}(\bm{w}) =\ &\frac{1}{n^-\tau}\sum_{i=1}^{n^-\tau} s_{[i]}^-, \\
  \label{eq:defin_quantile0_np} t_3^{\rm NP}(\bm{w})\quad \text{solves} \quad &\frac{1}{n}\sum_{i=1}^{n^-}l(\beta(s_i^- - t)) = \tau.
\end{align}
Replacing the true counts by their surrogates results in the Neyman-Pearson variant \GrillNP
\begin{equation}\label{eq:problem_grill_np}
  \begin{aligned}
  \minimize{}
  & \quad \frac{1}{n^{+}}\fns(\bm{w}, t) + \frac{1}{n^{-}}\fps(\bm{w}, t) + \frac{\lambda}{2}\norm{\bm{w}}^2\\
  \st
  & \quad t\text{ is the Neyman-Pearson threshold: it solves }\eqref{eq:defin_quantile_np}.
  \end{aligned}
\end{equation}
Similarly, the Neyman-Pearson alternative to \TopMeanK reads
\begin{equation}\label{eq:problem_topmeank_np}
  \begin{aligned}
  \minimize{}
  & \quad\frac{1}{n^{+}}\fns(\bm{w}, t) + \frac{\lambda}{2}\norm{\bm{w}}^2 \\
  \st
  & \quad t = \frac 1{n^ - \tau}(s_{[1]}^- + \dots + s_{[n^- \tau]}^-), \\
  & \quad \text{components of }\bm{s}^-\text{ equal to } s^- = \bm{w}^\top \bm{x}^-\text{ for }\bm{x}^- \in \Xc.
  \end{aligned}
\end{equation}
This problem already appeared in~\cite{zhang2018tau} under the name \tauFPL. Finally, \PatMatNP reads
\begin{equation}\label{eq:problem_patmat_np}
  \begin{aligned}
  \minimize{}
  & \quad \frac{1}{n^{+}}\fns(\bm{w},t) + \frac{\lambda}{2}\norm{\bm{w}}^2\\
  \st
  & \quad t\text{ is the surrogate Neyman-Pearson threshold: it solves }\eqref{eq:defin_quantile0_np}.
  \end{aligned}
\end{equation}

We may see~\eqref{eq:problem_topmeank_np} from two different viewpoints. First, \tauFPL provide convex approximations of \GrillNP. Second, \tauFPL has the same form as \TopPushK. The only difference is that for \tauFPL we have~$k=n^-\tau$ while for \TopPushK the value of~$k$ is small. Thus, even though we started from two different problems, we arrived at two approximations which differ only in the value of one parameter. This shows a close relation of the ranking problem and the Neyman-Pearson problem and the need for a unified theory to handle these problems.

\section{Summary}

\begin{table}
  \centering
  \begin{NiceTabular}{lcccc}
    \toprule
    \textbf{Name} & \textbf{Source} & $\lambda_1$ & $\lambda_2$ & \textbf{Threshold} \\
    \midrule
    \TopPush &  & 0 & $\frac{1}{n^+}$ & $s_{[1]}^-$ \\
    \TopPushK & ours & 0 & $\frac{1}{n^+}$ & $\sum_{i = 1}^{K} s_{[i]}^-$\\
    \midrule
    \PatMat & & 0 & $\frac{1}{n^+}$ &\\
    \PatMatNP & & 0 & $\frac{1}{n^+}$ & \\
    \bottomrule
  \end{NiceTabular}
  \caption{Summary of ...}
  \label{tab: aatp summary}
\end{table}