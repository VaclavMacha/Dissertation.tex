\chapter{Numerical Experiments}\label{chap: experiments}

In the previous sections, we derived general framework for classification at the top and showed that multiple well-known formulations fall into it. The summary of all formulations presented in this work is in Table~\ref{tab: summary formulations}. The goal of this chapter is to experimentally verify the properties of these formulations. 

\section{Settings}\label{sec: settings}

In this section we describe in detail all settings used for experiments. Formulations from Table~\ref{tab: summary formulations} can be divided into three categories:
\begin{itemize}
  \item The first category contains \TopPush and \TopPushK formulations. Formulations from this category minimize the surrogate approximation of the false-negative rate. As a threshold, these formulations use the mean of a small fraction of the negative samples with the highest scores.
  \item The second category consists of \Grill, \TopMeanK, and \PatMat formulation. These three formulations again use the surrogate approximation of the false-negative rate as an objective function. The only exception is the \Grill formulation also adds the surrogate approximation of the false-positive rate into the objective function for better stability. All three formulation uses some kind of approximation of the top $\tau$-quantile of all scores as a threshold.
  \item The last category consists of \GrillNP, \tauFPL, and \PatMatNP. These formulations use the same objectives as their corresponding formulations from the previous category and differ only in the definition of the decision threshold. All three formulation uses some kind of approximation of the top $\tau$-quantile of negative scores as a threshold.
\end{itemize}
To simplify the setup of all experiments, we decided to focus on formulations that use only negative samples for the threshold computation, i.e. formulations from the first and third categories. The performance of these formulations can be easily compared using some basic performance metrics as we show later in Section~\ref{sec: performance criteria}.

\subsection{Formulations and Hyperparameters}

At the beginning of this section, we discussed which formulations we use for experiments. In total, we use five different formulations from Table~\ref{tab: summary formulations}, namely \TopPush, \TopPushK, \GrillNP, \tauFPL, and \PatMatNP. Moreover, for \TopPushK we use two different values of~$K = \{5, 10\}$ and consider the resulting formulations as separate formulations, i.e. we have \TopPushK(5) and \TopPushK(10). Similarly, for \GrillNP, \tauFPL and \PatMat we use two different values of~$\tau = \{0.01, 0.05\}.$ For all formulations, we use the hinge loss defined in Notation~\ref{not: surrogates} as a surrogate function.

We have a total of 9 different formulations, however, all these formulations fall into our general framework. To show that these formulations work properly, we have to compare them to some standard methods. In previous chapters, we showed how to solve presented formulations in their primal (Chapters~\ref{chap: linear} and~\ref{chap: deep}) and dual form (Chapter~\ref{chap: dual}). Whenever we use the primal form in the experiments, we use binary cross-entropy defined in the following way as a baseline formulation 
\begin{mini}{\bm{w}}{
  \frac{1}{\nall} \sum_{i \in \I} \Brac{- y_i \log(s_i) - (1 - y_i) \log (1 - s_i)}
  }{\label{eq: crossentropy}}{}
  \addConstraint{s_i}{= f(\bm{x}_i; \bm{w}), \quad i \in \I.}
\end{mini}
We decided to use binary cross-entropy, since it is one of the most used objective functions for binary classification in machine learning applications. In the following text, we will denote binary cross-entropy as \BaseLine for simplicity. In experiments with dual forms, we use C-SVC variant of SVM~\cite{boser1992training, cortes1995support,chang2011libsvm} defined as follows
\begin{mini}{\bm{w}, b, \bm{\xi}}{
  \frac{1}{2} \norm{\bm{w}}^2 + C \sum_{i \in \I} \xi_i
  }{\label{eq: SVM}}{}
  \addConstraint{y_i}{\Brac{\bm{w}^{\top} \phi(\bm{x}_i) + b} \geq 1 - \xi_i, \quad i \in \I}
  \addConstraint{\xi_i}{\geq 0, \quad i \in \I,}
\end{mini}
where~$y_i \in \{-1, 1\}$ for all~$i \in \I$ and~$\phi(\bm{x}_i)$ maps~$\bm{x}_i$ into a higher-dimensional space (see Section~\ref{sec: kernels}). The corresponding dual form is as follows
\begin{maxi}{\bm{\alpha}}{
  - \frac{1}{2} \bm{\alpha}^{\top} \K \bm{\alpha} - \sum_{i = 1}^{\nall} \alpha_i
  }{\label{eq: SVM dual}}{}
  \addConstraint{\sum_{i = 1}^{\nall} y_i \alpha_i}{= 0}
  \addConstraint{0 \leq \alpha_i }{\leq C, \quad i = 1, 2, \ldots, \nall,}
\end{maxi}
where the kernel matrix~$\K$ is defined as
\begin{equation*}
  \K_{i,j} = y_i y_j k(\bm{x}_i, \bm{x}_j) = \phi(\bm{x}_i)^{\top} \phi(\bm{x}_j),
\end{equation*}
for all~$i, j = 1, 2, \ldots, \nall.$ Note that the dual form of C-SVC is very similar to the dual forms of our formulations derived in Chapter~\ref{chap: dual}. In the following text, we will denote C-SVC as \SVM for simplicity.

In total, we have 11 different formulations. However, not all formulations are used for all experiments. For example \GrillNP formulations are not used for experiments with dual forms, since the primal problem is not convex and therefore we did not derive the dual form. Moreover, \BaseLine formulation is also not used for experiments with dual forms. On the other hand, \SVM is used only for experiments with dual forms. The summary of all formulations used for experiments is in Table~\ref{tab: formulations experiments summary}.

Since considered formulations differ in the number of available hyper-parameters, we decided to fix the number of hyper-parameters per formulation to six. For most of the considered formulations, the remaining hyper-parameter is the regularization constant~$\lambda$. The only exceptions are the formulations derived from \PatMatNP, since they also have the scaling parameter~$\vartheta.$ Therefore, we use the following six values of this hyper-parameter
\begin{equation*}
  \lambda \in \Brac[c]{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1}
\end{equation*}
for all formulations except \PatMatNP. Since we used a slightly different (but equivalent) primal formulation for the derivation of the dual forms, we use~$\lambda$ to compute hyper-parameter~$C$ used in dual forms
\begin{equation*}
  C = \frac{1}{\lambda \ntil},
\end{equation*}
where~$\ntil = \nall$ for \SVM and~$\ntil = \npos$ otherwise. For formulations derived from \PatMatNP, we fixed~$\lambda$ to~$10^{-3}$ and use the following six different values of the scaling parameter
\begin{equation*}
  \vartheta \in \Brac[c]{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1}.
\end{equation*}
In all experiments, the best hyperparameter is selected based on the validation data and the appropriate performance metric.

\begin{table}[!ht]
  \centering
  \begin{NiceTabular}{lcccc}
    \CodeBefore
    \rowcolor{\headercol}{1}
    \rowcolors{3}{\rowcol}{}[restart]
    \Body
    \toprule
    \textbf{Formulation}
      & \textbf{Fixed parameters}
      & \textbf{Hyper-parameter}
      & \textbf{Primal Form}
      & \textbf{Dual Form} \\
    \midrule
    \BaseLine
      & ---
      & $\lambda$
      & \yesmark
      & \nomark \\
    \SVM
      & ---
      & $\lambda$
      & \nomark 
      & \yesmark \\
    \midrule
    \TopPush
      & ---
      & $\lambda$
      & \yesmark
      & \yesmark \\
    \TopPushK(5)
      & $K = 5$
      & $\lambda$
      & \yesmark
      & \yesmark \\
    \TopPushK(10)
      & $K = 10$
      & $\lambda$
      & \yesmark
      & \yesmark \\
    \GrillNP(0.01)
      & $\tau = 0.01$
      & $\lambda$
      & \yesmark
      & \nomark \\
    \GrillNP(0.05)
      & $\tau = 0.05$
      & $\lambda$
      & \yesmark
      & \nomark \\
    \tauFPL(0.01)
      & $\tau = 0.01$
      & $\lambda$
      & \yesmark
      & \yesmark \\
    \tauFPL(0.05)
      & $\tau = 0.05$
      & $\lambda$
      & \yesmark
      & \yesmark \\
    \PatMatNP(0.01)
      & $\tau = 0.01,$ $\lambda = 0.001$
      & $\vartheta$
      & \yesmark
      & \yesmark \\
    \PatMatNP(0.05)
      & $\tau = 0.05,$ $\lambda = 0.001$
      & $\vartheta$
      & \yesmark
      & \yesmark \\
    \bottomrule
  \end{NiceTabular}
  \caption{Summary of all formulations used for experiments. The first column shows the aliases used for the formulations when describing the experiment results. The second column shows fixed hyperparameters used for each formulation, while the third column shows which hyper-parameters are tuned using validation data. The last two columns indicate whether the formulation is used in primal experiments, dual experiments, or both.}
  \label{tab: formulations experiments summary}
\end{table}

\subsection{Performance Criteria}\label{sec: performance criteria}

In the previous subsections, we described all formulations used for the experiments. In this section, we describe which performance criteria are used for evaluation and how these criteria are related to the tested formulations.

As we discussed at the beginning of Section~\ref{sec: settings}, we decided to test only formulations that minimize the false-negative rate (or a combination of false-negative and false-positive rate) and use only negative samples for the threshold computation. This choice allows us to use simple metrics to compare used formulations. The first metric that we use in experiments is~$\tpratk$ defined as follows
\begin{equation*}
  \tpratk = \frac{1}{\npos} \sum_{i \in \Ipos} \Iverson{s_i \geq t} \quad \text{where} \quad t = \sum_{j = 1}^{K} s^{-}_{[j]}.
\end{equation*}
This metric computes the true-positive rate at threshold~$t$ which is the mean of $K$-largest negative scores. For~$K = 1$ the threshold corresponds to the threshold used by \TopPush formulation, and otherwise threshold~$t$ corresponds to the threshold used by \TopPushK. Moreover, since minimizing the false-negative rate is equivalent to maximizing the true-positive rate, both \TopPush and \TopPushK should optimize the $\tpratk$ metric. In the upcoming experiments, we use this metric with three different values of~$K \in \{1, 5, 10\}.$

The second metric is defined in a similar way
\begin{equation*}
  \tpratfpr = \frac{1}{\npos} \sum_{i \in \Ipos} \Iverson{s_i \geq t} \quad \text{where} \quad t
  = \max \Set{t}{\frac{1}{\nneg} \sum_{i \in \Ineg} \Iverson{s_i \geq t} \geq \tau}.
\end{equation*}
This metric computes the true-positive rate at a specific top $\tau$-quantile of negative scores. This metric is ideal for testing the performance of \Grill, \tauFPL and \PatMatNP, since all three formulations maximize true-positive rate and use some kind of approximation of the true top $\tau$-quantile of negative scores as a threshold. In experiments, we use this metric with two different values of~$\tau \in \{0.01, 0.05\}.$ 

The two previous metric are specific for the formulations from our framework. However, we should also test if the baseline formulations work properly. Since the baseline methods are designed to optimize overall perfomance, we use area under ROC curve to measure the overall performance. The summary of all used metrics is in Table~\ref{tab: metrics summary}.

\begin{table}[!ht]
  \centering
  \begin{NiceTabular}{lcccccc}
    \CodeBefore
    \rowcolor{\headercol}{1-2}
    \rowcolors{4}{\rowcol}{}[restart]
    \Body
    \toprule
    \Block[c]{2-1}{\textbf{Formulation}}
      & \Block[c]{2-1}{$\auroc$}
      & \Block[c]{1-3}{$\tpratk$}
      &&& \Block[c]{1-2}{$\tpratfpr$} \\
    \cline{3-7}
      && $1$  
      & $5$
      & $10$
      & $0.01$
      & $0.05$ \\
    \midrule
    \BaseLine
      & \yesmark
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \nomark \\
    \SVM
      & \yesmark
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \nomark \\
    \midrule
    \TopPush
      & \nomark
      & \yesmark
      & \nomark
      & \nomark
      & \nomark
      & \nomark \\
    \TopPushK(5)
      & \nomark
      & \nomark
      & \yesmark
      & \nomark
      & \nomark
      & \nomark \\
    \TopPushK(10)
      & \nomark
      & \nomark
      & \nomark
      & \yesmark
      & \nomark
      & \nomark \\
    \GrillNP(0.01)
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \yesmark
      & \nomark \\
    \GrillNP(0.05)
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \yesmark \\
    \tauFPL(0.01)
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \yesmark
      & \nomark \\
    \tauFPL(0.05)
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \yesmark \\
    \PatMatNP(0.01)
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \yesmark
      & \nomark \\
    \PatMatNP(0.05)
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \nomark
      & \yesmark \\
    \bottomrule
  \end{NiceTabular}
  \caption{The summary of all used perofmance metrics used for evaluation. In total we use six different metrics and eleven different formulations. For each formulation~\yesmark denotes the metric in which the formulation should be the best.}
  \label{tab: metrics summary}
\end{table}

\newpage

\subsection{Datasets Description}

For the numerical experiments, we consider variety of different datasets summarized in Table~\ref{tab: datasets summary}, that can be divided into three categories. In the following subsections, we describe in details all three categories. It is worth mentioning, that not all datasets that are used in experiments are primarily designed for the classification at the top. In fact, all datasets from the first category are general image classification datasets. We use these datasets for two reasons. The first one is that they are publicly available. The second reason is, that all these datasets are well known, and therefore it is easier to present the results on them. However, there are also some drawbacks. For example, many state-of-the-art neural network architectures achieve almost the perfect classification on these datasets and therefore there is no room for improvement. For this reason, we use much simpler architectures in the experiments to show the behavior of formulations presented in this work.

\subsubsection{Visual recognition datasets}

This category contains six well-known visual recognition datasets. MNIST \cite{deng2012mnist} and FashionMNIST \cite{xiao2017fashionmnist} are grayscale datasets of digits and fashion items, respectively. CIFAR100~\cite{krizhevsky2009learning} is a dataset of coloured images of different items grouped into 100 classes. CIFAR10 and CIFAR20 merge these classes into 10 and 20 superclasses, respectively. Finally, SVHN2~\cite{netzer2011reading} contains coloured images of house numbers. All datasets from this category are publicly available.

All dataset in this category are originally divided only into training and test sets. Therefore, we select 25\% samples from the training set to obtain the validation set. 

\subsubsection{Steganography datasets}

To show the importance of classification at the top, we need to find the field in which the maximizing true-positive rate at the low false-positive rate is an important task. Such a field can be for example steganography and steganalysis. The standard way how to share secret information these days is the use of encryption. However, in such a case, the presence of a secret message (even though encrypted) is obvious. Steganography aims to hide the fact that communication taking place, by hiding the secret message within an ordinary file (usually called cover file) in order to avoid detection. The secret message is then extracted at its destination. The secret data can be hidden in almost any type of digital content, however the most popular are images. There are two reasons for this. The first of them is the ubiquity of images on the Internet and therefore the ease to use them as cover files for secret messages. The second reason is their large potential payload, i.e. it is possible to hide a lot of information in the images with high resolution. With an appropriate cover image and steganography tools, it is possible to create an stego-image (image with a hidden message) that can not be recognized from the cover image by human perception. However, each tool leaves a fingerprint or signature in the image, that can be used to detect stego images. The field that tries to detect stego images and possibly decrypt messages from them is called steganalysis. In steganalysis, the goal is to achieve the best true-positive rate with the lowest possible false-positive rate. Therefore steganalysis is the branch suitable for the classification at the top, since many of the formulations derived in this work focus precisely on this task. \cite{morkel2005overview, silman2001steganography} 

For the eperiments, we have a large dataset of cover-images that consists of approximatel 450 000 images from Flickr. All these images are in the JPEG format with quality factor 80. Since the dataset does not contain any stego images, we use two different ways to generate them:
\begin{itemize}
  \item \textbf{Nsf5:} Stego images were generated using simulated F5 with matrix embedding turned off. All images were described using 22 500 features and split into train / validation / test set in ratio 0.45 / 0.05 / 0.5.
  \item  \textbf{JMiPOD:} We first select all images that can be cropped to to size $256 \times 256 \times 3$ and than cropped them looselesly using \emph{jpegtran} library. Than, we use JMiPOD~\cite{cogranne2020steganography} algorithm to generate stego images. We split the data into train / validation / test set in ratio 0.375 / 0.125 / 0.5.
\end{itemize}
Since we are interested in low false-positive rates, we need a lot of negative (cover) samples to estimate it. This is evident in~\eqref{eq: patmat np} where the threshold~$t$ is a surrogate approximation of false-positive rate, i.e., the threshold is computed only from negative samples. Positive (stego) samples occurs only in the objective function. Since generating of stego images is expensive and we do not need them to estimate false-positive rate, we decided to use 10\% of all cover images to generate their stego counterparts. The resulting sizes of train / validation / test splits as well as the number of stego images in them, are in Table~\ref{tab: datasets summary}.

\subsubsection{Malware detection dataset}

A renowned cybersecurity company provided malware analysis reports of executable files. This is an extremely tough dataset as individual samples are JSON files whose size ranges from 1kB to 2.5MB. Its structure is highly complicated because each sample has a different number of features, and features may have a complicated structure, such as a list of ports to which the file connects. This is in sharp contrast with standard datasets, where each sample has the same number of features, and each feature is a real number.

\begin{table}[!ht]
  \centering
  \resizebox{\columnwidth}{!}{%
    \begin{NiceTabular}{lccrrrrrr}
      \CodeBefore
      \rowcolor{\headercol}{1-2}
      \rowcolors{4}{\rowcol}{}[restart]
      \Body
      \toprule
      \Block[c]{2-1}{\textbf{Dataset}}
      & \Block[c]{2-1}{$y^+$}
      & \Block[c]{2-1}{$d$}
      & \Block[c]{1-2}{\textbf{Train}}
      && \Block[c]{1-2}{\textbf{Validation}}
      && \Block[c]{1-2}{\textbf{Test}} \\
      \cline{4-9}
      &&& \Block[c]{1-1}{$n$}
      & \Block[c]{1-1}{$\frac{\npos}{n}$}
      & \Block[c]{1-1}{$n$}
      & \Block[c]{1-1}{$\frac{\npos}{n}$}
      & \Block[c]{1-1}{$n$}
      & \Block[c]{1-1}{$\frac{\npos}{n}$} \\
      \midrule
      MNIST
      & 1
      & $28 \times 28 \times 1$
      & 45 000
      & 11.3\%
      & 15 000
      & 11.2\%
      & 10 000
      & 11.4\% \\
      FashionMNIST
      & 1
      & $28 \times 28\times 1$
      & 45 000
      & 10.0\%
      & 15 000
      & 9.9\%
      & 10 000
      & 10.0\% \\
      CIFAR10
      & 1
      & $32 \times 32 \times 3$
      & 37 500
      & 10.0\%
      & 12 500
      & 9.9\%
      & 10 000
      & 10.0\% \\
      CIFAR20
      & 1
      & $32 \times 32 \times 3$
      & 37 500
      & 5.0\%
      & 12 500
      & 5.1\%
      & 10 000
      & 5.0\% \\
      CIFAR100
      & 1
      & $32 \times 32 \times 3$
      & 37 500
      & 1.0\%
      & 12 500
      & 1.0\%
      & 10 000
      & 1.0\% \\
      SVHN2
      & 1
      & $32 \times 32\times 3$
      & 54 944
      & 18.9\%
      & 18 313
      & 18.9\%
      & 26 032
      & 19.6\% \\
      SVHN2-Extra
      & 1
      & $32 \times 32\times 3$
      & 453 291
      & 17.3\%
      & 151 097
      & 17.1\%
      & 26 032
      & 19.6\% \\
      \midrule
      \bad{Nsf5}
      & ---
      & $22 510 \times 1$
      & 186 583
      & 9.1\%
      & 62 194
      & 9.1\%
      & 248 776
      & 9.1\% \\
      \bad{JMiPOD}
      & ---
      & $256 \times 256\times 3$
      & 186 515
      & 9.1\%
      & 62 172
      & 9.1\%
      & 248 686
      & 9.1\% \\
      \midrule
      \bad{Malware}
      & ---
      & variable
      & 6 580 166
      & 87.22\%
      & ---
      & ---
      & 800 346
      & 91.8\% \\
      \bottomrule
    \end{NiceTabular}
  }
  \caption{Structure of the used datasets: The training, validation and testing sets show the positive label~$y^+,$ the number of features~$d$, samples~$n$ and the fraction of positive samples~$\frac{\npos}{n}$. Datasets depicted in red are not publicly available.}
  \label{tab: datasets summary}
\end{table}

\section{Results}

\subsection{Linear Model and Full Data}

\subsection{Linear Model and SGD}

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width = \linewidth]{images/critical_diagrams_primal.pdf}
%   \caption{Critical difference (CD) diagrams (level of importance 0.05) of the Nemenyi post hoc test for the Friedman test. Each diagram shows the mean rank of each method, with rank 1 being the best. Black wide horizontal lines group together methods with the mean ranks that are not significantly different. The critical difference diagrams were computed for mean rank averages over all datasets of the tpr@fpr at $\tau=0.05$ (top) and at $\tau=0.01$ (bottom).}
%   \label{fig: critical diagrams primal}
% \end{figure}

% \subsection{Dual Formulations}

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width = \linewidth]{images/critical_diagrams_dual_linear.pdf}
%   \caption{Critical difference (CD) diagrams (level of importance 0.05) of the Nemenyi post hoc test for the Friedman test. Each diagram shows the mean rank of each method, with rank 1 being the best. Black wide horizontal lines group together methods with the mean ranks that are not significantly different. The critical difference diagrams were computed for mean rank averages over all datasets of the tpr@fpr at $\tau=0.05$ (top) and at $\tau=0.01$ (bottom).}
%   \label{fig: critical diagrams dual linear}
% \end{figure}

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width = \linewidth]{images/critical_diagrams_dual.pdf}
%   \caption{Critical difference (CD) diagrams (level of importance 0.05) of the Nemenyi post hoc test for the Friedman test. Each diagram shows the mean rank of each method, with rank 1 being the best. Black wide horizontal lines group together methods with the mean ranks that are not significantly different. The critical difference diagrams were computed for mean rank averages over all datasets of the tpr@fpr at $\tau=0.05$ (top) and at $\tau=0.01$ (bottom).}
%   \label{fig: critical diagrams dual}
% \end{figure}

% \subsection{Non-linear model and SGD}

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width = \linewidth]{images/critical_diagrams_primalnn.pdf}
%   \caption{Critical difference (CD) diagrams (level of importance 0.05) of the Nemenyi post hoc test for the Friedman test. Each diagram shows the mean rank of each method, with rank 1 being the best. Black wide horizontal lines group together methods with the mean ranks that are not significantly different. The critical difference diagrams were computed for mean rank averages over all datasets of the tpr@fpr at $\tau=0.05$ (top) and at $\tau=0.01$ (bottom).}
%   \label{fig: critical diagrams primalnn}
% \end{figure}

\section{Conclusion}

\subsection{Linear Model}

In this paper, we achieved the following results:
\begin{itemize}
  \item We presented a unified framework for the three criteria from Chapter~\ref{chap: framework}. These criteria include ranking, accuracy at the top and hypothesis testing.
  \item We showed that several known methods (\TopPush, \Grill, \tauFPL) fall into our framework and derived some completely new methods (\PatMat, \PatMatNP).
  \item We performed a theoretical analysis of the methods. We showed that known methods suffer from certain disadvantages. While \TopPush and \tauFPL are sensitive to outliers, \Grill is non-convex. We proved the global convergence of the stochastic gradient descent for \PatMat and \PatMatNP.
  \item We performed a numerical comparison and we showed a good performance of our method \PatMatNP.
\end{itemize}


\subsection{Dual}\label{sec:Conclusion}

In this paper, we analyzed and extended the general framework for binary classification on top samples from~\cite{adam2021general} to nonlinear problems. Achieved results can be summarized as follows:
\begin{itemize}
  \item We derived the dual formulations for \TopPush, \TopPushK and \PatMat.
  \item We proposed a new method for solving the dual problems. We performed its complexity analysis. For selected surrogate functions we also derived the exact formulas needed in the method.
  \item We performed a numerical analysis of the proposed method. We showed its good convergence as well as improved performance of nonlinear kernels over the linear one.
\end{itemize}
Based on the numerical analysis from Section~\ref{sec:Numerical experiments}, we recommend using \TopPush or \TopPushK for problems where the resulting recall should be small. Otherwise, we recommend using \PatMat with an appropriately selected~$\tau$ parameter.

\subsection{Neural Networks}

We proposed \DeepTopPush as an efficient method for solving the constrained non-decomposable problem of accuracy at the top, which focuses on the performance only above a threshold. We implicitly removed some optimization variables, created an unconstrained end-to-end network and used the stochastic gradient descent to train it. We modified the minibatch so that the sampled threshold (computed on a minibatch) is a good estimate of the true threshold (computed on all samples). We showed both theoretically and numerically that this procedure reduces the bias of the sampled gradient. The time increase over the standard method with no threshold is small. We demonstrated the usefulness of \DeepTopPush both on visual recognition datasets, a ranking problem and on a real-world application of malware detection.
