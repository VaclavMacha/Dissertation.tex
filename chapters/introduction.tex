\chapter{Introduction}

The problem of data classification is very important mathematical problem. The goal of classification is to find a relation between a set of objects and a target variable based on some properties of the objects. The properties of the objects are usually called features. There are many problems in research as well as in the real world that can be formulated as classification tasks. We can find applications of data classification across all the fields:
\begin{itemize}
  \item \textbf{Medical Diagnonsis:} In medicine, the classification is often used to improve disease diagnosis. In such a case, the features are medical records such as the patient's blood tests, temperature, or roentgen images. The target variable is if the patient has some disease. As an example, classification is used to process mammogram images and detect cancer~\cite{viale2012current, levy2016breast}.
  \item \textbf{Internet Secutiry:} These days, the internet is a crucial part of our lives. With the increasing usage of the internet, the number of attacks increases as well. An essential part of the defense are intrusion detection systems~\cite{grill2016learning, scarfone2007guide} that search for malicious activities (network attacks) in network traffic. Classification can be used to improve such systems~\cite{giacinto2002intrusion, shanbhag2009accurate}.
  \item \textbf{Marketing:} In marketing, the task can be to classify customers based on their buying interests. Such information can be used to build a personalized recommendation system for customers and therefore increase income~\cite{kaefer2005neural, zhang2007building}.
\end{itemize}
Many other classification problems can be found in almost all fields. Also, there is a vast number of classification algorithms that try to solve these classifications problems. Typically these algorithms consist of two phases:
\begin{itemize}
  \item \textbf{Training Phase:} In the training phase, the algorithm uses training data to build a model. The classification algorithms fall into the category of supervised learning algorithms. It means, that these algorithms must have labeled training data to build the model, i.e. the algorithm must have the knowledge of the target classes. The training data typically consists of pairs (sample, label) and can be described as follows
  \begin{equation*}\label{eq: training set}
    \mathcal{D}_{\mathrm{train}} = \Brac[c]{(\bm{x}_i, y_i)}_{i=1}^{n},
  \end{equation*}
  where the sample~$\bm{x}_i \in \R^d$ is a $d$-dimensional vector of features that describes the object of interes and the label~$y_i \in \{1, 2, \ldots, k\}$ represents target class. Moreover~$n \in \N$ is a number of training samples and~$k \in \N$ is a number of target classes.
  \item \textbf{Testing Phase:} In the testing phase, the model is used to assign labels~$\hat{y}_i \in \{1, 2, \ldots, k\}$ to the data from testing set which was not known during the training phase
  \begin{equation*}\label{eq: test set}
    \mathcal{D}_{\mathrm{test}} = \Brac[c]{(\bm{x}_i, y_i)}_{i=1}^{m},
  \end{equation*}
  where~$\bm{x}_i \in \R^d,$~$y_i \in \{1, 2, \ldots, k\}$ and~$m \in \N$ is a number of testing samples. The ultimate goal of all classification algorithms is to classify testing samples with the highest accuracy possible.
\end{itemize}
The previous definitions of training and test set are general for classification problems with multiple classes. However, the main focus of this work is on a special subclass of classification problems with only two target classes: binary classification.


\section{Binary Classification}

The binary classification is a special case of classification in which the number of classes is~$k=2.$ These two classes are usually referred to as negative and positive classes and the positive class is the one that we are more interested in. If we go back to the mammogram example, the positive class would represent cancer. The positive class is usually encoded using label~$1.$ The encoding of the negative class differs for different algorithms. For example, SVM-like algorithms (from Support Vector Machines~\cite{cortes1995support}) usually use the label~$-1$ for the negative class. On the other hand, neural networks usually use the label~$0$ for the negative class. In this work, we will follow the notation used for neural networks. To simplify future notation, we denote set of all indices of training samples as~$\I = \I^- \cup \I^+,$ where
\begin{equation}
  \begin{aligned}
    \I^- & = \Set{i}{i \in \{1, 2, \ldots, n\} \; \land \; y_i = 0}, \\
    \I^+ & = \Set{i}{i \in \{1, 2, \ldots, n\} \; \land \; y_i = 1}.
  \end{aligned}
\end{equation}
We also denote the number of negative samples in the training set as~$n^- = \Brac[v]{\I^-}$ and the number of positive samples in the training set as~$n^+ = \Brac[v]{\I^+},$ i.e. total number of training samples is~$n = n^- + n^+.$ Generally, the problem of binary classification can be written as follows
\begin{mini}{\bm{w}}{
    \lambda_1 \sum_{i \in \I^-} \Iverson{s_i \ge t} + \lambda_2 \sum_{i \in \I^+} \Iverson{s_i < t}
  }{\label{eq: Binary classification}}{}
  \addConstraint{s_i}{= f(\bm{w}; \bm{x}_i), \quad}{i = 1, 2, \ldots, n}
  \addConstraint{t}{= 0.5,}
\end{mini}
where~$\lambda_1, \lambda_2 \in \R,$ the function $f \colon \R^d \to \R$ represents the model and maps samples~$\bm{x}_i$ to classification scores~$s_i$ and~$\Iverson{\cdot{}}$ represents Iverson function that is used to counts misclassified samples and is defined as
\begin{equation*}
  \Iverson{x} = \begin{cases}
    0 & \text{if } x \text{ is false}, \\
    1 & \text{if } x \text{ is true}. \\
  \end{cases}
\end{equation*}
Moreover, the vector~$\bm{w} \in \R^d$ represents trainable parameters (weights) of the model~$f$ and~$t \in R$ is a decision threshold. The parameters~$\bm{w}$ are determined from training data during the training phase of classification algorithm. The decision threshold may be also determined from training data. However, in most cases, this parameter is fixed. The reason is, that for many algorithms the classification score~$s_i$ given by the model~$f$ represents the probability that the sample~$\bm{x}_i$ belongs to the positive class. Therefore, it makes sense to set the decision threshold to~$t = 0.5$ and classify the sample as positive if its classification score is larger than this threshold, i.e. probability that the sample belongs to the positive class is larger or equal to~$0.5.$

\subsection{Performance Criteria}

In the previous section we defined general binary classification problem~\ref{eq: Binary classification}. However, we did not discuss yet how to measure the accuracy of classifiers. Consider the dataset in the following form
\begin{equation*}\label{eq: test set bin}
  \mathcal{D} = \Brac[c]{(\bm{x}_i, y_i)}_{i=1}^{m},
\end{equation*}
where~$\bm{x}_i \in \R^d,$~$y_i \in \{0, 1\}$ and~$m \in \N$ is a number of samples in the datase. With fixed classifier~$f$ (classifier with fixed weights~$\bm{w}$) and fixed decision threshold~$t,$ the following formula can be used to obtain predictions~$\hat{y}_i$ for all~$i = 1, 2, \ldots, m$
\begin{equation*}
  \hat{y}_i = \begin{cases}
    1 \ldots & \text{if } f(\bm{w}; \bm{x}_i) \ge t, \\
    0 \ldots & \text{otherwise.}
  \end{cases}
\end{equation*}
Based on the prediction~$\hat{y}_i$ and an actual label~$y_i$ of the sample~$\bm{x}_i,$ each sample can be assigned to one of the following categories
\begin{itemize}
  \item \textbf{True positive:} the sample~$\bm{x}_i$ is actually positive and is classified as positive, i.e.~$y_i = 1 \land \hat{y}_i = 1.$
  \item \textbf{False negative:} the sample~$\bm{x}_i$ is actually positive and is classified as negative, i.e.~$y_i = 1 \land \hat{y}_i = 0.$
  \item \textbf{False positive:} the sample~$\bm{x}_i$ is actually negative and is classified as positive, i.e.~$y_i = 0 \land \hat{y}_i = 1.$
  \item \textbf{True negative:} the sample~$\bm{x}_i$ is actually negative and is classified as negative, i.e.~$y_i = 0 \land \hat{y}_i = 0.$
\end{itemize}
With this in mind, the objective function in problem~(\ref{eq: Binary classification}) consists of two parts: the first part represents negative samples classified as positive and the second part represents positive samples classified as negative. To simplify the notation, we define the true-positive, false-negative, true-negative and false-positive counts by
\begin{equation}\label{eq: confusion counts}
  \begin{aligned}
    \tp(\bm{s}, t) & = \sum_{i \in \I^+}\Iverson{s_i \ge t}, &
    \fn(\bm{s}, t) & = \sum_{i \in \I^+}\Iverson{s_i < t}, \\
    \tn(\bm{s}, t) & = \sum_{i \in \I^-}\Iverson{s_i < t}, &
    \fp(\bm{s}, t) & = \sum_{i \in \I^-}\Iverson{s_i \ge t}.
  \end{aligned}
\end{equation}
Using this notation, the problem~(\ref{eq: Binary classification}) can be written in more compact form as follows
\begin{mini}{\bm{w}}{
    \lambda_1 \cdot \fp(\bm{s}, t) + \lambda_2 \cdot \fn(\bm{s}, t)
  }{\label{eq: Binary classification counts}}{}
  \addConstraint{s_i}{= f(\bm{w}; \bm{x}_i), \quad}{i = 1, 2, \ldots, n}
  \addConstraint{t}{= 0.5.}
\end{mini}

\begin{table}[!ht]
  \centering
  \begin{NiceTabular}{cccccc}[cell-space-limits = 7pt]
    && \Block[draw=black, line-width=2pt, rounded-corners]{1-2}{
      \longcell{\textbf{Predicted condition}}
    } \\
    && $\hat{y} = 0$
    &  $\hat{y} = 1$
    && \Block{1-1}{\textbf{Row total:}} \\
    \Block[draw=black, line-width=2pt, rounded-corners]{2-1}{
      \rotate \longcell{\textbf{Actual} \\ \textbf{condition}}
    }
    & $y = 0$
    & \Block[draw=mygreen, fill=mygreen!50, rounded-corners]{1-1}{
      \longcell{true \\ negatives \\ \textbf{tn}}
    }
    & \Block[draw=myred, fill=myred!50, rounded-corners]{1-1}{
      \longcell{false \\ positives \\ \textbf{fp}}
    }
    & $\rightarrow$
    & \Block[draw=black, rounded-corners]{1-1}{\longcell{all \\ negatives \\ \textbf{n}}} \\
    & $y = 1$
    & \Block[draw=myred, fill=myred!50, rounded-corners]{1-1}{
      \longcell{false \\ negatives \\ \textbf{fn}}
    }
    & \Block[draw=mygreen, fill=mygreen!50, rounded-corners]{1-1}{
      \longcell{true \\ positives \\ \textbf{tp}}
    }
    & $\rightarrow$
    & \Block[draw=black, rounded-corners]{1-1}{\longcell{all \\ positives \\ \textbf{p}}} \\
    && $\downarrow$
    &  $\downarrow$ \\
    \Block{1-2}{\longcell{\textbf{Column} \\ \textbf{total:}}}
    && \Block[draw=black, rounded-corners]{1-1}{
      \longcell{all predicted \\ negatives}
    }
    & \Block[draw=black, rounded-corners]{1-1}{
      \longcell{all predicted \\ positives}
    }
  \end{NiceTabular}
  \caption{Confusion matrix}
  \label{tbl: confusion matrix}
\end{table}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/confusion_rates.pdf}
  \caption{aaa}
  \label{fig: scores and rates}
\end{figure}