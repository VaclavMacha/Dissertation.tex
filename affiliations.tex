% ------------------------------------------------------------------------------
% Affiliation
% ------------------------------------------------------------------------------
\title{General Framework for Classification at the Top}
\subtitle{Dissertation}
\date{February, 2023}

\NumberOfPages{137}
\Year{2023}
\AcademicYear{2022/2023}

% author
\Author{Ing. Václav Mácha}
\AuthorAffCZE{
  České vysoké učení technické v Praze, \\
  Fakulta jaderná a fyzikálně inženýrská, \\
  Katedra matematiky
}
\AuthorAffENG{
  Czech Technical University in Prague, \\
  Faculty of Nuclear Sciences and Physical Engineering, \\
  Department of Mathematics
}
\DegreeProgrammeCZE{Aplikace přírodních věd}
\DegreeProgrammeENG{Application of Natural Sciences}
\FieldCZE{Matematické inženýrství}
\FieldENG{Mathematical Engineering}

% Supervisor
\Supervisor{doc. Ing. Václav Šmídl, Ph.D.}
\SupervisorAffCZE{
  Ústav teorie informace a automatizace, \\
  Akademie věd České republiky
}
\SupervisorAffENG{
  Institute of Information Theory and Automation, \\
  Czech Academy of Sciences
}

% Supervisor specialist
\SupervisorSpec{Mgr. Lukáš Adam, Ph.D.}
\SupervisorSpecAffCZE{
  České vysoké učení technické v Praze, \\
  Fakulta elektrotechnická
}
\SupervisorSpecAffENG{
  Czech Technical University in Prague, \\
  Faculty of Electrical Engineering
}

% Title
\TitleCZE{Title title title title title title}
\TitleENG{General Framework for Classification at the Top}

\Acknowledgment{
  I would like to thank both my supervisor Václav Šmídl and supervisor-specialist Lukáš Adam for their guidance, support, and time they devoted to me throughout my doctoral studies. I would also like to thank my family and my friends.
}

\Declaration{
  I hereby declare that this work is the result of my own work and all sources I have used are listed in the bibliography.
}

% Abstracts
\AbstractCZE{Abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract}

\AbstractENG{
  Standard binary classification aims to classify all samples with the lowest possible error. However, in many applications, the error for one class is more severe than the other, especially if the classes are not balanced. A typical example is cancer detection. Classifying a sick patient as healthy is a more serious error than the other way around. However, we still want to minimize both errors since trying to cure a healthy patient is also not ideal. Therefore, the goal is to minimize the number of sick patients classified as healthy with some constraint on the number of healthy patients classified as sick. This constraint can be formulated as a decision threshold computed from the classification scores of healthy patients. Then the goal is to minimize the number of sick patients with classification scores lower than the decision threshold. In other words, the performance is evaluated only on a small number of relevant (top) samples. The problems that try to solve this kind of optimization task are the main object of this work, and we collectively call them classification at the top.

  Many well-known categories of problems, such as ranking, accuracy at the top, or hypothesis testing, are closely related to classification at the top. In this work, we introduce a unified framework for classification at the top, show that several known formulations fall into it, and propose entirely new formulations (\PatMat, \PatMatNP) that fall into the framework. We provide a theoretical analysis of the proposed framework for different classifiers and analyze the properties of individual formulations and potential pitfalls that some formulations may encounter. Besides that, we show the convergence of stochastic gradient descent for selected formulations even though the gradient estimate is inherently biased. Moreover, we derive dual forms of selected formulations, show how to incorporate non-linear kernels into these forms, and derive an efficient coordinate descent algorithm to solve them. We also study the primal forms with non-linear models. We show that when we use a non-linear model, the resulting formulations are non-decomposable. This property prevents us from using stochastic gradient descent in a standard way. We introduce modified a stochastic gradient descent and show that this modification leads to a biased estimate of the true gradient. To mitigate this issue, we propose a new formulation \DeepTopPush. Lastly, we demonstrate the performance of proposed formulations on visual recognition datasets and real-world applications on steganalysis and malware detection datasets.
}

%  keywords
\KeywordsCZE{Binární klasifikace, Ranking, Accuracy at the Top, testování hypotéz}

\KeywordsENG{Binary Classification, Ranking, Accuracy at the Top, Hypothesis Testing}
