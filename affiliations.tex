% ------------------------------------------------------------------------------
% Affiliation
% ------------------------------------------------------------------------------
\title{General Framework for Classification at the Top}
\subtitle{Dissertation}
\date{February, 2023}

\NumberOfPages{137}
\Year{2023}
\AcademicYear{2022/2023}

% author
\Author{Ing. Václav Mácha}
\AuthorAffCZE{
  České vysoké učení technické v Praze, \\
  Fakulta jaderná a fyzikálně inženýrská, \\
  Katedra matematiky
}
\AuthorAffENG{
  Czech Technical University in Prague, \\
  Faculty of Nuclear Sciences and Physical Engineering, \\
  Department of Mathematics
}
\DegreeProgrammeCZE{Aplikace přírodních věd}
\DegreeProgrammeENG{Application of Natural Sciences}
\FieldCZE{Matematické inženýrství}
\FieldENG{Mathematical Engineering}

% Supervisor
\Supervisor{doc. Ing. Václav Šmídl, Ph.D.}
\SupervisorAffCZE{
  Ústav teorie informace a automatizace, \\
  Akademie věd České republiky
}
\SupervisorAffENG{
  Institute of Information Theory and Automation, \\
  Czech Academy of Sciences
}

% Supervisor specialist
\SupervisorSpec{Mgr. Lukáš Adam, Ph.D.}
\SupervisorSpecAffCZE{
  České vysoké učení technické v Praze, \\
  Fakulta elektrotechnická
}
\SupervisorSpecAffENG{
  Czech Technical University in Prague, \\
  Faculty of Electrical Engineering
}

% Title
\TitleCZE{Obecný rámec pro klasifikaci na vrchu}
\TitleENG{General Framework for Classification at the Top}

\Acknowledgment{
  I would like to thank both my supervisor Václav Šmídl and supervisor-specialist Lukáš Adam for their guidance, support, and time they devoted to me throughout my doctoral studies. I would also like to thank my family and my friends.
}

\Declaration{
  I hereby declare that this work is the result of my own work and all sources I have used are listed in the bibliography.
}

% Abstracts
\AbstractCZE{
  Cílem standardní binární klasifikace je klasifikovat všechny vzorky s nejnižší možnou chybou. V mnoha aplikacích je však chyba u jedné třídy vzorků závažnější než chyba u třídy druhé. Obvlášť pokud tyto třídy nejsou vyvážené. Typickým příkladem je detekce rakoviny, kde klasifikovat nemocného pacienta jako zdravého je závažnější chyba než klasifikovat zdravého jako nemocného. Nicméně i v tomto případě chceme minimalizovat obě chyby, protože snažit se léčit zdravého pacienta také není ideální. Cílem je tedy minimalizovat počet nemocných pacientů, které klasifikujeme jako zdravé a zároveň mít  dané omezení na počet zdravých pacientů, které klasifikujeme jako nemocné. Toto omezení lze formulovat jako rozhodovací práh vypočítaný z klasifikačních skóre zdravých pacientů. Tím dostaneme úlohu minimalizace počtu nemocných pacientů, kteří mají klasifikační skóre nižší, než je daný rozhodovací práh. Jinými slovy, chyba se počítá pouze z malého počtu vzorků s nejvyšším klasifikačním skóre (vzorky nahoře). Problémy, které se pokoušejí vyřešit tento druh optimalizačních úloh, jsou hlavním předmětem této práce a souhrnně je nazýváme klasifikace na vrchu.

  Mnoho známých kategorií problémů, jako je \emph{ranking}, \emph{accuracy at the top} nebo \emph{testování hypotéz}, úzce souvisí s klasifikací na vrchu. Tato práce představuje jednotný rámec pro klasifikaci na vrchu, ukazuje, že do tohoto rámce spadají některé již existující formulace, a navrhuje zcela nové formulace (\PatMat, \PatMatNP), které do rámce spadají také. Dále práce poskytuje teoretickou analýzu navrženého rámce pro různé klasifikátory a analyzuje vlastnosti jednotlivých formulací a potenciální úskalí, se kterými se lze setkat při použití některých z nich. Kromě toho práce ukazuje konvergenci metody stochastického gradientního sestupu pro vybrané formulace, i když je odhad gradientu ze své podstaty vychýlený. Navíc práce odvozuje duální formy vybraných formulací, ukazuje, jak do těchto forem začlenit nelineární jádra a představuje efektivní algoritmus souřadnicového sestupu pro jejich řešení. Práce také studuje primární formy s nelineárními modely. Ukazuje, že pokud použijeme nelineární model, výsledné formulace jsou nerozložitelné. Tato vlastnost brání použítí metody stochastického gradientního sestupu standardním způsobem. Práce tedy zavádí modifikaci této metody a ukazuje, že tato modifikace vede k vychýlenému odhadu skutečného gradientu. Pro zmírnění tohoto problému práce navrhuje novou formulaci \DeepTopPush. Nakonec je výkon navrhovaných formulací demonstrován na datových sadách pro rozpoznávání obrazu a na reálných datových sadách pro steganalýzu a detekci malwaru.
}

\AbstractENG{
  Standard binary classification aims to classify all samples with the lowest possible error. However, in many applications, the error for one class is more severe than the other, especially if the classes are not balanced. A typical example is cancer detection. Classifying a sick patient as healthy is a more serious error than the other way around. However, we still want to minimize both errors since trying to cure a healthy patient is also not ideal. Therefore, the goal is to minimize the number of sick patients classified as healthy with some constraint on the number of healthy patients classified as sick. This constraint can be formulated as a decision threshold computed from the classification scores of healthy patients. Then the goal is to minimize the number of sick patients with classification scores lower than the decision threshold. In other words, the error is computed only on a small number of samples with the highest classification scores (samples at the top). The problems that try to solve this kind of optimization task are the main object of this work, and we collectively call them classification at the top.

  Many well-known categories of problems, such as \emph{ranking}, \emph{accuracy at the top}, or \emph{hypothesis testing}, are closely related to classification at the top. In this work, we introduce a unified framework for classification at the top, show that several known formulations fall into it, and propose entirely new formulations (\PatMat, \PatMatNP) that fall into the framework. We provide a theoretical analysis of the proposed framework for different classifiers and analyze the properties of individual formulations and potential pitfalls that some formulations may encounter. Besides that, we show the convergence of stochastic gradient descent for selected formulations even though the gradient estimate is inherently biased. Moreover, we derive dual forms of selected formulations, show how to incorporate non-linear kernels into these forms, and derive an efficient coordinate descent algorithm to solve them. We also study the primal forms with non-linear models. We show that when we use a non-linear model, the resulting formulations are non-decomposable. This property prevents us from using stochastic gradient descent in a standard way. We introduce modified a stochastic gradient descent and show that this modification leads to a biased estimate of the true gradient. To mitigate this issue, we propose a new formulation \DeepTopPush. Lastly, we demonstrate the performance of proposed formulations on visual recognition datasets and real-world applications on steganalysis and malware detection datasets.
}

%  keywords
\KeywordsCZE{binární klasifikace, ranking, accuracy at the top, testování hypotéz}

\KeywordsENG{Binary Classification, Ranking, Accuracy at the Top, Hypothesis Testing}
